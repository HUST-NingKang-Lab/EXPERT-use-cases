{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed to get more reproducible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(2)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from expert.src.utils import read_genus_abu, read_labels, load_otlg, zero_weight_unk, parse_otlg, get_dmax\n",
    "from expert.src.preprocessing import *\n",
    "from expert.src.model import *\n",
    "from expert.CLI.CLI_utils import find_pkg_resource as find_expert_resource\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, AlphaDropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.initializers import HeUniform, GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "info1 = pd.read_csv('dataFiles/filereport_read_run_PRJNA392012.tsv', sep='\\t')\n",
    "info2 = pd.read_csv('dataFiles/HADZA_MAPPING_350_20201210_9am.csv')\n",
    "info1['#SampleID'] = info1.sample_alias.str.lstrip('TZHADZA_').astype(int)\n",
    "meta = pd.merge(left=info1, right=info2, on='#SampleID').drop(columns=['sample_accession', 'experiment_accession', 'scientific_name', 'fastq_ftp', 'submitted_ftp', \n",
    "                                                                'sra_ftp', 'Study', 'Country', 'Population']\n",
    "                                                      ).set_index('run_accession').loc[abu.columns.tolist(), :]\n",
    "meta = meta[meta.Metagenomics != 'Yes']\n",
    "meta.index.name = 'SampleID'\n",
    "meta.to_csv('dataFiles/samples_meta.csv')\n",
    "abu.loc[:, meta.index.tolist()].to_csv('dataFiles/species_abundance.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('dataFiles/samples_meta.csv')\n",
    "meta['Env'] = 'root:'+ meta.SEASON.str.lstrip('20134-LE')\n",
    "meta.to_csv('dataFiles/samples_meta_new.csv')\n",
    "abu = abu.loc[:, meta.SampleID]\n",
    "abu.to_csv('dataFiles/species_abundance.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python ../../UniPCoA/UniPCoA.py -i dataFiles/species_abundance.csv -m dataFiles/samples_meta_new.csv --metric unweighted_unifrac -o PCoA.un.Unifrac -t ../../UniPCoA/LTPs132_SSU_tree.newick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_2 = ['SRR5761120', 'SRR5760853', 'SRR5761144', 'SRR5760882', 'SRR5761191']\n",
    "meta.loc[meta.SampleID.isin(queries_2), :].to_csv('experiments/QueryMapper2.csv')\n",
    "abu.loc[:, abu.columns.to_series().isin(queries_2)].to_csv('experiments/QueryCM2.tsv', sep='\\t')\n",
    "\n",
    "abu_rest = abu.loc[:, ~abu.columns.to_series().isin(queries_2)]\n",
    "mapper = meta[~meta.SampleID.isin(queries_2)]\n",
    "\n",
    "for exp, season in enumerate(['2013-LD', '2014-EW', '2014-LW', '2014-ED', '2014-LD']):\n",
    "    queries_1 = mapper.groupby(by='SEASON').sample(frac=0.1).SampleID\n",
    "    mapper[mapper.SampleID.isin(queries_1)].to_csv('experiments/exp_{}/QueryMapper1.csv'.format(exp))\n",
    "    abu_rest[queries_1].to_csv('experiments/exp_{}/QueryCM1.tsv'.format(exp), sep='\\t')\n",
    "    mapper[~mapper.SampleID.isin(queries_1)].to_csv('experiments/exp_{}/SourceMapper.csv'.format(exp))\n",
    "    abu_rest.loc[:, ~abu_rest.columns.to_series().isin(queries_1)].to_csv('experiments/exp_{}/SourceCM.tsv'.format(exp), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ontology using mapper file of source samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading microbiome structure...\n",
      "Generating Ontology...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 11881.88it/s]\n",
      "root\n",
      "├── root:D\n",
      "└── root:W\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!expert construct -i microbiomes.txt -o ontology.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data, using EXPERT's command-line API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760853  SRR5760882  SRR5761120  SRR5761144  SRR5761191\n",
      "count  487.000000  487.000000  487.000000  487.000000  487.000000\n",
      "mean     0.353330    0.281948    0.357357    0.312273    0.274692\n",
      "std      4.009807    3.354944    4.269951    2.352340    2.076948\n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000\n",
      "max     80.505304   69.899519   87.655334   40.719208   36.390414\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760853   SRR5760882   SRR5761120   SRR5761144   SRR5761191\n",
      "count  6006.000000  6006.000000  6006.000000  6006.000000  6006.000000\n",
      "mean      0.028403     0.022290     0.028488     0.024355     0.021695\n",
      "std       1.144770     0.957096     1.218443     0.672959     0.595054\n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "max      80.505302    69.899521    87.655334    40.719208    36.390415\n",
      "Normalizing results...\n",
      "        SRR5760853   SRR5760882   SRR5761120   SRR5761144   SRR5761191\n",
      "count  6006.000000  6006.000000  6006.000000  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167     0.000167     0.000167     0.000167\n",
      "std       0.006711     0.007149     0.007121     0.004601     0.004567\n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "max       0.471927     0.522120     0.512316     0.278367     0.279275\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       5\n",
      "Unknown    0\n",
      "dtype: int64\n",
      "root:D     3\n",
      "root:W     2\n",
      "Unknown    0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760852  SRR5760855  SRR5760856  ...  SRR5761187  SRR5761189  SRR5761196\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.341040    0.350467    0.333661  ...    0.312495    0.344657    0.337987\n",
      "std      3.542116    3.910157    3.905181  ...    2.601293    2.649880    4.130398\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     56.613300   81.226306   81.312616  ...   40.492937   38.901994   84.659529\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760852   SRR5760855  ...   SRR5761189   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026221     0.027945  ...     0.027343     0.026742\n",
      "std       1.009538     1.116249  ...     0.759155     1.178373\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      56.613300    81.226303  ...    38.901993    84.659531\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Normalizing results...\n",
      "        SRR5760852   SRR5760855  ...   SRR5761189   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006410     0.006651  ...     0.004623     0.007337\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.359487     0.483960  ...     0.236884     0.527105\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5761075  SRR5761058  SRR5760996  ...  SRR5760885  SRR5761086  SRR5760890\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.317336    0.372841    0.313466  ...    0.383034    0.366653    0.304780\n",
      "std      1.835790    4.351395    3.274436  ...    7.614272    5.783572    2.805410\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     20.621118   85.757438   66.044394  ...  167.989167  126.428000   50.849208\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5761075   SRR5761058  ...   SRR5761086   SRR5760890\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.021933     0.029383  ...     0.029367     0.023375\n",
      "std       0.491677     1.240533  ...     1.648331     0.800529\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      20.621119    85.757439  ...   126.428001    50.849209\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "        SRR5761075   SRR5761058  ...   SRR5761086   SRR5760890\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003733     0.007030  ...     0.009345     0.005702\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.156545     0.485950  ...     0.716797     0.362202\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       179\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:D     93\n",
      "root:W     86\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:D     10\n",
      "root:W      9\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760852  SRR5760855  SRR5760856  ...  SRR5761189  SRR5761190  SRR5761196\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.341040    0.350467    0.333661  ...    0.344657    0.340801    0.337987\n",
      "std      3.542116    3.910157    3.905181  ...    2.649880    3.018878    4.130398\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     56.613300   81.226306   81.312616  ...   38.901994   61.817056   84.659529\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760852   SRR5760855  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026221     0.027945  ...     0.024690     0.026742\n",
      "std       1.009538     1.116249  ...     0.855009     1.178373\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      56.613300    81.226303  ...    61.817055    84.659531\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Normalizing results...\n",
      "        SRR5760852   SRR5760855  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006410     0.006651  ...     0.005766     0.007337\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.359487     0.483960  ...     0.416867     0.527105\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5761067  SRR5760910  SRR5761075  ...  SRR5761045  SRR5760893  SRR5760959\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.368360    0.382028    0.317336  ...    0.331738    0.356273    0.339563\n",
      "std      4.032674    7.470264    1.835790  ...    5.350897    5.091696    3.939517\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     77.424317  164.628821   20.621118  ...  116.691494  110.883370   82.723472\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5761067   SRR5760910  ...   SRR5760893   SRR5760959\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.029283     0.030727  ...     0.028310     0.026255\n",
      "std       1.151365     2.127695  ...     1.451469     1.121877\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      77.424316   164.628815  ...   110.883369    82.723473\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "        SRR5761067   SRR5760910  ...   SRR5760893   SRR5760959\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006546     0.011529  ...     0.008537     0.007115\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.440224     0.892070  ...     0.652147     0.524606\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       179\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:D     93\n",
      "root:W     86\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:D     10\n",
      "root:W      9\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760852  SRR5760855  SRR5760856  ...  SRR5761187  SRR5761190  SRR5761196\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.341040    0.350467    0.333661  ...    0.312495    0.340801    0.337987\n",
      "std      3.542116    3.910157    3.905181  ...    2.601293    3.018878    4.130398\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     56.613300   81.226306   81.312616  ...   40.492937   61.817056   84.659529\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760852   SRR5760855  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026221     0.027945  ...     0.024690     0.026742\n",
      "std       1.009538     1.116249  ...     0.855009     1.178373\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      56.613300    81.226303  ...    61.817055    84.659531\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Normalizing results...\n",
      "        SRR5760852   SRR5760855  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006410     0.006651  ...     0.005766     0.007337\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.359487     0.483960  ...     0.416867     0.527105\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760871  SRR5760864  SRR5761084  ...  SRR5760888  SRR5760884  SRR5761166\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.372392    0.361797    0.279106  ...    0.346511    0.391491    0.380563\n",
      "std      4.890737    4.455748    2.588791  ...    4.580345    6.444214    6.017698\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     99.941404   80.899842   53.040171  ...   97.136577  139.099041  128.338708\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760871   SRR5760864  ...   SRR5760884   SRR5761166\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.029750     0.029062  ...     0.031608     0.030666\n",
      "std       1.394971     1.271388  ...     1.836397     1.715084\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      99.941406    80.899841  ...   139.099045   128.338715\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "        SRR5760871   SRR5760864  ...   SRR5760884   SRR5761166\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.007807     0.007284  ...     0.009674     0.009312\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.559342     0.463483  ...     0.732728     0.696817\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       179\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:D     93\n",
      "root:W     86\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:D     10\n",
      "root:W      9\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760855  SRR5760856  SRR5760857  ...  SRR5761189  SRR5761190  SRR5761196\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.350467    0.333661    0.341525  ...    0.344657    0.340801    0.337987\n",
      "std      3.910157    3.905181    2.892207  ...    2.649880    3.018878    4.130398\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     81.226306   81.312616   47.043407  ...   38.901994   61.817056   84.659529\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760855   SRR5760856  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.027945     0.026626  ...     0.024690     0.026742\n",
      "std       1.116249     1.114576  ...     0.855009     1.178373\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      81.226303    81.312614  ...    61.817055    84.659531\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Normalizing results...\n",
      "        SRR5760855   SRR5760856  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006651     0.006970  ...     0.005766     0.007337\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.483960     0.508463  ...     0.416867     0.527105\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5761121  SRR5760864  SRR5760996  ...  SRR5760966  SRR5760888  SRR5760852\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.371217    0.361797    0.313466  ...    0.328709    0.346511    0.341040\n",
      "std      4.912160    4.455748    3.274436  ...    4.215572    4.580345    3.542116\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max    101.695121   80.899842   66.044394  ...   90.269151   97.136577   56.613300\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5761121   SRR5760864  ...   SRR5760888   SRR5760852\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.029768     0.029062  ...     0.027535     0.026221\n",
      "std       1.401055     1.271388  ...     1.306205     1.009538\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max     101.695122    80.899841  ...    97.136574    56.613300\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "        SRR5761121   SRR5760864  ...   SRR5760888   SRR5760852\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.007837     0.007284  ...     0.007899     0.006410\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.568817     0.463483  ...     0.587379     0.359487\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       179\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:D     93\n",
      "root:W     86\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:D     10\n",
      "root:W      9\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760852  SRR5760855  SRR5760856  ...  SRR5761189  SRR5761190  SRR5761196\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.341040    0.350467    0.333661  ...    0.344657    0.340801    0.337987\n",
      "std      3.542116    3.910157    3.905181  ...    2.649880    3.018878    4.130398\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     56.613300   81.226306   81.312616  ...   38.901994   61.817056   84.659529\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760852   SRR5760855  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026221     0.027945  ...     0.024690     0.026742\n",
      "std       1.009538     1.116249  ...     0.855009     1.178373\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      56.613300    81.226303  ...    61.817055    84.659531\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Normalizing results...\n",
      "        SRR5760852   SRR5760855  ...   SRR5761190   SRR5761196\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006410     0.006651  ...     0.005766     0.007337\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.359487     0.483960  ...     0.416867     0.527105\n",
      "\n",
      "[6 rows x 179 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "       SRR5760871  SRR5760912  SRR5761075  ...  SRR5761043  SRR5761038  SRR5761045\n",
      "count  487.000000  487.000000  487.000000  ...  487.000000  487.000000  487.000000\n",
      "mean     0.372392    0.316807    0.317336  ...    0.376901    0.338597    0.331738\n",
      "std      4.890737    2.202831    1.835790  ...    6.527564    4.324551    5.350897\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     99.941404   33.436331   20.621118  ...  143.661523   93.020618  116.691494\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/487 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR5760871   SRR5760912  ...   SRR5761038   SRR5761045\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.029750     0.025300  ...     0.026228     0.026260\n",
      "std       1.394971     0.632426  ...     1.231634     1.524738\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      99.941406    33.436333  ...    93.020615   116.691490\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "        SRR5760871   SRR5760912  ...   SRR5761038   SRR5761045\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.007807     0.004162  ...     0.007818     0.009668\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.559342     0.220046  ...     0.590501     0.739890\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       179\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:D     93\n",
      "root:W     86\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:D     10\n",
      "root:W      9\n",
      "Unknown     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 165.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 161.32it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 117.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 298.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 254.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 120.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 291.06it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 33.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 188.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 27.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 259.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 308.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 115.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 282.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.71it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 291.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 36.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 276.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 289.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 36.51it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls experiments/QueryCM2.tsv > tmp; expert convert -i tmp --in-cm -o experiments/QueryCM2.h5\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/QueryMapper2.csv -o experiments/QueryLabels2.h5\n",
    "for i in {0,1,2,3,4}; do\n",
    "ls experiments/exp_$i/SourceCM.tsv > tmp; expert convert -i tmp --in-cm -o experiments/exp_$i/SourceCM.h5;\n",
    "ls experiments/exp_$i/QueryCM1.tsv > tmp; expert convert -i tmp --in-cm -o experiments/exp_$i/QueryCM1.h5;\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/exp_$i/SourceMapper.csv -o experiments/exp_$i/SourceLabels.h5;\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/exp_$i/QueryMapper1.csv -o experiments/exp_$i/QueryLabels1.h5;\n",
    "done\n",
    "rm tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordering labels and samples...\n",
      "Total matched samples: 179\n",
      "Total correct samples: 179?179\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006795  0.024863\n",
      "4      0.006725  0.024869\n",
      "...         ...       ...\n",
      "18013  0.000114  0.000321\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000285  0.001141\n",
      "18016  0.000015  0.000087\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 204ms/step - loss: 0.8609 - acc: 0.4876 - auROC: 0.5436 - val_loss: 0.8064 - val_acc: 0.3611 - val_auROC: 0.4290\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.7487 - acc: 0.4876 - auROC: 0.5080 - val_loss: 0.7464 - val_acc: 0.3611 - val_auROC: 0.4120\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.7233 - acc: 0.4783 - auROC: 0.5102 - val_loss: 0.6944 - val_acc: 0.4444 - val_auROC: 0.5448\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 0.6959 - acc: 0.5186 - auROC: 0.6115 - val_loss: 0.6803 - val_acc: 0.4722 - val_auROC: 0.6096\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6622 - acc: 0.5652 - auROC: 0.7221 - val_loss: 0.6427 - val_acc: 0.5556 - val_auROC: 0.7253\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.6475 - acc: 0.5932 - auROC: 0.7557 - val_loss: 0.6186 - val_acc: 0.6111 - val_auROC: 0.8071\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6362 - acc: 0.6242 - auROC: 0.7685 - val_loss: 0.6141 - val_acc: 0.6389 - val_auROC: 0.8148\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6316 - acc: 0.6366 - auROC: 0.7743 - val_loss: 0.6145 - val_acc: 0.6944 - val_auROC: 0.8117\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6246 - acc: 0.6770 - auROC: 0.7956 - val_loss: 0.6155 - val_acc: 0.6944 - val_auROC: 0.7809\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6205 - acc: 0.7112 - auROC: 0.8014 - val_loss: 0.6183 - val_acc: 0.7222 - val_auROC: 0.7793\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6154 - acc: 0.7391 - auROC: 0.8090 - val_loss: 0.6230 - val_acc: 0.7500 - val_auROC: 0.8025\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6114 - acc: 0.7734 - auROC: 0.8239\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6066 - acc: 0.7640 - auROC: 0.8266 - val_loss: 0.6248 - val_acc: 0.7500 - val_auROC: 0.8056\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6021 - acc: 0.7795 - auROC: 0.8346 - val_loss: 0.6248 - val_acc: 0.7500 - val_auROC: 0.7994\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6016 - acc: 0.7795 - auROC: 0.8344 - val_loss: 0.6248 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6010 - acc: 0.7826 - auROC: 0.8360 - val_loss: 0.6248 - val_acc: 0.7500 - val_auROC: 0.7963\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6005 - acc: 0.7826 - auROC: 0.8369 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7963\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6023 - acc: 0.7812 - auROC: 0.8311\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5999 - acc: 0.7826 - auROC: 0.8370 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7948\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5994 - acc: 0.7826 - auROC: 0.8375 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7948\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5994 - acc: 0.7826 - auROC: 0.8375 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5993 - acc: 0.7826 - auROC: 0.8375 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5993 - acc: 0.7857 - auROC: 0.8375 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6213 - acc: 0.7500 - auROC: 0.7725\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.5992 - acc: 0.7857 - auROC: 0.8375 - val_loss: 0.6247 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 00022: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 22/321\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 0.6318 - acc: 0.6304 - auROC: 0.7763 - val_loss: 0.6146 - val_acc: 0.6389 - val_auROC: 0.8040\n",
      "Epoch 23/321\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.6278 - acc: 0.6398 - auROC: 0.7877 - val_loss: 0.6141 - val_acc: 0.6389 - val_auROC: 0.7978\n",
      "Epoch 24/321\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6235 - acc: 0.6460 - auROC: 0.7960 - val_loss: 0.6143 - val_acc: 0.6389 - val_auROC: 0.8009\n",
      "Epoch 25/321\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.6211 - acc: 0.6584 - auROC: 0.8046 - val_loss: 0.6144 - val_acc: 0.6389 - val_auROC: 0.7994\n",
      "Epoch 26/321\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6176 - acc: 0.6615 - auROC: 0.8129 - val_loss: 0.6148 - val_acc: 0.6389 - val_auROC: 0.7978\n",
      "Epoch 27/321\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6151 - acc: 0.6739 - auROC: 0.8191 - val_loss: 0.6151 - val_acc: 0.6389 - val_auROC: 0.7948\n",
      "Epoch 28/321\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6128 - acc: 0.6770 - auROC: 0.8232 - val_loss: 0.6154 - val_acc: 0.6389 - val_auROC: 0.7870\n",
      "Epoch 29/321\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6104 - acc: 0.6863 - auROC: 0.8309 - val_loss: 0.6160 - val_acc: 0.6389 - val_auROC: 0.7855\n",
      "Epoch 30/321\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6081 - acc: 0.6957 - auROC: 0.8344 - val_loss: 0.6168 - val_acc: 0.6389 - val_auROC: 0.7824\n",
      "Epoch 31/321\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6057 - acc: 0.7019 - auROC: 0.8388 - val_loss: 0.6175 - val_acc: 0.6389 - val_auROC: 0.7824\n",
      "Epoch 32/321\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6035 - acc: 0.7112 - auROC: 0.8425 - val_loss: 0.6181 - val_acc: 0.6389 - val_auROC: 0.7870\n",
      "Epoch 33/321\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6012 - acc: 0.7143 - auROC: 0.8475 - val_loss: 0.6188 - val_acc: 0.6389 - val_auROC: 0.7824\n",
      "Epoch 34/321\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5991 - acc: 0.7174 - auROC: 0.8500 - val_loss: 0.6196 - val_acc: 0.6389 - val_auROC: 0.7917\n",
      "Epoch 35/321\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5968 - acc: 0.7205 - auROC: 0.8532 - val_loss: 0.6204 - val_acc: 0.6111 - val_auROC: 0.7963\n",
      "Epoch 36/321\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5947 - acc: 0.7236 - auROC: 0.8557 - val_loss: 0.6211 - val_acc: 0.6111 - val_auROC: 0.7963\n",
      "Epoch 37/321\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5928 - acc: 0.7298 - auROC: 0.8564 - val_loss: 0.6218 - val_acc: 0.6111 - val_auROC: 0.7948\n",
      "Epoch 38/321\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5913 - acc: 0.7360 - auROC: 0.8587Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.5913 - acc: 0.7360 - auROC: 0.8587 - val_loss: 0.6223 - val_acc: 0.6111 - val_auROC: 0.7948\n",
      "Epoch 00038: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0  0.085752  ...    0.0 -0.250077 -0.175771    0.0\n",
      "1     0.0    0.0    0.0 -0.271014  ...    0.0 -0.100080 -0.175771    0.0\n",
      "2     0.0    0.0    0.0 -0.217780  ...    0.0  0.085994  0.704050    0.0\n",
      "3     0.0    0.0    0.0 -0.273309  ...    0.0 -0.250077 -0.175771    0.0\n",
      "4     0.0    0.0    0.0 -0.264610  ...    0.0 -0.060537 -0.175771    0.0\n",
      "5     0.0    0.0    0.0  1.112036  ...    0.0  0.076233 -0.175771    0.0\n",
      "6     0.0    0.0    0.0 -0.246137  ...    0.0  6.692550  0.528707    0.0\n",
      "7     0.0    0.0    0.0  0.037334  ...    0.0  1.823079 -0.175771    0.0\n",
      "8     0.0    0.0    0.0  0.653454  ...    0.0  0.206243 -0.175771    0.0\n",
      "9     0.0    0.0    0.0 -0.236390  ...    0.0 -0.250077 -0.175771    0.0\n",
      "10    0.0    0.0    0.0  0.128392  ...    0.0 -0.250077 -0.175771    0.0\n",
      "11    0.0    0.0    0.0  1.138985  ...    0.0 -0.084196 -0.175771    0.0\n",
      "12    0.0    0.0    0.0 -0.261597  ...    0.0 -0.250077 -0.175771    0.0\n",
      "13    0.0    0.0    0.0 -0.265549  ...    0.0 -0.250077 -0.175771    0.0\n",
      "14    0.0    0.0    0.0 -0.162600  ...    0.0 -0.250077 -0.175771    0.0\n",
      "15    0.0    0.0    0.0  2.552932  ...    0.0 -0.250077 -0.175771    0.0\n",
      "16    0.0    0.0    0.0 -0.273309  ...    0.0 -0.250077 -0.175771    0.0\n",
      "17    0.0    0.0    0.0 -0.251026  ...    0.0 -0.250077 -0.175771    0.0\n",
      "18    0.0    0.0    0.0  0.989163  ...    0.0 -0.178445 -0.175771    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "   0      1      2         3      ...  18014     18015     18016  18017\n",
      "0    0.0    0.0    0.0 -0.240100  ...    0.0 -0.250077 -0.175771    0.0\n",
      "1    0.0    0.0    0.0  0.274182  ...    0.0 -0.209225 -0.175771    0.0\n",
      "2    0.0    0.0    0.0 -0.262286  ...    0.0 -0.250077 -0.175771    0.0\n",
      "3    0.0    0.0    0.0 -0.256371  ...    0.0 -0.250077 -0.175771    0.0\n",
      "4    0.0    0.0    0.0 -0.254023  ...    0.0 -0.160036 -0.175771    0.0\n",
      "\n",
      "[5 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 179\n",
      "Total correct samples: 179?179\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.007333  0.025070\n",
      "4      0.007260  0.025073\n",
      "...         ...       ...\n",
      "18013  0.000117  0.000324\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000331  0.001277\n",
      "18016  0.000016  0.000087\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 213ms/step - loss: 0.8648 - acc: 0.4783 - auROC: 0.5492 - val_loss: 0.7436 - val_acc: 0.4167 - val_auROC: 0.4522\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.7587 - acc: 0.4720 - auROC: 0.5135 - val_loss: 0.6998 - val_acc: 0.4444 - val_auROC: 0.5231\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.7157 - acc: 0.4845 - auROC: 0.6189 - val_loss: 0.6747 - val_acc: 0.4722 - val_auROC: 0.6759\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6746 - acc: 0.5124 - auROC: 0.7196 - val_loss: 0.6273 - val_acc: 0.5833 - val_auROC: 0.8102\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6500 - acc: 0.5435 - auROC: 0.7470 - val_loss: 0.6248 - val_acc: 0.6944 - val_auROC: 0.8179\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6380 - acc: 0.5807 - auROC: 0.7675 - val_loss: 0.6172 - val_acc: 0.7222 - val_auROC: 0.8349\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6349 - acc: 0.5807 - auROC: 0.7677 - val_loss: 0.6037 - val_acc: 0.7222 - val_auROC: 0.8580\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6309 - acc: 0.5932 - auROC: 0.7720 - val_loss: 0.6164 - val_acc: 0.7222 - val_auROC: 0.8210\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6208 - acc: 0.6335 - auROC: 0.7897 - val_loss: 0.6157 - val_acc: 0.7500 - val_auROC: 0.8302\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6143 - acc: 0.6770 - auROC: 0.7957 - val_loss: 0.6103 - val_acc: 0.7778 - val_auROC: 0.8318\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6090 - acc: 0.6957 - auROC: 0.8034 - val_loss: 0.6029 - val_acc: 0.7778 - val_auROC: 0.8441\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6027 - acc: 0.6925 - auROC: 0.8141 - val_loss: 0.5942 - val_acc: 0.7778 - val_auROC: 0.8580\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5995 - acc: 0.6801 - auROC: 0.8135 - val_loss: 0.5898 - val_acc: 0.8056 - val_auROC: 0.8580\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5948 - acc: 0.6770 - auROC: 0.8188 - val_loss: 0.5862 - val_acc: 0.7778 - val_auROC: 0.8673\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5880 - acc: 0.6894 - auROC: 0.8331 - val_loss: 0.5818 - val_acc: 0.7778 - val_auROC: 0.8627\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5833 - acc: 0.6988 - auROC: 0.8333 - val_loss: 0.5732 - val_acc: 0.7222 - val_auROC: 0.8627\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5784 - acc: 0.7019 - auROC: 0.8424 - val_loss: 0.5709 - val_acc: 0.7222 - val_auROC: 0.8580\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5721 - acc: 0.7205 - auROC: 0.8428 - val_loss: 0.5708 - val_acc: 0.7500 - val_auROC: 0.8534\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5645 - acc: 0.7081 - auROC: 0.8458 - val_loss: 0.5703 - val_acc: 0.7500 - val_auROC: 0.8642\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5630 - acc: 0.6988 - auROC: 0.8515 - val_loss: 0.5578 - val_acc: 0.7778 - val_auROC: 0.8873\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5568 - acc: 0.7050 - auROC: 0.8537 - val_loss: 0.5523 - val_acc: 0.7222 - val_auROC: 0.8611\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5531 - acc: 0.7112 - auROC: 0.8458 - val_loss: 0.5622 - val_acc: 0.7778 - val_auROC: 0.8503\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5505 - acc: 0.7422 - auROC: 0.8440 - val_loss: 0.5254 - val_acc: 0.8056 - val_auROC: 0.9043\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5453 - acc: 0.7329 - auROC: 0.8494 - val_loss: 0.5082 - val_acc: 0.8056 - val_auROC: 0.9228\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5385 - acc: 0.7236 - auROC: 0.8528 - val_loss: 0.4967 - val_acc: 0.8056 - val_auROC: 0.9321\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5336 - acc: 0.7298 - auROC: 0.8541 - val_loss: 0.4838 - val_acc: 0.7778 - val_auROC: 0.9414\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5290 - acc: 0.7298 - auROC: 0.8582 - val_loss: 0.4932 - val_acc: 0.8056 - val_auROC: 0.9275\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5227 - acc: 0.7329 - auROC: 0.8592 - val_loss: 0.5018 - val_acc: 0.7778 - val_auROC: 0.8981\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5187 - acc: 0.7609 - auROC: 0.8595 - val_loss: 0.4980 - val_acc: 0.8333 - val_auROC: 0.9120\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.5102 - acc: 0.8168 - auROC: 0.8654 - val_loss: 0.4955 - val_acc: 0.8889 - val_auROC: 0.9120\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4902 - acc: 0.8828 - auROC: 0.8917\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5052 - acc: 0.8509 - auROC: 0.8647 - val_loss: 0.4891 - val_acc: 0.8889 - val_auROC: 0.9120\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4991 - acc: 0.8602 - auROC: 0.8681 - val_loss: 0.4892 - val_acc: 0.8889 - val_auROC: 0.9120\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4981 - acc: 0.8634 - auROC: 0.8688 - val_loss: 0.4893 - val_acc: 0.8889 - val_auROC: 0.9136\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4973 - acc: 0.8634 - auROC: 0.8689 - val_loss: 0.4902 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4965 - acc: 0.8634 - auROC: 0.8682 - val_loss: 0.4906 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4593 - acc: 0.8906 - auROC: 0.8931\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4962 - acc: 0.8602 - auROC: 0.8674 - val_loss: 0.4910 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4952 - acc: 0.8602 - auROC: 0.8681 - val_loss: 0.4908 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4951 - acc: 0.8602 - auROC: 0.8682 - val_loss: 0.4905 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4950 - acc: 0.8602 - auROC: 0.8683 - val_loss: 0.4902 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4949 - acc: 0.8602 - auROC: 0.8683 - val_loss: 0.4900 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4988 - acc: 0.8359 - auROC: 0.8693\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4948 - acc: 0.8602 - auROC: 0.8684 - val_loss: 0.4898 - val_acc: 0.8611 - val_auROC: 0.9136\n",
      "Epoch 00041: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 41/340\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.5295 - acc: 0.7267 - auROC: 0.8606 - val_loss: 0.4898 - val_acc: 0.8333 - val_auROC: 0.9336\n",
      "Epoch 42/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5246 - acc: 0.7360 - auROC: 0.8640 - val_loss: 0.4951 - val_acc: 0.8056 - val_auROC: 0.9275\n",
      "Epoch 43/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5226 - acc: 0.7360 - auROC: 0.8646 - val_loss: 0.4958 - val_acc: 0.8056 - val_auROC: 0.9290\n",
      "Epoch 44/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5204 - acc: 0.7391 - auROC: 0.8673 - val_loss: 0.4956 - val_acc: 0.8333 - val_auROC: 0.9259\n",
      "Epoch 45/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5180 - acc: 0.7453 - auROC: 0.8711 - val_loss: 0.4948 - val_acc: 0.8056 - val_auROC: 0.9259\n",
      "Epoch 46/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5162 - acc: 0.7516 - auROC: 0.8735 - val_loss: 0.4933 - val_acc: 0.8056 - val_auROC: 0.9228\n",
      "Epoch 47/340\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5136 - acc: 0.7547 - auROC: 0.8742 - val_loss: 0.4921 - val_acc: 0.8056 - val_auROC: 0.9259\n",
      "Epoch 48/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5118 - acc: 0.7516 - auROC: 0.8756 - val_loss: 0.4921 - val_acc: 0.8056 - val_auROC: 0.9198\n",
      "Epoch 49/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5102 - acc: 0.7516 - auROC: 0.8779 - val_loss: 0.4920 - val_acc: 0.8056 - val_auROC: 0.9182\n",
      "Epoch 50/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5085 - acc: 0.7516 - auROC: 0.8799 - val_loss: 0.4918 - val_acc: 0.8056 - val_auROC: 0.9167\n",
      "Epoch 51/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5067 - acc: 0.7547 - auROC: 0.8814 - val_loss: 0.4913 - val_acc: 0.8056 - val_auROC: 0.9182\n",
      "Epoch 52/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5054 - acc: 0.7516 - auROC: 0.8819 - val_loss: 0.4914 - val_acc: 0.8056 - val_auROC: 0.9167\n",
      "Epoch 53/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5035 - acc: 0.7484 - auROC: 0.8827 - val_loss: 0.4903 - val_acc: 0.8056 - val_auROC: 0.9198\n",
      "Epoch 54/340\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5022 - acc: 0.7484 - auROC: 0.8835 - val_loss: 0.4888 - val_acc: 0.8056 - val_auROC: 0.9228\n",
      "Epoch 55/340\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5008 - acc: 0.7422 - auROC: 0.8849 - val_loss: 0.4879 - val_acc: 0.8056 - val_auROC: 0.9228\n",
      "Epoch 56/340\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4996 - acc: 0.7391 - auROC: 0.8855 - val_loss: 0.4868 - val_acc: 0.7778 - val_auROC: 0.9228\n",
      "Epoch 57/340\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4983 - acc: 0.7453 - auROC: 0.8863 - val_loss: 0.4853 - val_acc: 0.8056 - val_auROC: 0.9259\n",
      "Epoch 58/340\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4966 - acc: 0.7422 - auROC: 0.8877 - val_loss: 0.4838 - val_acc: 0.8056 - val_auROC: 0.9259\n",
      "Epoch 59/340\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4953 - acc: 0.7391 - auROC: 0.8883 - val_loss: 0.4824 - val_acc: 0.7778 - val_auROC: 0.9259\n",
      "Epoch 60/340\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4940 - acc: 0.7391 - auROC: 0.8900 - val_loss: 0.4820 - val_acc: 0.7778 - val_auROC: 0.9336\n",
      "Epoch 61/340\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4928 - acc: 0.7422 - auROC: 0.8921 - val_loss: 0.4818 - val_acc: 0.7778 - val_auROC: 0.9336\n",
      "Epoch 62/340\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4919 - acc: 0.7422 - auROC: 0.8935 - val_loss: 0.4814 - val_acc: 0.7778 - val_auROC: 0.9352\n",
      "Epoch 63/340\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4904 - acc: 0.7422 - auROC: 0.8949 - val_loss: 0.4808 - val_acc: 0.7778 - val_auROC: 0.9352\n",
      "Epoch 64/340\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4891 - acc: 0.7391 - auROC: 0.8959 - val_loss: 0.4805 - val_acc: 0.7500 - val_auROC: 0.9321\n",
      "Epoch 65/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4882 - acc: 0.7391 - auROC: 0.8959 - val_loss: 0.4809 - val_acc: 0.7500 - val_auROC: 0.9321\n",
      "Epoch 66/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4874 - acc: 0.7360 - auROC: 0.8962 - val_loss: 0.4812 - val_acc: 0.7500 - val_auROC: 0.9306\n",
      "Epoch 67/340\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4863 - acc: 0.7360 - auROC: 0.8978 - val_loss: 0.4814 - val_acc: 0.7500 - val_auROC: 0.9290\n",
      "Epoch 68/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4854 - acc: 0.7329 - auROC: 0.8995 - val_loss: 0.4817 - val_acc: 0.7500 - val_auROC: 0.9290\n",
      "Epoch 69/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4845 - acc: 0.7329 - auROC: 0.9002 - val_loss: 0.4826 - val_acc: 0.7500 - val_auROC: 0.9275\n",
      "Epoch 70/340\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4837 - acc: 0.7329 - auROC: 0.9017 - val_loss: 0.4839 - val_acc: 0.7500 - val_auROC: 0.9198\n",
      "Epoch 71/340\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4827 - acc: 0.7329 - auROC: 0.9025 - val_loss: 0.4879 - val_acc: 0.7500 - val_auROC: 0.9198\n",
      "Epoch 72/340\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4816 - acc: 0.7329 - auROC: 0.9036 - val_loss: 0.5053 - val_acc: 0.7222 - val_auROC: 0.8843\n",
      "Epoch 73/340\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4796 - acc: 0.7360 - auROC: 0.9079 - val_loss: 0.5172 - val_acc: 0.7222 - val_auROC: 0.8657\n",
      "Epoch 74/340\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4771 - acc: 0.7391 - auROC: 0.9092 - val_loss: 0.5155 - val_acc: 0.7222 - val_auROC: 0.8750\n",
      "Epoch 75/340\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4765 - acc: 0.7391 - auROC: 0.9096 - val_loss: 0.5129 - val_acc: 0.7222 - val_auROC: 0.8781\n",
      "Epoch 76/340\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4752 - acc: 0.7391 - auROC: 0.9107 - val_loss: 0.5103 - val_acc: 0.7222 - val_auROC: 0.8812\n",
      "Epoch 77/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4745 - acc: 0.7391 - auROC: 0.9111 - val_loss: 0.5081 - val_acc: 0.7500 - val_auROC: 0.8904\n",
      "Epoch 78/340\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4734 - acc: 0.7453 - auROC: 0.9113 - val_loss: 0.5060 - val_acc: 0.7500 - val_auROC: 0.8997\n",
      "Epoch 79/340\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4724 - acc: 0.7453 - auROC: 0.9111Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.4724 - acc: 0.7453 - auROC: 0.9111 - val_loss: 0.5039 - val_acc: 0.7500 - val_auROC: 0.9012\n",
      "Epoch 00079: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.283793  ...    0.0  1.354979 -0.184262    0.0\n",
      "1     0.0    0.0    0.0 -0.292515  ...    0.0 -0.233364 -0.184262    0.0\n",
      "2     0.0    0.0    0.0  0.063583  ...    0.0 -0.259460 -0.184262    0.0\n",
      "3     0.0    0.0    0.0 -0.019061  ...    0.0  0.257945 -0.184262    0.0\n",
      "4     0.0    0.0    0.0  0.404746  ...    0.0  0.003764 -0.184262    0.0\n",
      "5     0.0    0.0    0.0 -0.292515  ...    0.0 -0.259460 -0.184262    0.0\n",
      "6     0.0    0.0    0.0 -0.292515  ...    0.0 -0.106056 -0.184262    0.0\n",
      "7     0.0    0.0    0.0  0.626600  ...    0.0  0.148287 -0.184262    0.0\n",
      "8     0.0    0.0    0.0 -0.292515  ...    0.0 -0.259460 -0.184262    0.0\n",
      "9     0.0    0.0    0.0 -0.292515  ...    0.0 -0.259460 -0.184262    0.0\n",
      "10    0.0    0.0    0.0 -0.292515  ...    0.0 -0.221000 -0.184262    0.0\n",
      "11    0.0    0.0    0.0 -0.280901  ...    0.0 -0.259460 -0.184262    0.0\n",
      "12    0.0    0.0    0.0 -0.290485  ...    0.0 -0.259460 -0.184262    0.0\n",
      "13    0.0    0.0    0.0 -0.258119  ...    0.0 -0.211229 -0.184262    0.0\n",
      "14    0.0    0.0    0.0 -0.292515  ...    0.0 -0.259460 -0.184262    0.0\n",
      "15    0.0    0.0    0.0  2.510405  ...    0.0 -0.259460 -0.184262    0.0\n",
      "16    0.0    0.0    0.0 -0.292515  ...    0.0 -0.259460 -0.184262    0.0\n",
      "17    0.0    0.0    0.0 -0.153867  ...    0.0 -0.201550 -0.184262    0.0\n",
      "18    0.0    0.0    0.0 -0.286472  ...    0.0 -0.259460 -0.184262    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "   0      1      2         3      ...  18014     18015     18016  18017\n",
      "0    0.0    0.0    0.0 -0.259581  ...    0.0 -0.259460 -0.184262    0.0\n",
      "1    0.0    0.0    0.0  0.250457  ...    0.0 -0.222957 -0.184262    0.0\n",
      "2    0.0    0.0    0.0 -0.281583  ...    0.0 -0.259460 -0.184262    0.0\n",
      "3    0.0    0.0    0.0 -0.275717  ...    0.0 -0.259460 -0.184262    0.0\n",
      "4    0.0    0.0    0.0 -0.273389  ...    0.0 -0.179004 -0.184262    0.0\n",
      "\n",
      "[5 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 179\n",
      "Total correct samples: 179?179\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.007213  0.024885\n",
      "4      0.007137  0.024885\n",
      "...         ...       ...\n",
      "18013  0.000098  0.000250\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000335  0.001282\n",
      "18016  0.000014  0.000084\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.8438 - acc: 0.4565 - auROC: 0.5598 - val_loss: 0.7261 - val_acc: 0.5000 - val_auROC: 0.6204\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.7607 - acc: 0.4503 - auROC: 0.4771 - val_loss: 0.6896 - val_acc: 0.4722 - val_auROC: 0.5849\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7217 - acc: 0.4596 - auROC: 0.5582 - val_loss: 0.7132 - val_acc: 0.4167 - val_auROC: 0.6219\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6785 - acc: 0.4876 - auROC: 0.6827 - val_loss: 0.6910 - val_acc: 0.5000 - val_auROC: 0.6204\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6567 - acc: 0.5466 - auROC: 0.7124 - val_loss: 0.6699 - val_acc: 0.4722 - val_auROC: 0.6559\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6434 - acc: 0.5714 - auROC: 0.7414 - val_loss: 0.6606 - val_acc: 0.5278 - val_auROC: 0.6620\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6343 - acc: 0.5963 - auROC: 0.7557 - val_loss: 0.6613 - val_acc: 0.5278 - val_auROC: 0.6559\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6253 - acc: 0.5994 - auROC: 0.7665 - val_loss: 0.6652 - val_acc: 0.5278 - val_auROC: 0.6497\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6237 - acc: 0.5901 - auROC: 0.7695 - val_loss: 0.6696 - val_acc: 0.5000 - val_auROC: 0.6265\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6159 - acc: 0.6211 - auROC: 0.7789 - val_loss: 0.6545 - val_acc: 0.5833 - val_auROC: 0.6944\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6081 - acc: 0.6615 - auROC: 0.7990 - val_loss: 0.6416 - val_acc: 0.5278 - val_auROC: 0.7114\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.6038 - acc: 0.6739 - auROC: 0.8048 - val_loss: 0.6392 - val_acc: 0.5278 - val_auROC: 0.7238\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5967 - acc: 0.6677 - auROC: 0.8153 - val_loss: 0.6365 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.5933 - acc: 0.6739 - auROC: 0.8156 - val_loss: 0.6308 - val_acc: 0.5556 - val_auROC: 0.7593\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5889 - acc: 0.6801 - auROC: 0.8252 - val_loss: 0.6252 - val_acc: 0.5556 - val_auROC: 0.7593\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5846 - acc: 0.6739 - auROC: 0.8288 - val_loss: 0.6197 - val_acc: 0.5833 - val_auROC: 0.7593\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5786 - acc: 0.6957 - auROC: 0.8389 - val_loss: 0.6131 - val_acc: 0.6389 - val_auROC: 0.7654\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5740 - acc: 0.7112 - auROC: 0.8428 - val_loss: 0.6114 - val_acc: 0.6389 - val_auROC: 0.7623\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5733 - acc: 0.7050 - auROC: 0.8429 - val_loss: 0.6044 - val_acc: 0.6389 - val_auROC: 0.7747\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5667 - acc: 0.7050 - auROC: 0.8467 - val_loss: 0.5956 - val_acc: 0.6667 - val_auROC: 0.7793\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5624 - acc: 0.6863 - auROC: 0.8433 - val_loss: 0.5947 - val_acc: 0.6389 - val_auROC: 0.7870\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5592 - acc: 0.7019 - auROC: 0.8431 - val_loss: 0.5922 - val_acc: 0.6389 - val_auROC: 0.7762\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5528 - acc: 0.7050 - auROC: 0.8461 - val_loss: 0.5883 - val_acc: 0.6389 - val_auROC: 0.7886\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5464 - acc: 0.7174 - auROC: 0.8543 - val_loss: 0.5886 - val_acc: 0.5833 - val_auROC: 0.7870\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5425 - acc: 0.7174 - auROC: 0.8588 - val_loss: 0.5863 - val_acc: 0.6389 - val_auROC: 0.7886\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5355 - acc: 0.7391 - auROC: 0.8582 - val_loss: 0.5912 - val_acc: 0.6667 - val_auROC: 0.7824\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5344 - acc: 0.7329 - auROC: 0.8565 - val_loss: 0.5809 - val_acc: 0.6667 - val_auROC: 0.7855\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5252 - acc: 0.7236 - auROC: 0.8578 - val_loss: 0.5774 - val_acc: 0.6389 - val_auROC: 0.8009\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5199 - acc: 0.7298 - auROC: 0.8578 - val_loss: 0.5727 - val_acc: 0.6389 - val_auROC: 0.7932\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5123 - acc: 0.7981 - auROC: 0.8656 - val_loss: 0.5730 - val_acc: 0.7500 - val_auROC: 0.7948\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.5071 - acc: 0.8354 - auROC: 0.8668 - val_loss: 0.5724 - val_acc: 0.7500 - val_auROC: 0.7917\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4983 - acc: 0.8354 - auROC: 0.8678 - val_loss: 0.5689 - val_acc: 0.7222 - val_auROC: 0.8071\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4940 - acc: 0.8385 - auROC: 0.8656 - val_loss: 0.5666 - val_acc: 0.7500 - val_auROC: 0.7963\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4864 - acc: 0.8416 - auROC: 0.8751 - val_loss: 0.5602 - val_acc: 0.7500 - val_auROC: 0.8117\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4812 - acc: 0.8602 - auROC: 0.8806 - val_loss: 0.5567 - val_acc: 0.7222 - val_auROC: 0.8225\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4797 - acc: 0.8602 - auROC: 0.8756 - val_loss: 0.5552 - val_acc: 0.7500 - val_auROC: 0.8164\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4713 - acc: 0.8634 - auROC: 0.8835 - val_loss: 0.5635 - val_acc: 0.7500 - val_auROC: 0.8071\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4716 - acc: 0.8447 - auROC: 0.8845 - val_loss: 0.5524 - val_acc: 0.7500 - val_auROC: 0.8210\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4631 - acc: 0.8696 - auROC: 0.8879 - val_loss: 0.5471 - val_acc: 0.7778 - val_auROC: 0.8210\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4613 - acc: 0.8696 - auROC: 0.8881 - val_loss: 0.5506 - val_acc: 0.7500 - val_auROC: 0.8071\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4565 - acc: 0.8727 - auROC: 0.8949 - val_loss: 0.5548 - val_acc: 0.7500 - val_auROC: 0.7994\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4544 - acc: 0.8634 - auROC: 0.8915 - val_loss: 0.5596 - val_acc: 0.7500 - val_auROC: 0.7932\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4511 - acc: 0.8509 - auROC: 0.8926 - val_loss: 0.5560 - val_acc: 0.7500 - val_auROC: 0.8056\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4457 - acc: 0.8758 - auROC: 0.8944 - val_loss: 0.5453 - val_acc: 0.7778 - val_auROC: 0.8133\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4438 - acc: 0.8758 - auROC: 0.8940 - val_loss: 0.5479 - val_acc: 0.7778 - val_auROC: 0.8040\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4396 - acc: 0.8789 - auROC: 0.8967 - val_loss: 0.5589 - val_acc: 0.7500 - val_auROC: 0.8056\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4387 - acc: 0.8634 - auROC: 0.8963 - val_loss: 0.5570 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4348 - acc: 0.8820 - auROC: 0.8955 - val_loss: 0.5536 - val_acc: 0.7778 - val_auROC: 0.8009\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4398 - acc: 0.8828 - auROC: 0.8805\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4364 - acc: 0.8789 - auROC: 0.8922 - val_loss: 0.5631 - val_acc: 0.7500 - val_auROC: 0.7948\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4314 - acc: 0.8882 - auROC: 0.8959 - val_loss: 0.5601 - val_acc: 0.7500 - val_auROC: 0.7978\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4297 - acc: 0.8913 - auROC: 0.8979 - val_loss: 0.5554 - val_acc: 0.7500 - val_auROC: 0.8009\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4278 - acc: 0.8882 - auROC: 0.9003 - val_loss: 0.5510 - val_acc: 0.7500 - val_auROC: 0.8056\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4275 - acc: 0.8820 - auROC: 0.8997 - val_loss: 0.5497 - val_acc: 0.7500 - val_auROC: 0.8025\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4778 - acc: 0.8281 - auROC: 0.8450\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4273 - acc: 0.8820 - auROC: 0.8984 - val_loss: 0.5497 - val_acc: 0.7778 - val_auROC: 0.8009\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4265 - acc: 0.8820 - auROC: 0.8987 - val_loss: 0.5498 - val_acc: 0.7778 - val_auROC: 0.8009\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4263 - acc: 0.8820 - auROC: 0.8987 - val_loss: 0.5500 - val_acc: 0.7500 - val_auROC: 0.8009\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4258 - acc: 0.8820 - auROC: 0.8993 - val_loss: 0.5503 - val_acc: 0.7500 - val_auROC: 0.8009\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4254 - acc: 0.8820 - auROC: 0.9009 - val_loss: 0.5506 - val_acc: 0.7500 - val_auROC: 0.8025\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3849 - acc: 0.9219 - auROC: 0.9298\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4250 - acc: 0.8820 - auROC: 0.9021 - val_loss: 0.5510 - val_acc: 0.7500 - val_auROC: 0.8040\n",
      "Epoch 00059: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 59/358\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.4436 - acc: 0.8789 - auROC: 0.8964 - val_loss: 0.5485 - val_acc: 0.7778 - val_auROC: 0.8117\n",
      "Epoch 60/358\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4429 - acc: 0.8820 - auROC: 0.8970 - val_loss: 0.5496 - val_acc: 0.7500 - val_auROC: 0.8133\n",
      "Epoch 61/358\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4400 - acc: 0.8882 - auROC: 0.8982 - val_loss: 0.5473 - val_acc: 0.7778 - val_auROC: 0.8179\n",
      "Epoch 62/358\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4378 - acc: 0.8851 - auROC: 0.9003 - val_loss: 0.5481 - val_acc: 0.7778 - val_auROC: 0.8164\n",
      "Epoch 63/358\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4357 - acc: 0.8820 - auROC: 0.9021 - val_loss: 0.5488 - val_acc: 0.7778 - val_auROC: 0.8133\n",
      "Epoch 64/358\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4312 - acc: 0.8820 - auROC: 0.9052 - val_loss: 0.5494 - val_acc: 0.7778 - val_auROC: 0.8148\n",
      "Epoch 65/358\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4287 - acc: 0.8851 - auROC: 0.9060 - val_loss: 0.5497 - val_acc: 0.7778 - val_auROC: 0.8117\n",
      "Epoch 66/358\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4249 - acc: 0.8975 - auROC: 0.9081 - val_loss: 0.5491 - val_acc: 0.7778 - val_auROC: 0.8148\n",
      "Epoch 67/358\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4228 - acc: 0.8975 - auROC: 0.9101 - val_loss: 0.5494 - val_acc: 0.7778 - val_auROC: 0.8102\n",
      "Epoch 68/358\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4207 - acc: 0.8975 - auROC: 0.9112 - val_loss: 0.5496 - val_acc: 0.7778 - val_auROC: 0.8117\n",
      "Epoch 69/358\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4187 - acc: 0.9006 - auROC: 0.9122 - val_loss: 0.5502 - val_acc: 0.7778 - val_auROC: 0.8071\n",
      "Epoch 70/358\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4181 - acc: 0.9006 - auROC: 0.9124 - val_loss: 0.5507 - val_acc: 0.7500 - val_auROC: 0.8102\n",
      "Epoch 71/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4173 - acc: 0.9006 - auROC: 0.9136 - val_loss: 0.5485 - val_acc: 0.7778 - val_auROC: 0.8117\n",
      "Epoch 72/358\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4155 - acc: 0.9068 - auROC: 0.9143 - val_loss: 0.5463 - val_acc: 0.7778 - val_auROC: 0.8102\n",
      "Epoch 73/358\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4139 - acc: 0.9099 - auROC: 0.9138 - val_loss: 0.5449 - val_acc: 0.7778 - val_auROC: 0.8148\n",
      "Epoch 74/358\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4129 - acc: 0.9099 - auROC: 0.9142 - val_loss: 0.5436 - val_acc: 0.7778 - val_auROC: 0.8133\n",
      "Epoch 75/358\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4120 - acc: 0.9099 - auROC: 0.9145 - val_loss: 0.5423 - val_acc: 0.7778 - val_auROC: 0.8179\n",
      "Epoch 76/358\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.4110 - acc: 0.9099 - auROC: 0.9154 - val_loss: 0.5411 - val_acc: 0.7778 - val_auROC: 0.8194\n",
      "Epoch 77/358\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4102 - acc: 0.9099 - auROC: 0.9161 - val_loss: 0.5399 - val_acc: 0.7778 - val_auROC: 0.8225\n",
      "Epoch 78/358\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.4093 - acc: 0.9099 - auROC: 0.9166 - val_loss: 0.5387 - val_acc: 0.7778 - val_auROC: 0.8210\n",
      "Epoch 79/358\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4082 - acc: 0.9099 - auROC: 0.9171 - val_loss: 0.5374 - val_acc: 0.7778 - val_auROC: 0.8210\n",
      "Epoch 80/358\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4072 - acc: 0.9099 - auROC: 0.9175 - val_loss: 0.5364 - val_acc: 0.7778 - val_auROC: 0.8225\n",
      "Epoch 81/358\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4068 - acc: 0.9099 - auROC: 0.9177 - val_loss: 0.5365 - val_acc: 0.7778 - val_auROC: 0.8225\n",
      "Epoch 82/358\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.4055 - acc: 0.9099 - auROC: 0.9204 - val_loss: 0.5358 - val_acc: 0.7778 - val_auROC: 0.8210\n",
      "Epoch 83/358\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.4041 - acc: 0.9099 - auROC: 0.9216 - val_loss: 0.5346 - val_acc: 0.7778 - val_auROC: 0.8241\n",
      "Epoch 84/358\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4024 - acc: 0.9130 - auROC: 0.9224 - val_loss: 0.5337 - val_acc: 0.7778 - val_auROC: 0.8272\n",
      "Epoch 85/358\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.4013 - acc: 0.9130 - auROC: 0.9224 - val_loss: 0.5328 - val_acc: 0.7778 - val_auROC: 0.8287\n",
      "Epoch 86/358\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.3998 - acc: 0.9161 - auROC: 0.9230 - val_loss: 0.5327 - val_acc: 0.7500 - val_auROC: 0.8272\n",
      "Epoch 87/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3987 - acc: 0.9161 - auROC: 0.9238 - val_loss: 0.5330 - val_acc: 0.7500 - val_auROC: 0.8256\n",
      "Epoch 88/358\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3979 - acc: 0.9161 - auROC: 0.9264 - val_loss: 0.5321 - val_acc: 0.7500 - val_auROC: 0.8256\n",
      "Epoch 89/358\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3969 - acc: 0.9161 - auROC: 0.9264 - val_loss: 0.5294 - val_acc: 0.7778 - val_auROC: 0.8302\n",
      "Epoch 90/358\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3954 - acc: 0.9161 - auROC: 0.9268 - val_loss: 0.5275 - val_acc: 0.7778 - val_auROC: 0.8302\n",
      "Epoch 91/358\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3944 - acc: 0.9161 - auROC: 0.9273 - val_loss: 0.5257 - val_acc: 0.7778 - val_auROC: 0.8349\n",
      "Epoch 92/358\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3933 - acc: 0.9161 - auROC: 0.9280 - val_loss: 0.5243 - val_acc: 0.7500 - val_auROC: 0.8395\n",
      "Epoch 93/358\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.3915 - acc: 0.9224 - auROC: 0.9282 - val_loss: 0.5230 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 94/358\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.3908 - acc: 0.9224 - auROC: 0.9292 - val_loss: 0.5216 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 95/358\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.3900 - acc: 0.9224 - auROC: 0.9293 - val_loss: 0.5204 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 96/358\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.3893 - acc: 0.9224 - auROC: 0.9292 - val_loss: 0.5195 - val_acc: 0.7500 - val_auROC: 0.8426\n",
      "Epoch 97/358\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3882 - acc: 0.9224 - auROC: 0.9299 - val_loss: 0.5197 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 98/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3876 - acc: 0.9224 - auROC: 0.9302 - val_loss: 0.5200 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 99/358\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3869 - acc: 0.9224 - auROC: 0.9306 - val_loss: 0.5209 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 100/358\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3864 - acc: 0.9224 - auROC: 0.9307 - val_loss: 0.5217 - val_acc: 0.7500 - val_auROC: 0.8410\n",
      "Epoch 101/358\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3860 - acc: 0.9224 - auROC: 0.9310 - val_loss: 0.5229 - val_acc: 0.7500 - val_auROC: 0.8426\n",
      "Epoch 102/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3856 - acc: 0.9224 - auROC: 0.9313 - val_loss: 0.5245 - val_acc: 0.7500 - val_auROC: 0.8333\n",
      "Epoch 103/358\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.3851 - acc: 0.9224 - auROC: 0.9317 - val_loss: 0.5253 - val_acc: 0.7500 - val_auROC: 0.8318\n",
      "Epoch 104/358\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.3848 - acc: 0.9224 - auROC: 0.9329 - val_loss: 0.5259 - val_acc: 0.7500 - val_auROC: 0.8318\n",
      "Epoch 105/358\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3845 - acc: 0.9224 - auROC: 0.9332 - val_loss: 0.5266 - val_acc: 0.7778 - val_auROC: 0.8318\n",
      "Epoch 106/358\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3842 - acc: 0.9224 - auROC: 0.9335 - val_loss: 0.5273 - val_acc: 0.7778 - val_auROC: 0.8349\n",
      "Epoch 107/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3838 - acc: 0.9224 - auROC: 0.9336 - val_loss: 0.5285 - val_acc: 0.7500 - val_auROC: 0.8333\n",
      "Epoch 108/358\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3834 - acc: 0.9224 - auROC: 0.9335 - val_loss: 0.5299 - val_acc: 0.7500 - val_auROC: 0.8349\n",
      "Epoch 109/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3831 - acc: 0.9224 - auROC: 0.9339 - val_loss: 0.5303 - val_acc: 0.7500 - val_auROC: 0.8349\n",
      "Epoch 110/358\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3829 - acc: 0.9224 - auROC: 0.9342 - val_loss: 0.5301 - val_acc: 0.7500 - val_auROC: 0.8349\n",
      "Epoch 111/358\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3826 - acc: 0.9224 - auROC: 0.9342Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.3826 - acc: 0.9224 - auROC: 0.9342 - val_loss: 0.5301 - val_acc: 0.7500 - val_auROC: 0.8349\n",
      "Epoch 00111: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.278006  ...    0.0  0.301669 -0.168854    0.0\n",
      "1     0.0    0.0    0.0 -0.281070  ...    0.0 -0.175583 -0.168854    0.0\n",
      "2     0.0    0.0    0.0 -0.253405  ...    0.0 -0.164453 -0.168854    0.0\n",
      "3     0.0    0.0    0.0 -0.014375  ...    0.0  0.254334 -0.168854    0.0\n",
      "4     0.0    0.0    0.0  2.612849  ...    0.0 -0.133353 -0.168854    0.0\n",
      "5     0.0    0.0    0.0  0.212227  ...    0.0  0.602818  4.071401    0.0\n",
      "6     0.0    0.0    0.0 -0.289866  ...    0.0 -0.260935 -0.168854    0.0\n",
      "7     0.0    0.0    0.0  1.675378  ...    0.0 -0.210019 -0.168854    0.0\n",
      "8     0.0    0.0    0.0 -0.268588  ...    0.0 -0.185861 -0.168854    0.0\n",
      "9     0.0    0.0    0.0 -0.289866  ...    0.0 -0.260935 -0.168854    0.0\n",
      "10    0.0    0.0    0.0 -0.276072  ...    0.0 -0.260935 -0.168854    0.0\n",
      "11    0.0    0.0    0.0 -0.289866  ...    0.0 -0.260935 -0.168854    0.0\n",
      "12    0.0    0.0    0.0 -0.254403  ...    0.0 -0.211780 -0.168854    0.0\n",
      "13    0.0    0.0    0.0 -0.289866  ...    0.0 -0.260935 -0.168854    0.0\n",
      "14    0.0    0.0    0.0 -0.289866  ...    0.0 -0.260935 -0.168854    0.0\n",
      "15    0.0    0.0    0.0 -0.282808  ...    0.0 -0.192453 -0.168854    0.0\n",
      "16    0.0    0.0    0.0  0.044682  ...    0.0 -0.097273 -0.168854    0.0\n",
      "17    0.0    0.0    0.0 -0.278460  ...    0.0 -0.260935 -0.168854    0.0\n",
      "18    0.0    0.0    0.0 -0.262455  ...    0.0 -0.260935 -0.168854    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "   0      1      2         3      ...  18014     18015     18016  18017\n",
      "0    0.0    0.0    0.0 -0.256686  ...    0.0 -0.260935 -0.168854    0.0\n",
      "1    0.0    0.0    0.0  0.257151  ...    0.0 -0.224582 -0.168854    0.0\n",
      "2    0.0    0.0    0.0 -0.278853  ...    0.0 -0.260935 -0.168854    0.0\n",
      "3    0.0    0.0    0.0 -0.272943  ...    0.0 -0.260935 -0.168854    0.0\n",
      "4    0.0    0.0    0.0 -0.270598  ...    0.0 -0.180811 -0.168854    0.0\n",
      "\n",
      "[5 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 179\n",
      "Total correct samples: 179?179\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.007656  0.025282\n",
      "4      0.007576  0.025284\n",
      "...         ...       ...\n",
      "18013  0.000119  0.000326\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000297  0.001151\n",
      "18016  0.000015  0.000087\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.8312 - acc: 0.4720 - auROC: 0.5764 - val_loss: 0.8396 - val_acc: 0.3889 - val_auROC: 0.4506\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.7374 - acc: 0.4596 - auROC: 0.5284 - val_loss: 0.7891 - val_acc: 0.3889 - val_auROC: 0.4784\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.7163 - acc: 0.4938 - auROC: 0.5835 - val_loss: 0.7218 - val_acc: 0.3611 - val_auROC: 0.5077\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6962 - acc: 0.5031 - auROC: 0.6491 - val_loss: 0.6908 - val_acc: 0.3889 - val_auROC: 0.5463\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6708 - acc: 0.5621 - auROC: 0.7348 - val_loss: 0.6723 - val_acc: 0.3889 - val_auROC: 0.5725\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.6519 - acc: 0.6056 - auROC: 0.7615 - val_loss: 0.6511 - val_acc: 0.4444 - val_auROC: 0.6512\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.6386 - acc: 0.6149 - auROC: 0.7860 - val_loss: 0.6364 - val_acc: 0.4444 - val_auROC: 0.6605\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6302 - acc: 0.6304 - auROC: 0.7952 - val_loss: 0.6292 - val_acc: 0.4722 - val_auROC: 0.6898\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6248 - acc: 0.6677 - auROC: 0.7964 - val_loss: 0.6199 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6215 - acc: 0.6894 - auROC: 0.7941 - val_loss: 0.6203 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6165 - acc: 0.7112 - auROC: 0.7994 - val_loss: 0.6236 - val_acc: 0.5556 - val_auROC: 0.7191\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6120 - acc: 0.7174 - auROC: 0.8123 - val_loss: 0.6353 - val_acc: 0.5278 - val_auROC: 0.7068\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6073 - acc: 0.7267 - auROC: 0.8292 - val_loss: 0.6374 - val_acc: 0.5278 - val_auROC: 0.6929\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6144 - acc: 0.7266 - auROC: 0.8054\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6017 - acc: 0.7640 - auROC: 0.8365 - val_loss: 0.6302 - val_acc: 0.5556 - val_auROC: 0.7052\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5980 - acc: 0.7826 - auROC: 0.8431 - val_loss: 0.6297 - val_acc: 0.5556 - val_auROC: 0.7052\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.5974 - acc: 0.7857 - auROC: 0.8445 - val_loss: 0.6304 - val_acc: 0.5556 - val_auROC: 0.7052\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5967 - acc: 0.7826 - auROC: 0.8453 - val_loss: 0.6314 - val_acc: 0.5556 - val_auROC: 0.7068\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5958 - acc: 0.7857 - auROC: 0.8458 - val_loss: 0.6320 - val_acc: 0.5556 - val_auROC: 0.7006\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6141 - acc: 0.7266 - auROC: 0.7743\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5950 - acc: 0.7888 - auROC: 0.8461 - val_loss: 0.6326 - val_acc: 0.5556 - val_auROC: 0.7006\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5945 - acc: 0.7857 - auROC: 0.8455 - val_loss: 0.6327 - val_acc: 0.5556 - val_auROC: 0.7006\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5944 - acc: 0.7888 - auROC: 0.8454 - val_loss: 0.6327 - val_acc: 0.5556 - val_auROC: 0.7006\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5944 - acc: 0.7888 - auROC: 0.8452 - val_loss: 0.6327 - val_acc: 0.5556 - val_auROC: 0.7006\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5943 - acc: 0.7888 - auROC: 0.8453 - val_loss: 0.6328 - val_acc: 0.5556 - val_auROC: 0.7006\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6121 - acc: 0.7266 - auROC: 0.8071\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5942 - acc: 0.7888 - auROC: 0.8457 - val_loss: 0.6329 - val_acc: 0.5556 - val_auROC: 0.6991\n",
      "Epoch 00024: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 24/323\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.6221 - acc: 0.6770 - auROC: 0.7961 - val_loss: 0.6215 - val_acc: 0.5278 - val_auROC: 0.7130\n",
      "Epoch 25/323\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6181 - acc: 0.6894 - auROC: 0.8020 - val_loss: 0.6216 - val_acc: 0.5278 - val_auROC: 0.7114\n",
      "Epoch 26/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6153 - acc: 0.6894 - auROC: 0.8118 - val_loss: 0.6216 - val_acc: 0.5000 - val_auROC: 0.7207\n",
      "Epoch 27/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.6125 - acc: 0.6925 - auROC: 0.8227 - val_loss: 0.6211 - val_acc: 0.5000 - val_auROC: 0.7207\n",
      "Epoch 28/323\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.6106 - acc: 0.7081 - auROC: 0.8282 - val_loss: 0.6203 - val_acc: 0.5000 - val_auROC: 0.7191\n",
      "Epoch 29/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.6082 - acc: 0.7112 - auROC: 0.8337 - val_loss: 0.6191 - val_acc: 0.5000 - val_auROC: 0.7191\n",
      "Epoch 30/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.6058 - acc: 0.7081 - auROC: 0.8387 - val_loss: 0.6179 - val_acc: 0.5278 - val_auROC: 0.7222\n",
      "Epoch 31/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.6036 - acc: 0.7143 - auROC: 0.8403 - val_loss: 0.6172 - val_acc: 0.5278 - val_auROC: 0.7222\n",
      "Epoch 32/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.6022 - acc: 0.7205 - auROC: 0.8411 - val_loss: 0.6166 - val_acc: 0.5278 - val_auROC: 0.7253\n",
      "Epoch 33/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.6003 - acc: 0.7205 - auROC: 0.8430 - val_loss: 0.6163 - val_acc: 0.5278 - val_auROC: 0.7269\n",
      "Epoch 34/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5989 - acc: 0.7267 - auROC: 0.8459 - val_loss: 0.6161 - val_acc: 0.5278 - val_auROC: 0.7253\n",
      "Epoch 35/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5976 - acc: 0.7267 - auROC: 0.8502 - val_loss: 0.6159 - val_acc: 0.5278 - val_auROC: 0.7284\n",
      "Epoch 36/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5962 - acc: 0.7298 - auROC: 0.8538 - val_loss: 0.6159 - val_acc: 0.5278 - val_auROC: 0.7284\n",
      "Epoch 37/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5950 - acc: 0.7298 - auROC: 0.8542 - val_loss: 0.6159 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 38/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5939 - acc: 0.7391 - auROC: 0.8558 - val_loss: 0.6159 - val_acc: 0.5556 - val_auROC: 0.7269\n",
      "Epoch 39/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5930 - acc: 0.7453 - auROC: 0.8567 - val_loss: 0.6158 - val_acc: 0.5556 - val_auROC: 0.7269\n",
      "Epoch 40/323\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5920 - acc: 0.7484 - auROC: 0.8571 - val_loss: 0.6157 - val_acc: 0.5556 - val_auROC: 0.7269\n",
      "Epoch 41/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5911 - acc: 0.7516 - auROC: 0.8571 - val_loss: 0.6155 - val_acc: 0.5556 - val_auROC: 0.7269\n",
      "Epoch 42/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5900 - acc: 0.7516 - auROC: 0.8577 - val_loss: 0.6153 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 43/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5892 - acc: 0.7578 - auROC: 0.8585 - val_loss: 0.6151 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 44/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5884 - acc: 0.7609 - auROC: 0.8603 - val_loss: 0.6150 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 45/323\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5875 - acc: 0.7609 - auROC: 0.8626 - val_loss: 0.6150 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 46/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5866 - acc: 0.7640 - auROC: 0.8623 - val_loss: 0.6150 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 47/323\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5856 - acc: 0.7671 - auROC: 0.8652 - val_loss: 0.6150 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 48/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5847 - acc: 0.7702 - auROC: 0.8671 - val_loss: 0.6150 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 49/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5837 - acc: 0.7702 - auROC: 0.8708 - val_loss: 0.6149 - val_acc: 0.5556 - val_auROC: 0.7299\n",
      "Epoch 50/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5827 - acc: 0.7733 - auROC: 0.8735 - val_loss: 0.6147 - val_acc: 0.5556 - val_auROC: 0.7299\n",
      "Epoch 51/323\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5816 - acc: 0.7826 - auROC: 0.8780 - val_loss: 0.6145 - val_acc: 0.5556 - val_auROC: 0.7299\n",
      "Epoch 52/323\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5802 - acc: 0.7919 - auROC: 0.8814 - val_loss: 0.6143 - val_acc: 0.5556 - val_auROC: 0.7299\n",
      "Epoch 53/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5787 - acc: 0.7981 - auROC: 0.8862 - val_loss: 0.6141 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 54/323\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5767 - acc: 0.8043 - auROC: 0.8897 - val_loss: 0.6140 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 55/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5752 - acc: 0.8106 - auROC: 0.8914 - val_loss: 0.6138 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 56/323\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5736 - acc: 0.8137 - auROC: 0.8926 - val_loss: 0.6136 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 57/323\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5727 - acc: 0.8168 - auROC: 0.8927 - val_loss: 0.6135 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 58/323\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.5718 - acc: 0.8168 - auROC: 0.8934 - val_loss: 0.6135 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 59/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5709 - acc: 0.8168 - auROC: 0.8947 - val_loss: 0.6136 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 60/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5701 - acc: 0.8168 - auROC: 0.8955 - val_loss: 0.6136 - val_acc: 0.5556 - val_auROC: 0.7284\n",
      "Epoch 61/323\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5692 - acc: 0.8199 - auROC: 0.8958 - val_loss: 0.6136 - val_acc: 0.5556 - val_auROC: 0.7222\n",
      "Epoch 62/323\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5684 - acc: 0.8199 - auROC: 0.8972 - val_loss: 0.6137 - val_acc: 0.5556 - val_auROC: 0.7238\n",
      "Epoch 63/323\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5676 - acc: 0.8230 - auROC: 0.8973 - val_loss: 0.6136 - val_acc: 0.5556 - val_auROC: 0.7238\n",
      "Epoch 64/323\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5668 - acc: 0.8230 - auROC: 0.8992 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7238\n",
      "Epoch 65/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5661 - acc: 0.8168 - auROC: 0.9020 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7238\n",
      "Epoch 66/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5653 - acc: 0.8199 - auROC: 0.9026 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7238\n",
      "Epoch 67/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5646 - acc: 0.8168 - auROC: 0.9036 - val_loss: 0.6136 - val_acc: 0.5833 - val_auROC: 0.7238\n",
      "Epoch 68/323\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5640 - acc: 0.8168 - auROC: 0.9039 - val_loss: 0.6136 - val_acc: 0.5556 - val_auROC: 0.7238\n",
      "Epoch 69/323\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.5632 - acc: 0.8168 - auROC: 0.9072 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 70/323\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5624 - acc: 0.8168 - auROC: 0.9102 - val_loss: 0.6134 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 71/323\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.5617 - acc: 0.8168 - auROC: 0.9109 - val_loss: 0.6134 - val_acc: 0.5833 - val_auROC: 0.7253\n",
      "Epoch 72/323\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5605 - acc: 0.8168 - auROC: 0.9149 - val_loss: 0.6134 - val_acc: 0.5833 - val_auROC: 0.7284\n",
      "Epoch 73/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5587 - acc: 0.8168 - auROC: 0.9159 - val_loss: 0.6133 - val_acc: 0.5833 - val_auROC: 0.7299\n",
      "Epoch 74/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5579 - acc: 0.8168 - auROC: 0.9157 - val_loss: 0.6133 - val_acc: 0.5833 - val_auROC: 0.7299\n",
      "Epoch 75/323\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5574 - acc: 0.8199 - auROC: 0.9164 - val_loss: 0.6134 - val_acc: 0.5833 - val_auROC: 0.7299\n",
      "Epoch 76/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5569 - acc: 0.8292 - auROC: 0.9173 - val_loss: 0.6134 - val_acc: 0.5833 - val_auROC: 0.7284\n",
      "Epoch 77/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5565 - acc: 0.8292 - auROC: 0.9175 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7284\n",
      "Epoch 78/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5561 - acc: 0.8292 - auROC: 0.9175 - val_loss: 0.6136 - val_acc: 0.5833 - val_auROC: 0.7284\n",
      "Epoch 79/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5557 - acc: 0.8323 - auROC: 0.9177 - val_loss: 0.6136 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 80/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5553 - acc: 0.8323 - auROC: 0.9189 - val_loss: 0.6137 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 81/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5550 - acc: 0.8354 - auROC: 0.9189 - val_loss: 0.6136 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 82/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5546 - acc: 0.8323 - auROC: 0.9207 - val_loss: 0.6136 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 83/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5543 - acc: 0.8323 - auROC: 0.9222 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 84/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5539 - acc: 0.8323 - auROC: 0.9223 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 85/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5537 - acc: 0.8323 - auROC: 0.9223 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7269\n",
      "Epoch 86/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5534 - acc: 0.8323 - auROC: 0.9224 - val_loss: 0.6135 - val_acc: 0.5833 - val_auROC: 0.7253\n",
      "Epoch 87/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5532 - acc: 0.8323 - auROC: 0.9226 - val_loss: 0.6134 - val_acc: 0.5833 - val_auROC: 0.7253\n",
      "Epoch 88/323\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5530 - acc: 0.8323 - auROC: 0.9227 - val_loss: 0.6133 - val_acc: 0.5833 - val_auROC: 0.7253\n",
      "Epoch 89/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5527 - acc: 0.8323 - auROC: 0.9229 - val_loss: 0.6132 - val_acc: 0.5833 - val_auROC: 0.7253\n",
      "Epoch 90/323\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5523 - acc: 0.8323 - auROC: 0.9245 - val_loss: 0.6131 - val_acc: 0.5833 - val_auROC: 0.7253\n",
      "Epoch 91/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5521 - acc: 0.8323 - auROC: 0.9259 - val_loss: 0.6130 - val_acc: 0.5833 - val_auROC: 0.7315\n",
      "Epoch 92/323\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.5519 - acc: 0.8292 - auROC: 0.9261 - val_loss: 0.6129 - val_acc: 0.5833 - val_auROC: 0.7346\n",
      "Epoch 93/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5516 - acc: 0.8323 - auROC: 0.9260 - val_loss: 0.6128 - val_acc: 0.5833 - val_auROC: 0.7315\n",
      "Epoch 94/323\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.5513 - acc: 0.8323 - auROC: 0.9284 - val_loss: 0.6126 - val_acc: 0.5833 - val_auROC: 0.7315\n",
      "Epoch 95/323\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5510 - acc: 0.8292 - auROC: 0.9289 - val_loss: 0.6125 - val_acc: 0.5833 - val_auROC: 0.7299\n",
      "Epoch 96/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5508 - acc: 0.8323 - auROC: 0.9290 - val_loss: 0.6124 - val_acc: 0.5833 - val_auROC: 0.7299\n",
      "Epoch 97/323\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5505 - acc: 0.8354 - auROC: 0.9310 - val_loss: 0.6122 - val_acc: 0.5833 - val_auROC: 0.7330\n",
      "Epoch 98/323\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.5501 - acc: 0.8416 - auROC: 0.9311 - val_loss: 0.6120 - val_acc: 0.5833 - val_auROC: 0.7330\n",
      "Epoch 99/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5495 - acc: 0.8447 - auROC: 0.9336 - val_loss: 0.6118 - val_acc: 0.5833 - val_auROC: 0.7330\n",
      "Epoch 100/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5491 - acc: 0.8478 - auROC: 0.9346 - val_loss: 0.6115 - val_acc: 0.5833 - val_auROC: 0.7330\n",
      "Epoch 101/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5483 - acc: 0.8478 - auROC: 0.9340 - val_loss: 0.6112 - val_acc: 0.5833 - val_auROC: 0.7299\n",
      "Epoch 102/323\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5477 - acc: 0.8509 - auROC: 0.9343 - val_loss: 0.6108 - val_acc: 0.5833 - val_auROC: 0.7315\n",
      "Epoch 103/323\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5473 - acc: 0.8509 - auROC: 0.9345 - val_loss: 0.6105 - val_acc: 0.5833 - val_auROC: 0.7361\n",
      "Epoch 104/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5469 - acc: 0.8509 - auROC: 0.9346 - val_loss: 0.6103 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 105/323\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5467 - acc: 0.8509 - auROC: 0.9359 - val_loss: 0.6101 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 106/323\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.5466 - acc: 0.8509 - auROC: 0.9346 - val_loss: 0.6099 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 107/323\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.5463 - acc: 0.8478 - auROC: 0.9362 - val_loss: 0.6097 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 108/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5460 - acc: 0.8447 - auROC: 0.9368 - val_loss: 0.6096 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 109/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5458 - acc: 0.8478 - auROC: 0.9369 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 110/323\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5455 - acc: 0.8509 - auROC: 0.9370 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7377\n",
      "Epoch 111/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5454 - acc: 0.8509 - auROC: 0.9369 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7377\n",
      "Epoch 112/323\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5452 - acc: 0.8478 - auROC: 0.9366 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7377\n",
      "Epoch 113/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5450 - acc: 0.8447 - auROC: 0.9371 - val_loss: 0.6094 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 114/323\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.5447 - acc: 0.8478 - auROC: 0.9373 - val_loss: 0.6094 - val_acc: 0.5833 - val_auROC: 0.7454\n",
      "Epoch 115/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5445 - acc: 0.8478 - auROC: 0.9368 - val_loss: 0.6093 - val_acc: 0.5833 - val_auROC: 0.7454\n",
      "Epoch 116/323\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.5444 - acc: 0.8478 - auROC: 0.9370 - val_loss: 0.6093 - val_acc: 0.5833 - val_auROC: 0.7454\n",
      "Epoch 117/323\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5441 - acc: 0.8478 - auROC: 0.9372 - val_loss: 0.6093 - val_acc: 0.5833 - val_auROC: 0.7454\n",
      "Epoch 118/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5440 - acc: 0.8478 - auROC: 0.9372 - val_loss: 0.6093 - val_acc: 0.5833 - val_auROC: 0.7454\n",
      "Epoch 119/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5438 - acc: 0.8447 - auROC: 0.9365 - val_loss: 0.6093 - val_acc: 0.5833 - val_auROC: 0.7438\n",
      "Epoch 120/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5436 - acc: 0.8478 - auROC: 0.9371 - val_loss: 0.6094 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 121/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5435 - acc: 0.8478 - auROC: 0.9364 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 122/323\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5432 - acc: 0.8478 - auROC: 0.9367 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 123/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5430 - acc: 0.8478 - auROC: 0.9368 - val_loss: 0.6095 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 124/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5429 - acc: 0.8509 - auROC: 0.9368 - val_loss: 0.6096 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 125/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5427 - acc: 0.8509 - auROC: 0.9369 - val_loss: 0.6097 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 126/323\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5426 - acc: 0.8509 - auROC: 0.9370 - val_loss: 0.6098 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 127/323\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5424 - acc: 0.8509 - auROC: 0.9370 - val_loss: 0.6100 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 128/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5422 - acc: 0.8509 - auROC: 0.9371 - val_loss: 0.6102 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 129/323\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5421 - acc: 0.8540 - auROC: 0.9371 - val_loss: 0.6104 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 130/323\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5419 - acc: 0.8540 - auROC: 0.9372 - val_loss: 0.6106 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 131/323\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5418 - acc: 0.8571 - auROC: 0.9373 - val_loss: 0.6108 - val_acc: 0.5833 - val_auROC: 0.7407\n",
      "Epoch 132/323\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5417 - acc: 0.8540 - auROC: 0.9374Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.5417 - acc: 0.8540 - auROC: 0.9374 - val_loss: 0.6111 - val_acc: 0.5833 - val_auROC: 0.7423\n",
      "Epoch 00132: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.292070  ...    0.0 -0.080926 -0.172494    0.0\n",
      "1     0.0    0.0    0.0 -0.294165  ...    0.0 -0.162964 -0.172494    0.0\n",
      "2     0.0    0.0    0.0 -0.248213  ...    0.0  0.075084  0.707696    0.0\n",
      "3     0.0    0.0    0.0 -0.302824  ...    0.0 -0.258037 -0.172494    0.0\n",
      "4     0.0    0.0    0.0 -0.302824  ...    0.0 -0.258037 -0.172494    0.0\n",
      "5     0.0    0.0    0.0  1.885128  ...    0.0 -0.042901 -0.172494    0.0\n",
      "6     0.0    0.0    0.0 -0.295996  ...    0.0 -0.258037 -0.172494    0.0\n",
      "7     0.0    0.0    0.0 -0.271334  ...    0.0 -0.229225 -0.172494    0.0\n",
      "8     0.0    0.0    0.0 -0.276102  ...    0.0  6.623665  0.532280    0.0\n",
      "9     0.0    0.0    0.0 -0.284218  ...    0.0 -0.167240  0.427280    0.0\n",
      "10    0.0    0.0    0.0 -0.300810  ...    0.0 -0.169590 -0.172494    0.0\n",
      "11    0.0    0.0    0.0 -0.270209  ...    0.0 -0.210289 -0.172494    0.0\n",
      "12    0.0    0.0    0.0 -0.266515  ...    0.0 -0.258037 -0.172494    0.0\n",
      "13    0.0    0.0    0.0 -0.221970  ...    0.0 -0.258037 -0.172494    0.0\n",
      "14    0.0    0.0    0.0 -0.302824  ...    0.0 -0.258037 -0.172494    0.0\n",
      "15    0.0    0.0    0.0 -0.302824  ...    0.0 -0.258037 -0.172494    0.0\n",
      "16    0.0    0.0    0.0 -0.302824  ...    0.0 -0.258037 -0.172494    0.0\n",
      "17    0.0    0.0    0.0  0.026475  ...    0.0 -0.075734 -0.172494    0.0\n",
      "18    0.0    0.0    0.0 -0.206048  ...    0.0  0.401503 -0.172494    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "   0      1      2         3      ...  18014     18015     18016  18017\n",
      "0    0.0    0.0    0.0 -0.270164  ...    0.0 -0.258037 -0.172494    0.0\n",
      "1    0.0    0.0    0.0  0.235610  ...    0.0 -0.217544 -0.172494    0.0\n",
      "2    0.0    0.0    0.0 -0.291983  ...    0.0 -0.258037 -0.172494    0.0\n",
      "3    0.0    0.0    0.0 -0.286166  ...    0.0 -0.258037 -0.172494    0.0\n",
      "4    0.0    0.0    0.0 -0.283857  ...    0.0 -0.168787 -0.172494    0.0\n",
      "\n",
      "[5 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 179\n",
      "Total correct samples: 179?179\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.007209  0.023941\n",
      "4      0.007137  0.023943\n",
      "...         ...       ...\n",
      "18013  0.000115  0.000317\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000341  0.001282\n",
      "18016  0.000014  0.000084\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 0.8637 - acc: 0.4783 - auROC: 0.5594 - val_loss: 0.7999 - val_acc: 0.5000 - val_auROC: 0.5694\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.7655 - acc: 0.4503 - auROC: 0.5435 - val_loss: 0.7198 - val_acc: 0.5000 - val_auROC: 0.5355\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.7329 - acc: 0.4658 - auROC: 0.5384 - val_loss: 0.6917 - val_acc: 0.4722 - val_auROC: 0.6019\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.7059 - acc: 0.4720 - auROC: 0.6099 - val_loss: 0.6749 - val_acc: 0.4722 - val_auROC: 0.6991\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6650 - acc: 0.5342 - auROC: 0.7212 - val_loss: 0.6536 - val_acc: 0.4444 - val_auROC: 0.7315\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6619 - acc: 0.5621 - auROC: 0.7308 - val_loss: 0.6281 - val_acc: 0.5278 - val_auROC: 0.7886\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6543 - acc: 0.5932 - auROC: 0.7390 - val_loss: 0.6327 - val_acc: 0.5833 - val_auROC: 0.7917\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6380 - acc: 0.6025 - auROC: 0.7667 - val_loss: 0.6305 - val_acc: 0.5833 - val_auROC: 0.7886\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6367 - acc: 0.6025 - auROC: 0.7639 - val_loss: 0.6328 - val_acc: 0.6111 - val_auROC: 0.7809\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6282 - acc: 0.6242 - auROC: 0.7723 - val_loss: 0.6238 - val_acc: 0.6389 - val_auROC: 0.8009\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6139 - acc: 0.6584 - auROC: 0.7943 - val_loss: 0.6139 - val_acc: 0.6667 - val_auROC: 0.8025\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6137 - acc: 0.6770 - auROC: 0.7923 - val_loss: 0.6082 - val_acc: 0.6667 - val_auROC: 0.8086\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6073 - acc: 0.6832 - auROC: 0.8011 - val_loss: 0.6026 - val_acc: 0.6667 - val_auROC: 0.8009\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.6011 - acc: 0.6832 - auROC: 0.8026 - val_loss: 0.5970 - val_acc: 0.6667 - val_auROC: 0.8287\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5975 - acc: 0.6863 - auROC: 0.8081 - val_loss: 0.5916 - val_acc: 0.6944 - val_auROC: 0.8164\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5910 - acc: 0.6925 - auROC: 0.8099 - val_loss: 0.5872 - val_acc: 0.6944 - val_auROC: 0.8349\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5855 - acc: 0.6988 - auROC: 0.8201 - val_loss: 0.5758 - val_acc: 0.6944 - val_auROC: 0.8318\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5786 - acc: 0.7019 - auROC: 0.8279 - val_loss: 0.5645 - val_acc: 0.6944 - val_auROC: 0.8503\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5685 - acc: 0.7081 - auROC: 0.8390 - val_loss: 0.5587 - val_acc: 0.6944 - val_auROC: 0.8457\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5655 - acc: 0.7112 - auROC: 0.8449 - val_loss: 0.5547 - val_acc: 0.6944 - val_auROC: 0.8256\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5539 - acc: 0.7050 - auROC: 0.8479 - val_loss: 0.5256 - val_acc: 0.7222 - val_auROC: 0.8349\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5488 - acc: 0.7391 - auROC: 0.8508 - val_loss: 0.5119 - val_acc: 0.7778 - val_auROC: 0.8364\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5391 - acc: 0.7547 - auROC: 0.8608 - val_loss: 0.4972 - val_acc: 0.7778 - val_auROC: 0.8843\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.5340 - acc: 0.7764 - auROC: 0.8692 - val_loss: 0.4897 - val_acc: 0.8056 - val_auROC: 0.9012\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.5222 - acc: 0.8075 - auROC: 0.8759 - val_loss: 0.4874 - val_acc: 0.8611 - val_auROC: 0.8673\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.5195 - acc: 0.8075 - auROC: 0.8707 - val_loss: 0.4745 - val_acc: 0.8611 - val_auROC: 0.8920\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.5087 - acc: 0.8292 - auROC: 0.8802 - val_loss: 0.4661 - val_acc: 0.8611 - val_auROC: 0.9167\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4972 - acc: 0.8354 - auROC: 0.8911 - val_loss: 0.4552 - val_acc: 0.8611 - val_auROC: 0.9074\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4893 - acc: 0.8354 - auROC: 0.8833 - val_loss: 0.4443 - val_acc: 0.8889 - val_auROC: 0.8981\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4784 - acc: 0.8509 - auROC: 0.8864 - val_loss: 0.4362 - val_acc: 0.8889 - val_auROC: 0.9460\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4742 - acc: 0.8634 - auROC: 0.8927 - val_loss: 0.4318 - val_acc: 0.8889 - val_auROC: 0.9259\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4594 - acc: 0.8696 - auROC: 0.8976 - val_loss: 0.4416 - val_acc: 0.8889 - val_auROC: 0.8843\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4577 - acc: 0.8602 - auROC: 0.8968 - val_loss: 0.4405 - val_acc: 0.8889 - val_auROC: 0.9012\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4538 - acc: 0.8665 - auROC: 0.9011 - val_loss: 0.4381 - val_acc: 0.8889 - val_auROC: 0.9074\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4473 - acc: 0.8634 - auROC: 0.8983 - val_loss: 0.4379 - val_acc: 0.8889 - val_auROC: 0.8765\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4720 - acc: 0.8594 - auROC: 0.8425\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4456 - acc: 0.8727 - auROC: 0.8924 - val_loss: 0.4331 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4395 - acc: 0.8758 - auROC: 0.8964 - val_loss: 0.4322 - val_acc: 0.8889 - val_auROC: 0.8843\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4388 - acc: 0.8758 - auROC: 0.8973 - val_loss: 0.4312 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4378 - acc: 0.8758 - auROC: 0.8982 - val_loss: 0.4303 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4368 - acc: 0.8789 - auROC: 0.8984 - val_loss: 0.4296 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4361 - acc: 0.8789 - auROC: 0.8996 - val_loss: 0.4289 - val_acc: 0.8889 - val_auROC: 0.8858\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4357 - acc: 0.8820 - auROC: 0.8976 - val_loss: 0.4283 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4352 - acc: 0.8820 - auROC: 0.8983 - val_loss: 0.4280 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4346 - acc: 0.8820 - auROC: 0.8993 - val_loss: 0.4277 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4340 - acc: 0.8820 - auROC: 0.8997 - val_loss: 0.4274 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4335 - acc: 0.8820 - auROC: 0.9003 - val_loss: 0.4273 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4330 - acc: 0.8851 - auROC: 0.9004 - val_loss: 0.4272 - val_acc: 0.8889 - val_auROC: 0.8781\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4330 - acc: 0.8851 - auROC: 0.8986 - val_loss: 0.4272 - val_acc: 0.8889 - val_auROC: 0.8781\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4321 - acc: 0.8851 - auROC: 0.8994 - val_loss: 0.4266 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4314 - acc: 0.8851 - auROC: 0.9001 - val_loss: 0.4262 - val_acc: 0.8889 - val_auROC: 0.8935\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4312 - acc: 0.8851 - auROC: 0.8984 - val_loss: 0.4259 - val_acc: 0.8889 - val_auROC: 0.8951\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4305 - acc: 0.8851 - auROC: 0.8986 - val_loss: 0.4258 - val_acc: 0.8889 - val_auROC: 0.8951\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4298 - acc: 0.8851 - auROC: 0.8989 - val_loss: 0.4257 - val_acc: 0.8889 - val_auROC: 0.8966\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.4294 - acc: 0.8851 - auROC: 0.8993 - val_loss: 0.4256 - val_acc: 0.8889 - val_auROC: 0.8981\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4287 - acc: 0.8851 - auROC: 0.8999 - val_loss: 0.4254 - val_acc: 0.8889 - val_auROC: 0.8981\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4283 - acc: 0.8851 - auROC: 0.9000 - val_loss: 0.4252 - val_acc: 0.8889 - val_auROC: 0.8935\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4277 - acc: 0.8851 - auROC: 0.9001 - val_loss: 0.4250 - val_acc: 0.8889 - val_auROC: 0.8935\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4272 - acc: 0.8851 - auROC: 0.9006 - val_loss: 0.4248 - val_acc: 0.8889 - val_auROC: 0.8935\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4268 - acc: 0.8851 - auROC: 0.9009 - val_loss: 0.4245 - val_acc: 0.8889 - val_auROC: 0.8873\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4263 - acc: 0.8851 - auROC: 0.9014 - val_loss: 0.4244 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4261 - acc: 0.8851 - auROC: 0.9016 - val_loss: 0.4242 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4255 - acc: 0.8882 - auROC: 0.9021 - val_loss: 0.4242 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4253 - acc: 0.8882 - auROC: 0.9022 - val_loss: 0.4239 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4246 - acc: 0.8882 - auROC: 0.9024 - val_loss: 0.4240 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4244 - acc: 0.8851 - auROC: 0.9028 - val_loss: 0.4241 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4240 - acc: 0.8851 - auROC: 0.9033 - val_loss: 0.4237 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4237 - acc: 0.8882 - auROC: 0.9033 - val_loss: 0.4233 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4231 - acc: 0.8882 - auROC: 0.9036 - val_loss: 0.4233 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4227 - acc: 0.8882 - auROC: 0.9039 - val_loss: 0.4231 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4222 - acc: 0.8882 - auROC: 0.9049 - val_loss: 0.4232 - val_acc: 0.8889 - val_auROC: 0.8843\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4218 - acc: 0.8882 - auROC: 0.9033 - val_loss: 0.4234 - val_acc: 0.8889 - val_auROC: 0.8873\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4214 - acc: 0.8882 - auROC: 0.9026 - val_loss: 0.4235 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4211 - acc: 0.8882 - auROC: 0.9040 - val_loss: 0.4233 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 74/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4215 - acc: 0.8828 - auROC: 0.9227\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4209 - acc: 0.8882 - auROC: 0.9052 - val_loss: 0.4235 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4202 - acc: 0.8882 - auROC: 0.9045 - val_loss: 0.4234 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4202 - acc: 0.8882 - auROC: 0.9046 - val_loss: 0.4233 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4202 - acc: 0.8882 - auROC: 0.9050 - val_loss: 0.4232 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4201 - acc: 0.8882 - auROC: 0.9050 - val_loss: 0.4232 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 79/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.4118 - acc: 0.8984 - auROC: 0.9167\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4200 - acc: 0.8882 - auROC: 0.9049 - val_loss: 0.4231 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4200 - acc: 0.8882 - auROC: 0.9056 - val_loss: 0.4230 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4199 - acc: 0.8882 - auROC: 0.9056 - val_loss: 0.4229 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4198 - acc: 0.8882 - auROC: 0.9056 - val_loss: 0.4229 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.4198 - acc: 0.8882 - auROC: 0.9054 - val_loss: 0.4228 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4197 - acc: 0.8882 - auROC: 0.9055 - val_loss: 0.4228 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4197 - acc: 0.8882 - auROC: 0.9055 - val_loss: 0.4227 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.4196 - acc: 0.8882 - auROC: 0.9056 - val_loss: 0.4226 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.4196 - acc: 0.8882 - auROC: 0.9058 - val_loss: 0.4226 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4195 - acc: 0.8913 - auROC: 0.9058 - val_loss: 0.4225 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4195 - acc: 0.8913 - auROC: 0.9058 - val_loss: 0.4225 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4195 - acc: 0.8913 - auROC: 0.9059 - val_loss: 0.4224 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4194 - acc: 0.8913 - auROC: 0.9059 - val_loss: 0.4224 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4193 - acc: 0.8913 - auROC: 0.9059 - val_loss: 0.4224 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4193 - acc: 0.8913 - auROC: 0.9060 - val_loss: 0.4224 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4192 - acc: 0.8913 - auROC: 0.9060 - val_loss: 0.4223 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4192 - acc: 0.8913 - auROC: 0.9060 - val_loss: 0.4223 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4191 - acc: 0.8913 - auROC: 0.9060 - val_loss: 0.4222 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4191 - acc: 0.8913 - auROC: 0.9060 - val_loss: 0.4222 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4190 - acc: 0.8913 - auROC: 0.9061 - val_loss: 0.4222 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4190 - acc: 0.8913 - auROC: 0.9060 - val_loss: 0.4222 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4189 - acc: 0.8913 - auROC: 0.9053 - val_loss: 0.4222 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4189 - acc: 0.8913 - auROC: 0.9061 - val_loss: 0.4221 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4188 - acc: 0.8913 - auROC: 0.9054 - val_loss: 0.4221 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4188 - acc: 0.8913 - auROC: 0.9055 - val_loss: 0.4221 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4187 - acc: 0.8913 - auROC: 0.9055 - val_loss: 0.4220 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4187 - acc: 0.8913 - auROC: 0.9056 - val_loss: 0.4220 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4186 - acc: 0.8913 - auROC: 0.9057 - val_loss: 0.4220 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4186 - acc: 0.8913 - auROC: 0.9058 - val_loss: 0.4220 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4185 - acc: 0.8913 - auROC: 0.9058 - val_loss: 0.4219 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4184 - acc: 0.8913 - auROC: 0.9058 - val_loss: 0.4219 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4184 - acc: 0.8913 - auROC: 0.9067 - val_loss: 0.4219 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4183 - acc: 0.8913 - auROC: 0.9068 - val_loss: 0.4219 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4183 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4219 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4182 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4218 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4182 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4218 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4181 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4218 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4180 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4217 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4180 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4217 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4179 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4217 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4179 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4217 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4178 - acc: 0.8913 - auROC: 0.9070 - val_loss: 0.4217 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4177 - acc: 0.8913 - auROC: 0.9071 - val_loss: 0.4217 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4177 - acc: 0.8913 - auROC: 0.9071 - val_loss: 0.4216 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4176 - acc: 0.8913 - auROC: 0.9071 - val_loss: 0.4216 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4176 - acc: 0.8913 - auROC: 0.9065 - val_loss: 0.4216 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4175 - acc: 0.8913 - auROC: 0.9066 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4174 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 127/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4174 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4173 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4172 - acc: 0.8913 - auROC: 0.9070 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4172 - acc: 0.8913 - auROC: 0.9070 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4171 - acc: 0.8913 - auROC: 0.9070 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4170 - acc: 0.8913 - auROC: 0.9071 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4170 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4169 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4168 - acc: 0.8913 - auROC: 0.9066 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4168 - acc: 0.8913 - auROC: 0.9067 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4167 - acc: 0.8913 - auROC: 0.9068 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4166 - acc: 0.8913 - auROC: 0.9066 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4165 - acc: 0.8913 - auROC: 0.9066 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4165 - acc: 0.8913 - auROC: 0.9065 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4164 - acc: 0.8913 - auROC: 0.9066 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4163 - acc: 0.8913 - auROC: 0.9066 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4163 - acc: 0.8913 - auROC: 0.9070 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4162 - acc: 0.8913 - auROC: 0.9071 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 145/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4161 - acc: 0.8913 - auROC: 0.9071 - val_loss: 0.4213 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4160 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4213 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 147/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4160 - acc: 0.8913 - auROC: 0.9070 - val_loss: 0.4213 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4159 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4212 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 149/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4158 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4212 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 150/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4157 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4212 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 151/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4157 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4212 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 152/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4156 - acc: 0.8913 - auROC: 0.9074 - val_loss: 0.4211 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 153/300\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.4155 - acc: 0.8913 - auROC: 0.9076 - val_loss: 0.4211 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 154/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4155 - acc: 0.8913 - auROC: 0.9078 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 155/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4154 - acc: 0.8913 - auROC: 0.9086 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 156/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4153 - acc: 0.8913 - auROC: 0.9087 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 157/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4152 - acc: 0.8913 - auROC: 0.9087 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 158/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4152 - acc: 0.8913 - auROC: 0.9086 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 159/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4151 - acc: 0.8913 - auROC: 0.9086 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 160/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4151 - acc: 0.8913 - auROC: 0.9079 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 161/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4150 - acc: 0.8913 - auROC: 0.9079 - val_loss: 0.4210 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 162/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4149 - acc: 0.8913 - auROC: 0.9079 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 163/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4148 - acc: 0.8913 - auROC: 0.9086 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 164/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4148 - acc: 0.8913 - auROC: 0.9086 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 165/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4147 - acc: 0.8913 - auROC: 0.9088 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 166/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4147 - acc: 0.8913 - auROC: 0.9088 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 167/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4146 - acc: 0.8913 - auROC: 0.9088 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 168/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4145 - acc: 0.8913 - auROC: 0.9088 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 169/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4145 - acc: 0.8913 - auROC: 0.9089 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 170/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4144 - acc: 0.8913 - auROC: 0.9088 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 171/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4144 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 172/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4143 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 173/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4143 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 174/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4142 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 175/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4141 - acc: 0.8913 - auROC: 0.9091 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 176/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4141 - acc: 0.8913 - auROC: 0.9092 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 177/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4140 - acc: 0.8913 - auROC: 0.9093 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 178/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4140 - acc: 0.8913 - auROC: 0.9091 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 179/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4139 - acc: 0.8913 - auROC: 0.9093 - val_loss: 0.4208 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 180/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4139 - acc: 0.8913 - auROC: 0.9092 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 181/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4138 - acc: 0.8913 - auROC: 0.9094 - val_loss: 0.4207 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 182/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4138 - acc: 0.8913 - auROC: 0.9094 - val_loss: 0.4206 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 183/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4137 - acc: 0.8913 - auROC: 0.9093 - val_loss: 0.4205 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 184/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4137 - acc: 0.8913 - auROC: 0.9094 - val_loss: 0.4205 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 185/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4136 - acc: 0.8913 - auROC: 0.9093 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 186/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4136 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 187/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4135 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 188/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4135 - acc: 0.8913 - auROC: 0.9090 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 189/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4134 - acc: 0.8913 - auROC: 0.9068 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 190/300\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.4134 - acc: 0.8913 - auROC: 0.9067 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 191/300\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.4133 - acc: 0.8913 - auROC: 0.9068 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 192/300\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4133 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 193/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4132 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4202 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 194/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4132 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4202 - val_acc: 0.8889 - val_auROC: 0.8812\n",
      "Epoch 195/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4131 - acc: 0.8913 - auROC: 0.9068 - val_loss: 0.4202 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 196/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4131 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4202 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 197/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4130 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4202 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 198/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4130 - acc: 0.8913 - auROC: 0.9078 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 199/300\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4129 - acc: 0.8913 - auROC: 0.9079 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 200/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4129 - acc: 0.8913 - auROC: 0.9079 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 201/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4128 - acc: 0.8913 - auROC: 0.9079 - val_loss: 0.4203 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 202/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4128 - acc: 0.8913 - auROC: 0.9080 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 203/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4127 - acc: 0.8913 - auROC: 0.9081 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 204/300\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4127 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 205/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4126 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 206/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4126 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4204 - val_acc: 0.8889 - val_auROC: 0.8843\n",
      "Epoch 207/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4125 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4205 - val_acc: 0.8889 - val_auROC: 0.8843\n",
      "Epoch 208/300\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4125 - acc: 0.8913 - auROC: 0.9072 - val_loss: 0.4205 - val_acc: 0.8889 - val_auROC: 0.8858\n",
      "Epoch 209/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3922 - acc: 0.9141 - auROC: 0.9244Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.4124 - acc: 0.8913 - auROC: 0.9073 - val_loss: 0.4205 - val_acc: 0.8889 - val_auROC: 0.8858\n",
      "Epoch 00209: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 209/508\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.4145 - acc: 0.8913 - auROC: 0.9041 - val_loss: 0.4189 - val_acc: 0.8889 - val_auROC: 0.8904\n",
      "Epoch 210/508\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.4106 - acc: 0.8913 - auROC: 0.9069 - val_loss: 0.4194 - val_acc: 0.8889 - val_auROC: 0.8966\n",
      "Epoch 211/508\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4075 - acc: 0.8913 - auROC: 0.9136 - val_loss: 0.4215 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 212/508\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4048 - acc: 0.8913 - auROC: 0.9144 - val_loss: 0.4219 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 213/508\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4029 - acc: 0.8944 - auROC: 0.9158 - val_loss: 0.4222 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 214/508\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4008 - acc: 0.8944 - auROC: 0.9176 - val_loss: 0.4223 - val_acc: 0.8889 - val_auROC: 0.8827\n",
      "Epoch 215/508\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3991 - acc: 0.8944 - auROC: 0.9194 - val_loss: 0.4209 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 216/508\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.3957 - acc: 0.8944 - auROC: 0.9219 - val_loss: 0.4214 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 217/508\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3926 - acc: 0.9006 - auROC: 0.9221 - val_loss: 0.4220 - val_acc: 0.8889 - val_auROC: 0.8889\n",
      "Epoch 218/508\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.3892 - acc: 0.9037 - auROC: 0.9232 - val_loss: 0.4221 - val_acc: 0.8889 - val_auROC: 0.8858\n",
      "Epoch 219/508\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3867 - acc: 0.9068 - auROC: 0.9252 - val_loss: 0.4232 - val_acc: 0.8889 - val_auROC: 0.8858\n",
      "Epoch 220/508\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3845 - acc: 0.9161 - auROC: 0.9254 - val_loss: 0.4229 - val_acc: 0.8889 - val_auROC: 0.8873\n",
      "Epoch 221/508\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3829 - acc: 0.9161 - auROC: 0.9262 - val_loss: 0.4226 - val_acc: 0.8889 - val_auROC: 0.8920\n",
      "Epoch 222/508\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3812 - acc: 0.9193 - auROC: 0.9268 - val_loss: 0.4223 - val_acc: 0.8889 - val_auROC: 0.8920\n",
      "Epoch 223/508\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3799 - acc: 0.9193 - auROC: 0.9274 - val_loss: 0.4218 - val_acc: 0.8889 - val_auROC: 0.8920\n",
      "Epoch 224/508\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3788 - acc: 0.9193 - auROC: 0.9280Restoring model weights from the end of the best epoch.\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.3788 - acc: 0.9193 - auROC: 0.9280 - val_loss: 0.4212 - val_acc: 0.8889 - val_auROC: 0.8920\n",
      "Epoch 00224: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.288801  ...    0.0  0.296855 -0.168854    0.0\n",
      "1     0.0    0.0    0.0 -0.260896  ...    0.0 -0.265994 -0.168854    0.0\n",
      "2     0.0    0.0    0.0  0.071758  ...    0.0 -0.265994 -0.168854    0.0\n",
      "3     0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "4     0.0    0.0    0.0 -0.279012  ...    0.0 -0.190887 -0.168854    0.0\n",
      "5     0.0    0.0    0.0  0.220748  ...    0.0  0.598134  4.071396    0.0\n",
      "6     0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "7     0.0    0.0    0.0 -0.266255  ...    0.0 -0.193622 -0.168854    0.0\n",
      "8     0.0    0.0    0.0  4.964308  ...    0.0 -0.265994 -0.168854    0.0\n",
      "9     0.0    0.0    0.0 -0.286790  ...    0.0 -0.265994 -0.168854    0.0\n",
      "10    0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "11    0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "12    0.0    0.0    0.0 -0.298982  ...    0.0 -0.265994 -0.168854    0.0\n",
      "13    0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "14    0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "15    0.0    0.0    0.0 -0.173555  ...    0.0 -0.140585 -0.168854    0.0\n",
      "16    0.0    0.0    0.0 -0.301129  ...    0.0 -0.231776 -0.168854    0.0\n",
      "17    0.0    0.0    0.0 -0.290388  ...    0.0 -0.225873 -0.168854    0.0\n",
      "18    0.0    0.0    0.0 -0.301129  ...    0.0 -0.265994 -0.168854    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "   0      1      2         3      ...  18014     18015     18016  18017\n",
      "0    0.0    0.0    0.0 -0.266641  ...    0.0 -0.265994 -0.168854    0.0\n",
      "1    0.0    0.0    0.0  0.267442  ...    0.0 -0.229626 -0.168854    0.0\n",
      "2    0.0    0.0    0.0 -0.289681  ...    0.0 -0.265994 -0.168854    0.0\n",
      "3    0.0    0.0    0.0 -0.283539  ...    0.0 -0.265994 -0.168854    0.0\n",
      "4    0.0    0.0    0.0 -0.281101  ...    0.0 -0.185835 -0.168854    0.0\n",
      "\n",
      "[5 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 15:30:28.304371: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-29 15:30:28.312783: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-29 15:30:28.315283: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564ca6cadf70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-29 15:30:28.315335: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-29 15:30:28.433598: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:30:28.445780: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:30:28.589483: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:30:28.676609: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:30:28.764672: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-29 15:30:44.792799: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2020-12-29 15:31:09.238489: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-29 15:31:09.247084: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-29 15:31:09.249540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e730eeffb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-29 15:31:09.249572: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2020-12-29 15:31:13.060835: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:14.089197: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:15.453804: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:16.513798: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-29 15:31:30.946388: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2020-12-29 15:31:55.003984: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-29 15:31:55.012237: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-29 15:31:55.014659: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55993d686160 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-29 15:31:55.014709: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-29 15:31:55.127424: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:55.137653: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:55.293747: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:55.375726: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:31:55.455558: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-29 15:32:21.007511: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2020-12-29 15:32:44.718625: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-29 15:32:44.727331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-29 15:32:44.729866: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559852c4bfc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-29 15:32:44.729917: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-29 15:32:44.857750: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:32:44.991274: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:32:44.998365: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:32:45.076924: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:32:45.156503: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-29 15:33:17.801045: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2020-12-29 15:33:41.865403: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-29 15:33:41.873952: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-29 15:33:41.876372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c2075650e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-29 15:33:41.876420: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-29 15:33:41.991705: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:33:42.053276: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:33:42.060349: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:33:42.137812: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-29 15:33:42.222369: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-29 15:34:15.856785: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {0,1,2,3,4}; do\n",
    "expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_DM \\\n",
    "        -m ../Disease-diagnosis/experiments/exp_3/TrainModel/ --finetune --update-statistics;\n",
    "expert search -i experiments/exp_$i/QueryCM1.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult1;\n",
    "expert search -i experiments/QueryCM2.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult2;\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "metadata = pd.read_csv('dataFiles/samples_meta_new.csv').set_index('SampleID')\n",
    "metadata['COLLECTION_DATE'] = pd.to_datetime(metadata.COLLECTION_DATE)\n",
    "sorted_months = metadata.COLLECTION_DATE.sort_values().copy()\n",
    "\n",
    "c1 = pd.concat([pd.read_csv('experiments/exp_{}/SearchResult1/layer-2.csv'.format(i), index_col=0) for i in range(5)])\n",
    "c2 = pd.concat([pd.read_csv('experiments/exp_{}/SearchResult2/layer-2.csv'.format(i), index_col=0).join(metadata, how='left').rename(index=lambda x: x+'_exp'+str(i)) for i in range(5)])\n",
    "\n",
    "c1 = c1.join(metadata, how='left')\n",
    "c1['group'] = 'reference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/guides/guides.py:197: PlotnineWarning: Cannot generate legend for the 'color' aesthetic. Make sure you have mapped a variable to it\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8728116711193)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Figure.pdf\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/guides/guides.py:197: PlotnineWarning: Cannot generate legend for the 'color' aesthetic. Make sure you have mapped a variable to it\n"
     ]
    }
   ],
   "source": [
    "from plotnine import *\n",
    "\n",
    "plot = (ggplot(c1, aes(x='SEASON', y='root:W'))\n",
    "        + geom_boxplot(aes(fill='SEASON', group='SEASON'), outlier_shape='', show_legend=True)\n",
    "        + scale_fill_manual([\"#E64B35FF\", \"#4DBBD5FF\", \"#00A087FF\", \"#3C5488FF\", \"#F39B7FFF\", \"#8491B4FF\", \"#91D1C2FF\", \"#DC0000FF\", \"#7E6148FF\", \"#B09C85FF\"])\n",
    "        #+ geom_boxplot(aes(group='Timepoint_str'), fill='white', alpha=0.5, show_legend=False, outlier_shape='', data=data[data.People != 'MT10'], width=0.4)\n",
    "        + geom_smooth(aes(group='group'), se=True, method='loess', show_legend=True)\n",
    "        #+ scale_linetype_manual(['solid', 'dashdot'])\n",
    "        + geom_point(color='black')\n",
    "        + geom_point(color='red', data=c2, shape='o', size=3)\n",
    "        + scale_color_manual(['brown', \"black\"])\n",
    "        + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(),\n",
    "             axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "        #+ geom_hline(yintercept=, linetype=\"dotted\")\n",
    "        + geom_vline(xintercept=[1, 2, 3, 4, 5], linetype=\"dashed\", size=0.6)\n",
    "        #+ geom_label(data='Early back', position=[15.5, 0.2])\n",
    "        + scale_x_discrete(limits=['2013-LD', '2014-EW', '2014-LW', '2014-ED', '2014-LD'])\n",
    "        + xlab('Season')\n",
    "        + ylab('Contribution from season: \"Wet\"')\n",
    ")\n",
    "\n",
    "print(plot)\n",
    "plot.save('Figure.pdf', dpi=120, width=6.4*1, height=4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>study_accession</th>\n",
       "      <th>sample_alias</th>\n",
       "      <th>#SampleID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>COLLECTION_DATE</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>BUSH_CAMP</th>\n",
       "      <th>Metagenomics</th>\n",
       "      <th>SEASONALITY_UNIQUE_ONLY_1</th>\n",
       "      <th>8_individuals</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>Env</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>SRR5760942</td>\n",
       "      <td>PRJNA392012</td>\n",
       "      <td>TZ_HADZA_267</td>\n",
       "      <td>267</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1/30/2014</td>\n",
       "      <td>2014-EW</td>\n",
       "      <td>SENGELI</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>root:W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SRR5760958</td>\n",
       "      <td>PRJNA392012</td>\n",
       "      <td>TZ_HADZA_227</td>\n",
       "      <td>227</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1/28/2014</td>\n",
       "      <td>2014-EW</td>\n",
       "      <td>SENGELI</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>root:W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>SRR5761074</td>\n",
       "      <td>PRJNA392012</td>\n",
       "      <td>TZ_HADZA_139</td>\n",
       "      <td>139</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>45.0</td>\n",
       "      <td>9/7/2013</td>\n",
       "      <td>2013-LD</td>\n",
       "      <td>SENGELI</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>root:D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>SRR5761079</td>\n",
       "      <td>PRJNA392012</td>\n",
       "      <td>TZ_HADZA_198</td>\n",
       "      <td>198</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1/26/2014</td>\n",
       "      <td>2014-EW</td>\n",
       "      <td>SENGELI</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "      <td>root:W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SampleID study_accession  sample_alias  #SampleID  Gender   Age  \\\n",
       "60   SRR5760942     PRJNA392012  TZ_HADZA_267        267  FEMALE  45.0   \n",
       "71   SRR5760958     PRJNA392012  TZ_HADZA_227        227  FEMALE  45.0   \n",
       "132  SRR5761074     PRJNA392012  TZ_HADZA_139        139  FEMALE  45.0   \n",
       "137  SRR5761079     PRJNA392012  TZ_HADZA_198        198  FEMALE  45.0   \n",
       "\n",
       "    COLLECTION_DATE   SEASON BUSH_CAMP Metagenomics SEASONALITY_UNIQUE_ONLY_1  \\\n",
       "60        1/30/2014  2014-EW   SENGELI           No                        No   \n",
       "71        1/28/2014  2014-EW   SENGELI           No                        No   \n",
       "132        9/7/2013  2013-LD   SENGELI           No                        No   \n",
       "137       1/26/2014  2014-EW   SENGELI           No                       Yes   \n",
       "\n",
       "    8_individuals  SUBJECT_ID     Env  \n",
       "60             No           5  root:W  \n",
       "71             No           5  root:W  \n",
       "132            No           5  root:D  \n",
       "137            No           5  root:W  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[meta.SUBJECT_ID == 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing the abundance data...\n",
      "Trying calculating jensenshannon beta_diversity using scikit-bio & scikit-learn package...\n",
      "This could be time-consuming.\n",
      "Failed, the metric you selected is not supported by neither scikit-bio nor scikit-learn.\n",
      "Trying using SciPy...\n",
      "Succeeded!\n",
      "Visualizing the data using plotnine package...\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Plots.JSD/PCoA.pdf\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 1 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Plots.JSD/PC1_boxplot.pdf\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 1 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Plots.JSD/PC2_boxplot.pdf\n",
      "Plots are saved in Plots.JSD. Import them into Illustrator for further improvements.\n"
     ]
    }
   ],
   "source": [
    "meta.to_csv('Metadata_PCoA.csv')\n",
    "abu.to_csv('abundance_PCoA.csv')\n",
    "!python ../../UniPCoA/UniPCoA.py -i abundance_PCoA.csv -m Metadata_PCoA.csv \\\n",
    "    --metric jensenshannon -o Plots.JSD\n",
    "\n",
    "#-t ../../UniPCoA/LTPs132_SSU_tree.newick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =[pd.read_csv('experiments/exp_{}/EvalResult_{}/overall.csv'.format(i, 'Adapt_ft_DM'), index_col=0).rename(columns=lambda x: '{}-exp_{}-{}'.format(x, i, 'Adapt_ft_DM')).dropna()\n",
    "        for i in range(5)]\n",
    "overall = pd.concat(metrics, axis=1)\n",
    "overall\n",
    "overall = overall.reset_index().melt(id_vars=['index'], value_vars=overall.columns.tolist(), var_name='metric').dropna()\n",
    "overall['Experiment'] = overall['metric'].str.split('-').apply(lambda x: '{}'.format( x[3])).map({'Adapt_ft_DM': 'Transfer (DM)', 'Adapt_ft_HM': 'Transfer (HM)', 'Train': 'Independent'})\n",
    "overall['Metric'] = overall['metric'].str.split('-').apply(lambda x: '{}-{}'.format(x[0], x[1]))\n",
    "overall = overall[overall.Metric == 'ROC-AUC'].groupby(by='index', as_index=False).mean().round(4)\n",
    "overall['index'] = overall['index'].apply(lambda x: x.split(' ')[2].rstrip(')'))\n",
    "overall['ROC-AUC'] = overall['value']\n",
    "overall['Stage'] = overall['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8737241783248)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 3.6 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: CRC_stage_performance.pdf\n"
     ]
    }
   ],
   "source": [
    "from plotnine import *\n",
    "plot = (ggplot(overall, aes(x='Stage', y='ROC-AUC', fill='Stage'))\n",
    "         + geom_bar(stat='identity', width=0.3, show_legend = False)\n",
    "         + scale_fill_manual(['#E64B35FF','#4DBBD5FF','#00A087FF','#3C5488FF','#F39B7FFF','#8491B4FF','#91D1C2FF'])\n",
    "         + geom_text(aes(label='value'), position=position_dodge(width=0.9), nudge_y=0.02)\n",
    "         + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(),\n",
    "                 axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "       )\n",
    "print(plot)\n",
    "plot.save('CRC_stage_performance.pdf', dpi=120, width=3.6, height=4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "        ERR475482   ERR475493   ERR475500  ...   ERR481063   ERR481064   ERR481065\n",
      "count  854.000000  854.000000  854.000000  ...  854.000000  854.000000  854.000000\n",
      "mean     0.188848    0.155293    0.091492  ...    0.221712    0.225143    0.222403\n",
      "std      2.274977    1.508092    1.018507  ...    4.716516    4.774864    4.758246\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     47.130700   37.035000   24.845200  ...  135.370960  137.099840  136.803670\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/854 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "         ERR475482    ERR475493  ...    ERR481064    ERR481065\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026303     0.020940  ...     0.031547     0.031352\n",
      "std       0.859542     0.568293  ...     1.800976     1.794912\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      47.130699    37.035000  ...   137.099838   136.803665\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Normalizing results...\n",
      "         ERR475482    ERR475493  ...    ERR481064    ERR481065\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.005441     0.004519  ...     0.009505     0.009532\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.298344     0.294471  ...     0.723581     0.726522\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.170845  ...    0.0  33.050780  69.748160    0.0\n",
      "1      0.0    0.0    0.0 -0.171821  ...    0.0   2.232037   4.557535    0.0\n",
      "2      0.0    0.0    0.0  5.015766  ...    0.0  15.904994  23.933508    0.0\n",
      "3      0.0    0.0    0.0 -0.170144  ...    0.0  11.578986  24.785040    0.0\n",
      "4      0.0    0.0    0.0  1.735051  ...    0.0   3.927780   8.491182    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "631    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "632    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "633    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "634    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.172956  ...    0.0  33.823331  71.783639    0.0\n",
      "1      0.0    0.0    0.0 -0.173955  ...    0.0   2.282320   4.691487    0.0\n",
      "2      0.0    0.0    0.0  5.141887  ...    0.0  16.275716  24.632632    0.0\n",
      "3      0.0    0.0    0.0 -0.172237  ...    0.0  11.848324  25.509002    0.0\n",
      "4      0.0    0.0    0.0  1.780061  ...    0.0   4.017805   8.739873    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "631    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "632    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "633    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "634    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.174914  ...    0.0  33.551714  69.381050    0.0\n",
      "1      0.0    0.0    0.0 -0.175921  ...    0.0   2.265456   4.532195    0.0\n",
      "2      0.0    0.0    0.0  5.177374  ...    0.0  16.145829  23.806587    0.0\n",
      "3      0.0    0.0    0.0 -0.174190  ...    0.0  11.754197  24.653654    0.0\n",
      "4      0.0    0.0    0.0  1.791862  ...    0.0   3.986923   8.445219    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "631    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "632    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "633    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "634    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.173595  ...    0.0  33.276793  69.338235    0.0\n",
      "1      0.0    0.0    0.0 -0.174612  ...    0.0   2.246413   4.529124    0.0\n",
      "2      0.0    0.0    0.0  5.233645  ...    0.0  16.013264  23.791703    0.0\n",
      "3      0.0    0.0    0.0 -0.172864  ...    0.0  11.657549  24.638252    0.0\n",
      "4      0.0    0.0    0.0  1.813374  ...    0.0   3.953801   8.439750    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "631    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "632    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "633    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "634    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.172859  ...    0.0  33.199631  69.466157    0.0\n",
      "1      0.0    0.0    0.0 -0.173845  ...    0.0   2.242247   4.538022    0.0\n",
      "2      0.0    0.0    0.0  5.068282  ...    0.0  15.976713  23.835978    0.0\n",
      "3      0.0    0.0    0.0 -0.172151  ...    0.0  11.631244  24.684081    0.0\n",
      "4      0.0    0.0    0.0  1.753075  ...    0.0   3.945619   8.455830    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "631    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "632    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "633    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "634    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('dataFiles/species_abundance.csv', index_col=0).to_csv('dataFiles/species_abundance.tsv', sep='\\t')\n",
    "!ls dataFiles/species_abundance.tsv > tmp\n",
    "!expert convert -i tmp -o CRC_cm.h5 --in-cm\n",
    "!for i in {0,1,2,3,4}; do expert search -i CRC_cm.h5 -o CRC_contribution_$i -m ../Disease-diagnosis/experiments/exp_$i/TrainModel; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
