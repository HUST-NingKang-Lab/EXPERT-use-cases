{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed to get more reproducible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(2)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from expert.src.utils import read_genus_abu, read_labels, load_otlg, zero_weight_unk, parse_otlg, get_dmax\n",
    "from expert.src.preprocessing import *\n",
    "from expert.src.model import *\n",
    "from expert.CLI.CLI_utils import find_pkg_resource as find_expert_resource\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, AlphaDropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.initializers import HeUniform, GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "abu = pd.read_csv('dataFiles/countmatrix.csv', index_col=0)\n",
    "abu = abu.set_index(abu.index.to_series().str.replace(';Other', ''))\n",
    "abu = abu.groupby(by=abu.index).sum()\n",
    "meta = pd.read_csv('dataFiles/metadata.csv').set_index('#SampleID')\n",
    "meta.index.name = 'SampleID'\n",
    "meta = meta[~meta.People.str.startswith('TTD')]\n",
    "meta.loc[meta.Group == 'BJN', 'Env'] = 'root:China'\n",
    "meta.loc[meta.Group == 'MTC', 'Env'] = 'root:China'\n",
    "meta.loc[meta.Group == 'MTT', 'Env'] = 'root:Trinidad and Tobago'\n",
    "meta.loc[meta.Group == 'MTB', 'Env'] = 'root:China'\n",
    "meta.loc[meta.Group == 'TTC', 'Env'] = 'root:Trinidad and Tobago'\n",
    "meta.loc[meta.Group == 'TTN', 'Env'] = 'root:Trinidad and Tobago'\n",
    "#meta['Env'] = meta.apply(lambda x: x['Env'] + ':' + x.Phase if x.name.startswith('MT') else x['Env'], axis=1)\n",
    "meta['is_MT10'] = (meta.People == 'MT10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    is_source = ~(meta.People == 'MT'+str(i))\n",
    "    if not os.path.isdir('experiments/exp_{}'.format(i)):\n",
    "        os.mkdir('experiments/exp_{}'.format(i))\n",
    "    meta.loc[~is_source, :].reset_index().to_csv('experiments/exp_{}/QueryMapper.csv'.format(i))\n",
    "    meta.loc[is_source&~meta.is_MT10, :].reset_index().to_csv('experiments/exp_{}/SourceMapper.csv'.format(i))\n",
    "    abu.loc[:, meta[~is_source].index.tolist()].to_csv('experiments/exp_{}/QueryCM.tsv'.format(i), sep='\\t')\n",
    "    abu.loc[:, meta[is_source&~meta.is_MT10].index.tolist()].to_csv('experiments/exp_{}/SourceCM.tsv'.format(i), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ontology using mapper file of source samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:China\n",
      "root:Trinidad and Tobago\n",
      "Reading microbiome structure...\n",
      "Generating Ontology...\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 12175.05it/s]\n",
      "root\n",
      "├── root:China\n",
      "└── root:Trinidad and Tobago\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!awk -F ',' '{print $9}' experiments/exp_1/SourceMapper.csv | grep -v \"Env\" | sort | uniq  > microbiomes.txt\n",
    "!cat microbiomes.txt\n",
    "!expert construct -i microbiomes.txt -o ontology.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data, using EXPERT's command-line API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC1T1      MTC1T2      MTT1T4  ...      MTT1T8      MTT1T9     MTT1T17\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007941    0.007944    0.007931  ...    0.007945    0.007947    0.007945\n",
      "std      0.063604    0.063045    0.058586  ...    0.051251    0.049582    0.054776\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.712427    0.693843    0.634973  ...    0.578762    0.648038    0.745457\n",
      "\n",
      "[6 rows x 11 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC1T1       MTC1T2  ...       MTT1T9      MTT1T17\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.020893     0.020862  ...     0.019249     0.020086\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.716715     0.696566  ...     0.808123     0.875130\n",
      "\n",
      "[6 rows x 11 columns]\n",
      "Normalizing results...\n",
      "            MTC1T1       MTC1T2  ...       MTT1T9      MTT1T17\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.004183     0.004175  ...     0.003851     0.004019\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.143489     0.139404  ...     0.161660     0.175125\n",
      "\n",
      "[6 rows x 11 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 249 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 249 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 249 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       249\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  123\n",
      "root:Trinidad and Tobago    126\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       11\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                  2\n",
      "root:Trinidad and Tobago    9\n",
      "Unknown                     0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC2T1      MTC2T2      MTT2T4  ...     MTB2T23     MTB2T24     MTB2T25\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007939    0.007943    0.007947  ...    0.007945    0.007943    0.007943\n",
      "std      0.073192    0.067095    0.061885  ...    0.050094    0.051974    0.053839\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.841957    0.770425    0.691859  ...    0.493139    0.501060    0.627922\n",
      "\n",
      "[6 rows x 18 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC2T1       MTC2T2  ...      MTB2T24      MTB2T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000831     0.000832  ...     0.000832     0.000832\n",
      "std       0.023843     0.021908  ...     0.017969     0.017842\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.844999     0.773872  ...     0.505478     0.629173\n",
      "\n",
      "[6 rows x 18 columns]\n",
      "Normalizing results...\n",
      "            MTC2T1       MTC2T2  ...      MTB2T24      MTB2T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.004775     0.004385  ...     0.003597     0.003571\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.169214     0.154892  ...     0.101175     0.125934\n",
      "\n",
      "[6 rows x 18 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 242 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 242 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 242 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       242\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  118\n",
      "root:Trinidad and Tobago    124\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       18\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                   7\n",
      "root:Trinidad and Tobago    11\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC3T1      MTC3T2      MTT3T4  ...     MTB3T23     MTB3T24     MTB3T25\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007943    0.007939    0.007936  ...    0.007940    0.007940    0.007935\n",
      "std      0.069117    0.057831    0.059522  ...    0.057365    0.057017    0.058585\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.789261    0.656899    0.660154  ...    0.662534    0.636487    0.656204\n",
      "\n",
      "[6 rows x 20 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC3T1       MTC3T2  ...      MTB3T24      MTB3T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000831  ...     0.000832     0.000831\n",
      "std       0.022627     0.019410  ...     0.019151     0.019457\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.798703     0.683742  ...     0.663778     0.685819\n",
      "\n",
      "[6 rows x 20 columns]\n",
      "Normalizing results...\n",
      "            MTC3T1       MTC3T2  ...      MTB3T24      MTB3T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.004529     0.003887  ...     0.003834     0.003898\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.159864     0.136925  ...     0.132905     0.137400\n",
      "\n",
      "[6 rows x 20 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 240 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 240 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 240 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       240\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  118\n",
      "root:Trinidad and Tobago    122\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       20\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                   7\n",
      "root:Trinidad and Tobago    13\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC4T1      MTC4T2    MTT4T12a  ...     MTB4T23     MTB4T24     MTB4T25\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007945    0.007944    0.007948  ...    0.007942    0.007943    0.007940\n",
      "std      0.049919    0.055963    0.057710  ...    0.054976    0.063267    0.058012\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.430396    0.595984    0.905410  ...    0.692193    0.778558    0.739012\n",
      "\n",
      "[6 rows x 13 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC4T1       MTC4T2  ...      MTB4T24      MTB4T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.017380     0.018962  ...     0.021020     0.019571\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583899     0.622391  ...     0.829293     0.825782\n",
      "\n",
      "[6 rows x 13 columns]\n",
      "Normalizing results...\n",
      "            MTC4T1       MTC4T2  ...      MTB4T24      MTB4T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003478     0.003795  ...     0.004207     0.003919\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116845     0.124564  ...     0.165977     0.165342\n",
      "\n",
      "[6 rows x 13 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 247 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 247 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 247 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       247\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  118\n",
      "root:Trinidad and Tobago    129\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       13\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                  7\n",
      "root:Trinidad and Tobago    6\n",
      "Unknown                     0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC5T1      MTC5T2      MTT5T4  ...     MTB5T23     MTB5T24     MTB5T25\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007947  ...    0.007947    0.007947    0.007948\n",
      "std      0.056735    0.059933    0.054667  ...    0.053718    0.057173    0.055186\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.576727    0.648223    0.497544  ...    0.560765    0.613848    0.562144\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC5T1       MTC5T2  ...      MTB5T24      MTB5T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.018946     0.019649  ...     0.018808     0.018573\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.581652     0.649188  ...     0.615759     0.567299\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "            MTC5T1       MTC5T2  ...      MTB5T24      MTB5T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003790     0.003931  ...     0.003763     0.003715\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116372     0.129870  ...     0.123184     0.113477\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       241\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  118\n",
      "root:Trinidad and Tobago    123\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                   7\n",
      "root:Trinidad and Tobago    12\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC6T1      MTC6T2      MTT6T3  ...     MTB6T27     MTB6T28     MTB6T29\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007939    0.007945    0.007947  ...    0.007943    0.007936    0.007943\n",
      "std      0.053960    0.073897    0.052834  ...    0.052080    0.052460    0.048549\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.602556    0.841433    0.541376  ...    0.573150    0.539394    0.459905\n",
      "\n",
      "[6 rows x 26 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC6T1       MTC6T2  ...      MTB6T28      MTB6T29\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000831     0.000832  ...     0.000831     0.000832\n",
      "std       0.018171     0.024175  ...     0.018252     0.016724\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.642746     0.860291  ...     0.609701     0.515162\n",
      "\n",
      "[6 rows x 26 columns]\n",
      "Normalizing results...\n",
      "            MTC6T1       MTC6T2  ...      MTB6T28      MTB6T29\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003639     0.004838  ...     0.003656     0.003347\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.128715     0.172149  ...     0.122142     0.103114\n",
      "\n",
      "[6 rows x 26 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 234 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 234 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 234 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       234\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  114\n",
      "root:Trinidad and Tobago    120\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       26\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                  11\n",
      "root:Trinidad and Tobago    15\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC7T1      MTC7T2      MTT7T3  ...     MTB7T22     MTB7T23     MTB7T24\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007936    0.007932    0.007942  ...    0.007939    0.007936    0.007942\n",
      "std      0.053952    0.052916    0.053572  ...    0.053664    0.059521    0.051999\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.530745    0.532680    0.550127  ...    0.509588    0.662336    0.483780\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC7T1       MTC7T2  ...      MTB7T23      MTB7T24\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000831     0.000831  ...     0.000831     0.000832\n",
      "std       0.018715     0.018684  ...     0.020256     0.017653\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.581842     0.616892  ...     0.764134     0.550841\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "            MTC7T1       MTC7T2  ...      MTB7T23      MTB7T24\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003749     0.003745  ...     0.004058     0.003534\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116564     0.123649  ...     0.153070     0.110267\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       241\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  119\n",
      "root:Trinidad and Tobago    122\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                   6\n",
      "root:Trinidad and Tobago    13\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC8T1      MTC8T2      MTT8T4  ...     MTB8T28     MTB8T29     MTB8T30\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007942    0.007940    0.007936  ...    0.007936    0.007940    0.007942\n",
      "std      0.067157    0.066833    0.048511  ...    0.053964    0.063306    0.069087\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.739490    0.757691    0.515768  ...    0.636467    0.722671    0.783274\n",
      "\n",
      "[6 rows x 22 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC8T1       MTC8T2  ...      MTB8T29      MTB8T30\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.022023     0.021988  ...     0.020875     0.022623\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.747052     0.779221  ...     0.737902     0.791875\n",
      "\n",
      "[6 rows x 22 columns]\n",
      "Normalizing results...\n",
      "            MTC8T1       MTC8T2  ...      MTB8T29      MTB8T30\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.004409     0.004402  ...     0.004180     0.004529\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.149553     0.156017  ...     0.147753     0.158524\n",
      "\n",
      "[6 rows x 22 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 238 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 238 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 238 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       238\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  113\n",
      "root:Trinidad and Tobago    125\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       22\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                  12\n",
      "root:Trinidad and Tobago    10\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           MTC9T1      MTC9T2      MTT9T4  ...     MTB9T25     MTT9T26     MTB9T27\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007941    0.007937    0.007948  ...    0.007947    0.007947    0.007947\n",
      "std      0.053760    0.051800    0.050726  ...    0.052687    0.053030    0.057928\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.514001    0.503500    0.646577  ...    0.472893    0.491400    0.604146\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            MTC9T1       MTC9T2  ...      MTT9T26      MTB9T27\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000831  ...     0.000832     0.000832\n",
      "std       0.018527     0.018223  ...     0.018088     0.019497\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.589014     0.604658  ...     0.511339     0.640589\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Normalizing results...\n",
      "            MTC9T1       MTC9T2  ...      MTT9T26      MTB9T27\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003709     0.003650  ...     0.003619     0.003901\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.117923     0.121111  ...     0.102294     0.128155\n",
      "\n",
      "[6 rows x 19 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 241 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       241\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  116\n",
      "root:Trinidad and Tobago    125\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       19\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                   9\n",
      "root:Trinidad and Tobago    10\n",
      "Unknown                      0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "          MTC10T1     MTC10T2     MTT10T4  ...    MTB10T23    MTB10T24    MTB10T25\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007937    0.007941    0.007944  ...    0.007944    0.007944    0.007942\n",
      "std      0.055321    0.056173    0.058332  ...    0.054237    0.056856    0.059745\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.568709    0.612498    0.751954  ...    0.494313    0.546296    0.646537\n",
      "\n",
      "[6 rows x 20 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "           MTC10T1      MTC10T2  ...     MTB10T24     MTB10T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000831     0.000832  ...     0.000832     0.000832\n",
      "std       0.019509     0.018880  ...     0.019402     0.020155\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.695265     0.662377  ...     0.565622     0.689491\n",
      "\n",
      "[6 rows x 20 columns]\n",
      "Normalizing results...\n",
      "           MTC10T1      MTC10T2  ...     MTB10T24     MTB10T25\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003908     0.003780  ...     0.003883     0.004035\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.139258     0.132617  ...     0.113200     0.138023\n",
      "\n",
      "[6 rows x 20 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "           BJN1T1      BJN1T2      BJN1T3  ...      TTN8T2      TTN9T1      TTN9T2\n",
      "count  629.000000  629.000000  629.000000  ...  629.000000  629.000000  629.000000\n",
      "mean     0.007946    0.007947    0.007946  ...    0.007947    0.007948    0.007949\n",
      "std      0.058261    0.056535    0.062635  ...    0.044299    0.039093    0.041647\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max      0.559495    0.568761    0.678146  ...    0.594150    0.376012    0.445141\n",
      "\n",
      "[6 rows x 260 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/629 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000832     0.000832  ...     0.000832     0.000832\n",
      "std       0.019575     0.018827  ...     0.014332     0.014310\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.583710     0.585251  ...     0.556075     0.450457\n",
      "\n",
      "[6 rows x 260 columns]\n",
      "Normalizing results...\n",
      "            BJN1T1       BJN1T2  ...       TTN9T1       TTN9T2\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.003916     0.003766  ...     0.002867     0.002862\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.116784     0.117077  ...     0.111229     0.090101\n",
      "\n",
      "[6 rows x 260 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       260\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:China                  125\n",
      "root:Trinidad and Tobago    135\n",
      "Unknown                       0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       20\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:China                  11\n",
      "root:Trinidad and Tobago     9\n",
      "Unknown                      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 116.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.86it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 208.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 31.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 222.82it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 26.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 114.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 255.24it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 27.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 314.95it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 23.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 103.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 271.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 33.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 295.52it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 193.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 30.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 253.77it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 33.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 104.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.74it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 259.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 22.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 309.06it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 91.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 203.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 32.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 231.17it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 23.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 118.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 252.86it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 30.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 231.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 28.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 177.95it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 22.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 271.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 78.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.99it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 291.73it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.44it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 298.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 89.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 177.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 29.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 219.51it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 30.07it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for((i=1; i<11; i++)); do \\\n",
    "ls experiments/exp_$i/QueryCM.tsv > tmp; expert convert -i tmp --in-cm -o experiments/exp_$i/QueryCM.h5; \\\n",
    "ls experiments/exp_$i/SourceCM.tsv > tmp; expert convert -i tmp --in-cm -o experiments/exp_$i/SourceCM.h5; \\\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/exp_$i/SourceMapper.csv -o experiments/exp_$i/SourceLabels.h5; \\\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/exp_$i/QueryMapper.csv -o experiments/exp_$i/QueryLabels.h5; \\\n",
    "done\n",
    "rm tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordering labels and samples...\n",
      "Total matched samples: 249\n",
      "Total correct samples: 249?249\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.005865  0.034154\n",
      "4      0.003519  0.020493\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000055\n",
      "18016  0.000003  0.000044\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 0.8153 - acc: 0.5134 - auROC: 0.5884 - val_loss: 0.7242 - val_acc: 0.5000 - val_auROC: 0.6320\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.7252 - acc: 0.5312 - auROC: 0.4911 - val_loss: 0.6589 - val_acc: 0.6800 - val_auROC: 0.6400\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6912 - acc: 0.5759 - auROC: 0.5351 - val_loss: 0.6630 - val_acc: 0.6600 - val_auROC: 0.6632\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6803 - acc: 0.5871 - auROC: 0.6181 - val_loss: 0.6864 - val_acc: 0.6200 - val_auROC: 0.6496\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6430 - acc: 0.6272 - auROC: 0.7722 - val_loss: 0.6940 - val_acc: 0.6600 - val_auROC: 0.6968\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6220 - acc: 0.6205 - auROC: 0.8280 - val_loss: 0.6762 - val_acc: 0.7200 - val_auROC: 0.7392\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6100 - acc: 0.6652 - auROC: 0.8410 - val_loss: 0.6516 - val_acc: 0.7200 - val_auROC: 0.7856\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5989 - acc: 0.7277 - auROC: 0.8544 - val_loss: 0.6313 - val_acc: 0.7000 - val_auROC: 0.7792\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5921 - acc: 0.7545 - auROC: 0.8549 - val_loss: 0.6354 - val_acc: 0.7000 - val_auROC: 0.7736\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5835 - acc: 0.7768 - auROC: 0.8647 - val_loss: 0.6279 - val_acc: 0.7000 - val_auROC: 0.7896\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5767 - acc: 0.7991 - auROC: 0.8634 - val_loss: 0.6181 - val_acc: 0.6800 - val_auROC: 0.7888\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5704 - acc: 0.8214 - auROC: 0.8716 - val_loss: 0.6106 - val_acc: 0.6800 - val_auROC: 0.8024\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.5645 - acc: 0.8326 - auROC: 0.8732 - val_loss: 0.6063 - val_acc: 0.7000 - val_auROC: 0.8064\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5582 - acc: 0.8393 - auROC: 0.8775 - val_loss: 0.5997 - val_acc: 0.7000 - val_auROC: 0.8224\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5516 - acc: 0.8393 - auROC: 0.8800 - val_loss: 0.5944 - val_acc: 0.7200 - val_auROC: 0.8256\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5434 - acc: 0.8393 - auROC: 0.8842 - val_loss: 0.5863 - val_acc: 0.7200 - val_auROC: 0.8392\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5356 - acc: 0.8259 - auROC: 0.8867 - val_loss: 0.5769 - val_acc: 0.7400 - val_auROC: 0.8408\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5267 - acc: 0.8192 - auROC: 0.8862 - val_loss: 0.5718 - val_acc: 0.7600 - val_auROC: 0.8240\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5167 - acc: 0.8214 - auROC: 0.8916 - val_loss: 0.5774 - val_acc: 0.7400 - val_auROC: 0.8240\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5082 - acc: 0.8237 - auROC: 0.8936 - val_loss: 0.5865 - val_acc: 0.7600 - val_auROC: 0.8136\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4993 - acc: 0.8304 - auROC: 0.8961 - val_loss: 0.5784 - val_acc: 0.6800 - val_auROC: 0.8024\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4909 - acc: 0.7991 - auROC: 0.8981 - val_loss: 0.5665 - val_acc: 0.6800 - val_auROC: 0.8208\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4830 - acc: 0.8036 - auROC: 0.9016 - val_loss: 0.5637 - val_acc: 0.6600 - val_auROC: 0.8152\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4775 - acc: 0.7969 - auROC: 0.9028 - val_loss: 0.5649 - val_acc: 0.6600 - val_auROC: 0.8064\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4714 - acc: 0.7969 - auROC: 0.9052 - val_loss: 0.5589 - val_acc: 0.6800 - val_auROC: 0.8184\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4649 - acc: 0.8192 - auROC: 0.9069 - val_loss: 0.5577 - val_acc: 0.7600 - val_auROC: 0.8136\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4581 - acc: 0.8862 - auROC: 0.9082 - val_loss: 0.5579 - val_acc: 0.7800 - val_auROC: 0.8136\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4528 - acc: 0.8884 - auROC: 0.9119 - val_loss: 0.5534 - val_acc: 0.7600 - val_auROC: 0.8136\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4471 - acc: 0.8906 - auROC: 0.9125 - val_loss: 0.5497 - val_acc: 0.7800 - val_auROC: 0.8096\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4416 - acc: 0.8929 - auROC: 0.9154 - val_loss: 0.5499 - val_acc: 0.7800 - val_auROC: 0.8096\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4367 - acc: 0.8929 - auROC: 0.9166 - val_loss: 0.5468 - val_acc: 0.7800 - val_auROC: 0.8072\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4315 - acc: 0.8929 - auROC: 0.9191 - val_loss: 0.5468 - val_acc: 0.7800 - val_auROC: 0.8096\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4268 - acc: 0.8929 - auROC: 0.9218 - val_loss: 0.5435 - val_acc: 0.7800 - val_auROC: 0.8112\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4216 - acc: 0.8929 - auROC: 0.9217 - val_loss: 0.5408 - val_acc: 0.7800 - val_auROC: 0.8128\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4166 - acc: 0.8951 - auROC: 0.9226 - val_loss: 0.5399 - val_acc: 0.7800 - val_auROC: 0.8112\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4119 - acc: 0.8951 - auROC: 0.9228 - val_loss: 0.5388 - val_acc: 0.7800 - val_auROC: 0.8136\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4068 - acc: 0.8951 - auROC: 0.9243 - val_loss: 0.5386 - val_acc: 0.7800 - val_auROC: 0.8152\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4020 - acc: 0.8973 - auROC: 0.9287 - val_loss: 0.5386 - val_acc: 0.7800 - val_auROC: 0.8120\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3977 - acc: 0.8996 - auROC: 0.9288 - val_loss: 0.5390 - val_acc: 0.7600 - val_auROC: 0.8048\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3929 - acc: 0.9018 - auROC: 0.9325 - val_loss: 0.5380 - val_acc: 0.7400 - val_auROC: 0.8064\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3886 - acc: 0.8996 - auROC: 0.9318 - val_loss: 0.5445 - val_acc: 0.7400 - val_auROC: 0.8032\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3831 - acc: 0.9040 - auROC: 0.9363 - val_loss: 0.5531 - val_acc: 0.7200 - val_auROC: 0.7984\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3781 - acc: 0.9062 - auROC: 0.9367 - val_loss: 0.5430 - val_acc: 0.7200 - val_auROC: 0.7992\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3718 - acc: 0.9129 - auROC: 0.9386 - val_loss: 0.5550 - val_acc: 0.7200 - val_auROC: 0.7896\n",
      "Epoch 45/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3119 - acc: 0.9688 - auROC: 0.9709\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3675 - acc: 0.9152 - auROC: 0.9388 - val_loss: 0.5470 - val_acc: 0.7200 - val_auROC: 0.7840\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3638 - acc: 0.9152 - auROC: 0.9391 - val_loss: 0.5430 - val_acc: 0.7200 - val_auROC: 0.7904\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3634 - acc: 0.9152 - auROC: 0.9390 - val_loss: 0.5437 - val_acc: 0.7200 - val_auROC: 0.7904\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3628 - acc: 0.9174 - auROC: 0.9386 - val_loss: 0.5439 - val_acc: 0.7200 - val_auROC: 0.7896\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3622 - acc: 0.9174 - auROC: 0.9384 - val_loss: 0.5517 - val_acc: 0.7200 - val_auROC: 0.7832\n",
      "Epoch 50/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3397 - acc: 0.9453 - auROC: 0.9608\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3617 - acc: 0.9174 - auROC: 0.9390 - val_loss: 0.5568 - val_acc: 0.7200 - val_auROC: 0.7832\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3613 - acc: 0.9174 - auROC: 0.9392 - val_loss: 0.5571 - val_acc: 0.7200 - val_auROC: 0.7840\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3612 - acc: 0.9174 - auROC: 0.9392 - val_loss: 0.5570 - val_acc: 0.7200 - val_auROC: 0.7840\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3612 - acc: 0.9174 - auROC: 0.9394 - val_loss: 0.5564 - val_acc: 0.7200 - val_auROC: 0.7840\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3611 - acc: 0.9174 - auROC: 0.9393 - val_loss: 0.5559 - val_acc: 0.7200 - val_auROC: 0.7840\n",
      "Epoch 55/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3587 - acc: 0.9375 - auROC: 0.9386\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3611 - acc: 0.9174 - auROC: 0.9393 - val_loss: 0.5546 - val_acc: 0.7200 - val_auROC: 0.7840\n",
      "Epoch 00055: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 55/354\n",
      "4/4 [==============================] - 1s 195ms/step - loss: 0.3894 - acc: 0.8973 - auROC: 0.9355 - val_loss: 0.5357 - val_acc: 0.7400 - val_auROC: 0.8120\n",
      "Epoch 56/354\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3857 - acc: 0.9062 - auROC: 0.9378 - val_loss: 0.5373 - val_acc: 0.7400 - val_auROC: 0.8120\n",
      "Epoch 57/354\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3836 - acc: 0.9062 - auROC: 0.9383 - val_loss: 0.5380 - val_acc: 0.7400 - val_auROC: 0.8112\n",
      "Epoch 58/354\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3816 - acc: 0.9085 - auROC: 0.9397 - val_loss: 0.5379 - val_acc: 0.7400 - val_auROC: 0.8112\n",
      "Epoch 59/354\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3787 - acc: 0.9152 - auROC: 0.9403 - val_loss: 0.5403 - val_acc: 0.7200 - val_auROC: 0.8080\n",
      "Epoch 60/354\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3768 - acc: 0.9174 - auROC: 0.9408 - val_loss: 0.5424 - val_acc: 0.7200 - val_auROC: 0.8048\n",
      "Epoch 61/354\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3753 - acc: 0.9219 - auROC: 0.9413 - val_loss: 0.5435 - val_acc: 0.7200 - val_auROC: 0.8048\n",
      "Epoch 62/354\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3740 - acc: 0.9219 - auROC: 0.9418 - val_loss: 0.5453 - val_acc: 0.7200 - val_auROC: 0.7984\n",
      "Epoch 63/354\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3727 - acc: 0.9241 - auROC: 0.9423 - val_loss: 0.5478 - val_acc: 0.7400 - val_auROC: 0.7992\n",
      "Epoch 64/354\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3715 - acc: 0.9241 - auROC: 0.9429 - val_loss: 0.5501 - val_acc: 0.7200 - val_auROC: 0.7992\n",
      "Epoch 65/354\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3705 - acc: 0.9241 - auROC: 0.9431 - val_loss: 0.5517 - val_acc: 0.7200 - val_auROC: 0.7984\n",
      "Epoch 66/354\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3696 - acc: 0.9241 - auROC: 0.9427 - val_loss: 0.5532 - val_acc: 0.7200 - val_auROC: 0.7944\n",
      "Epoch 67/354\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3687 - acc: 0.9241 - auROC: 0.9428 - val_loss: 0.5547 - val_acc: 0.7200 - val_auROC: 0.7936\n",
      "Epoch 68/354\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3678 - acc: 0.9241 - auROC: 0.9435 - val_loss: 0.5566 - val_acc: 0.7200 - val_auROC: 0.7888\n",
      "Epoch 69/354\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3671 - acc: 0.9241 - auROC: 0.9437 - val_loss: 0.5582 - val_acc: 0.7200 - val_auROC: 0.7872\n",
      "Epoch 70/354\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3715 - acc: 0.9219 - auROC: 0.9392Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.3664 - acc: 0.9241 - auROC: 0.9436 - val_loss: 0.5603 - val_acc: 0.7200 - val_auROC: 0.7848\n",
      "Epoch 00070: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.171731  ...    0.0 -0.214360 -0.073097    0.0\n",
      "1     0.0    0.0    0.0 -0.171731  ...    0.0  0.076654  0.017284    0.0\n",
      "2     0.0    0.0    0.0 -0.171731  ...    0.0 -0.214360 -0.073097    0.0\n",
      "3     0.0    0.0    0.0 -0.171731  ...    0.0 -0.065194  0.019557    0.0\n",
      "4     0.0    0.0    0.0 -0.171731  ...    0.0 -0.214360 -0.073097    0.0\n",
      "5     0.0    0.0    0.0 -0.171731  ...    0.0 -0.214360 -0.073097    0.0\n",
      "6     0.0    0.0    0.0 -0.170712  ...    0.0  0.008378 -0.073097    0.0\n",
      "7     0.0    0.0    0.0 -0.171731  ...    0.0 -0.080360 -0.073097    0.0\n",
      "8     0.0    0.0    0.0 -0.171731  ...    0.0 -0.214360 -0.073097    0.0\n",
      "9     0.0    0.0    0.0 -0.171731  ...    0.0 -0.086484 -0.073097    0.0\n",
      "10    0.0    0.0    0.0 -0.171731  ...    0.0 -0.117562 -0.073097    0.0\n",
      "\n",
      "[11 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC  F-max\n",
      "t                                  ...                                       \n",
      "0.00   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2      0.0    0.2\n",
      "0.01   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2      0.0    0.2\n",
      "0.02   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2      0.0    0.2\n",
      "0.03   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2      0.0    0.2\n",
      "0.04   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2      0.0    0.2\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...    ...\n",
      "0.97   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN      0.0    0.2\n",
      "0.98   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN      0.0    0.2\n",
      "0.99   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN      0.0    0.2\n",
      "1.00   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN      0.0    0.2\n",
      "1.01   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN      0.0    0.2\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0   1   0   8  0.8889  1.0  ...  1.0  1.0  0.8889  0.9412   0.9286  0.9412\n",
      "0.01   0   1   0   8  0.8889  1.0  ...  1.0  1.0  0.8889  0.9412   0.9286  0.9412\n",
      "0.02   0   1   0   8  0.8889  1.0  ...  1.0  1.0  0.8889  0.9412   0.9286  0.9412\n",
      "0.03   0   1   0   8  0.8889  1.0  ...  1.0  1.0  0.8889  0.9412   0.9286  0.9412\n",
      "0.04   0   1   0   8  0.8889  1.0  ...  1.0  1.0  0.8889  0.9412   0.9286  0.9412\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97   1   0   8   0  0.1111  0.0  ...  0.0  0.0  0.0000     NaN   0.9286  0.9412\n",
      "0.98   1   0   8   0  0.1111  0.0  ...  0.0  0.0  0.0000     NaN   0.9286  0.9412\n",
      "0.99   1   0   8   0  0.1111  0.0  ...  0.0  0.0  0.0000     NaN   0.9286  0.9412\n",
      "1.00   1   0   8   0  0.1111  0.0  ...  0.0  0.0  0.0000     NaN   0.9286  0.9412\n",
      "1.01   1   0   8   0  0.1111  0.0  ...  0.0  0.0  0.0000     NaN   0.9286  0.9412\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC  F-max\n",
      "t                                  ...                                       \n",
      "0.00   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3571   0.25\n",
      "0.01   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3571   0.25\n",
      "0.02   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3571   0.25\n",
      "0.03   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3571   0.25\n",
      "0.04   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3571   0.25\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...    ...\n",
      "0.97   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3571   0.25\n",
      "0.98   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3571   0.25\n",
      "0.99   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3571   0.25\n",
      "1.00   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3571   0.25\n",
      "1.01   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3571   0.25\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC   F-max\n",
      "t                                  ...                                        \n",
      "0.00   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.4286  0.2857\n",
      "0.01   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.4286  0.2857\n",
      "0.02   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.4286  0.2857\n",
      "0.03   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.4286  0.2857\n",
      "0.04   0   8   0   1  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.4286  0.2857\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...     ...\n",
      "0.97   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.4286  0.2857\n",
      "0.98   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.4286  0.2857\n",
      "0.99   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.4286  0.2857\n",
      "1.00   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.4286  0.2857\n",
      "1.01   8   0   1   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.4286  0.2857\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0   4   0   5  0.5556  1.0  ...  1.0  1.0  0.5556  0.7143      1.0    1.0\n",
      "0.01   0   4   0   5  0.5556  1.0  ...  1.0  1.0  0.5556  0.7143      1.0    1.0\n",
      "0.02   0   4   0   5  0.5556  1.0  ...  1.0  1.0  0.5556  0.7143      1.0    1.0\n",
      "0.03   0   4   0   5  0.5556  1.0  ...  1.0  1.0  0.5556  0.7143      1.0    1.0\n",
      "0.04   0   4   0   5  0.5556  1.0  ...  1.0  1.0  0.5556  0.7143      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97   4   0   5   0  0.4444  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98   4   0   5   0  0.4444  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99   4   0   5   0  0.4444  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00   4   0   5   0  0.4444  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01   4   0   5   0  0.4444  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0   9   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0   9   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0   9   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0   9   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0   9   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97   9   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98   9   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99   9   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00   9   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01   9   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 242\n",
      "Total correct samples: 242?242\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006035  0.034630\n",
      "4      0.003621  0.020778\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000055\n",
      "18016  0.000003  0.000045\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 0.8218 - acc: 0.5023 - auROC: 0.5918 - val_loss: 0.7149 - val_acc: 0.5000 - val_auROC: 0.6096\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.7184 - acc: 0.5207 - auROC: 0.5359 - val_loss: 0.6720 - val_acc: 0.5200 - val_auROC: 0.6112\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6731 - acc: 0.5507 - auROC: 0.6484 - val_loss: 0.6488 - val_acc: 0.5600 - val_auROC: 0.7224\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6390 - acc: 0.5806 - auROC: 0.7650 - val_loss: 0.6486 - val_acc: 0.6200 - val_auROC: 0.7416\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6148 - acc: 0.6636 - auROC: 0.8116 - val_loss: 0.6561 - val_acc: 0.6400 - val_auROC: 0.7128\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6051 - acc: 0.7258 - auROC: 0.8148 - val_loss: 0.6774 - val_acc: 0.6200 - val_auROC: 0.6696\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5903 - acc: 0.7535 - auROC: 0.8393 - val_loss: 0.6173 - val_acc: 0.6400 - val_auROC: 0.7824\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5793 - acc: 0.7627 - auROC: 0.8528 - val_loss: 0.5973 - val_acc: 0.6600 - val_auROC: 0.8192\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5683 - acc: 0.7604 - auROC: 0.8703 - val_loss: 0.6177 - val_acc: 0.7000 - val_auROC: 0.7696\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5573 - acc: 0.7719 - auROC: 0.8773 - val_loss: 0.6339 - val_acc: 0.6400 - val_auROC: 0.7176\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5451 - acc: 0.7627 - auROC: 0.8817 - val_loss: 0.6392 - val_acc: 0.6200 - val_auROC: 0.7200\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5346 - acc: 0.7673 - auROC: 0.8898 - val_loss: 0.6225 - val_acc: 0.6400 - val_auROC: 0.7480\n",
      "Epoch 13/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5257 - acc: 0.8047 - auROC: 0.8987\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5277 - acc: 0.7811 - auROC: 0.8927 - val_loss: 0.5984 - val_acc: 0.7200 - val_auROC: 0.7736\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5228 - acc: 0.7972 - auROC: 0.8977 - val_loss: 0.5939 - val_acc: 0.7200 - val_auROC: 0.7792\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5205 - acc: 0.7949 - auROC: 0.8993 - val_loss: 0.5894 - val_acc: 0.6800 - val_auROC: 0.7944\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5189 - acc: 0.8018 - auROC: 0.9001 - val_loss: 0.5877 - val_acc: 0.6800 - val_auROC: 0.7952\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5180 - acc: 0.8018 - auROC: 0.8989 - val_loss: 0.5874 - val_acc: 0.6800 - val_auROC: 0.7952\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5163 - acc: 0.7995 - auROC: 0.9012 - val_loss: 0.5882 - val_acc: 0.6800 - val_auROC: 0.7952\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5148 - acc: 0.7926 - auROC: 0.9021 - val_loss: 0.5920 - val_acc: 0.6600 - val_auROC: 0.7928\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5134 - acc: 0.7880 - auROC: 0.9023 - val_loss: 0.5976 - val_acc: 0.6600 - val_auROC: 0.7880\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5122 - acc: 0.7834 - auROC: 0.9047 - val_loss: 0.6031 - val_acc: 0.6600 - val_auROC: 0.7808\n",
      "Epoch 22/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5064 - acc: 0.7891 - auROC: 0.9221\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5113 - acc: 0.7811 - auROC: 0.9045 - val_loss: 0.6104 - val_acc: 0.6600 - val_auROC: 0.7568\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5107 - acc: 0.7788 - auROC: 0.9039 - val_loss: 0.6109 - val_acc: 0.6600 - val_auROC: 0.7552\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5107 - acc: 0.7788 - auROC: 0.9041 - val_loss: 0.6112 - val_acc: 0.6600 - val_auROC: 0.7536\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5106 - acc: 0.7788 - auROC: 0.9039 - val_loss: 0.6110 - val_acc: 0.6600 - val_auROC: 0.7536\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5105 - acc: 0.7788 - auROC: 0.9040 - val_loss: 0.6108 - val_acc: 0.6600 - val_auROC: 0.7536\n",
      "Epoch 27/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5155 - acc: 0.7500 - auROC: 0.9009\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5104 - acc: 0.7788 - auROC: 0.9044 - val_loss: 0.6103 - val_acc: 0.6600 - val_auROC: 0.7560\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5103 - acc: 0.7788 - auROC: 0.9044 - val_loss: 0.6100 - val_acc: 0.6600 - val_auROC: 0.7576\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5102 - acc: 0.7788 - auROC: 0.9044 - val_loss: 0.6097 - val_acc: 0.6600 - val_auROC: 0.7576\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5102 - acc: 0.7788 - auROC: 0.9044 - val_loss: 0.6094 - val_acc: 0.6600 - val_auROC: 0.7576\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5101 - acc: 0.7788 - auROC: 0.9048 - val_loss: 0.6089 - val_acc: 0.6600 - val_auROC: 0.7608\n",
      "Epoch 32/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5070 - acc: 0.8047 - auROC: 0.9246Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5100 - acc: 0.7788 - auROC: 0.9048 - val_loss: 0.6085 - val_acc: 0.6600 - val_auROC: 0.7608\n",
      "Epoch 00032: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 32/331\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 0.5167 - acc: 0.8041 - auROC: 0.8994 - val_loss: 0.5871 - val_acc: 0.7000 - val_auROC: 0.7944\n",
      "Epoch 33/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5122 - acc: 0.7949 - auROC: 0.9028 - val_loss: 0.5873 - val_acc: 0.7000 - val_auROC: 0.7976\n",
      "Epoch 34/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5091 - acc: 0.7834 - auROC: 0.9070 - val_loss: 0.5864 - val_acc: 0.6800 - val_auROC: 0.8016\n",
      "Epoch 35/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5069 - acc: 0.7857 - auROC: 0.9110 - val_loss: 0.5837 - val_acc: 0.6800 - val_auROC: 0.8032\n",
      "Epoch 36/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.5050 - acc: 0.7857 - auROC: 0.9128 - val_loss: 0.5822 - val_acc: 0.6800 - val_auROC: 0.8032\n",
      "Epoch 37/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.5026 - acc: 0.7926 - auROC: 0.9142 - val_loss: 0.5819 - val_acc: 0.6800 - val_auROC: 0.8040\n",
      "Epoch 38/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.5006 - acc: 0.7972 - auROC: 0.9165 - val_loss: 0.5808 - val_acc: 0.6800 - val_auROC: 0.8064\n",
      "Epoch 39/331\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.4987 - acc: 0.7995 - auROC: 0.9188 - val_loss: 0.5797 - val_acc: 0.6800 - val_auROC: 0.8080\n",
      "Epoch 40/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4970 - acc: 0.8018 - auROC: 0.9206 - val_loss: 0.5796 - val_acc: 0.6800 - val_auROC: 0.8088\n",
      "Epoch 41/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4949 - acc: 0.7995 - auROC: 0.9223 - val_loss: 0.5795 - val_acc: 0.6800 - val_auROC: 0.8088\n",
      "Epoch 42/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4931 - acc: 0.8018 - auROC: 0.9245 - val_loss: 0.5790 - val_acc: 0.6800 - val_auROC: 0.8136\n",
      "Epoch 43/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4911 - acc: 0.7926 - auROC: 0.9266 - val_loss: 0.5775 - val_acc: 0.6800 - val_auROC: 0.8104\n",
      "Epoch 44/331\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4894 - acc: 0.7926 - auROC: 0.9272 - val_loss: 0.5762 - val_acc: 0.6800 - val_auROC: 0.8168\n",
      "Epoch 45/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4879 - acc: 0.7926 - auROC: 0.9286 - val_loss: 0.5761 - val_acc: 0.6800 - val_auROC: 0.8168\n",
      "Epoch 46/331\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4863 - acc: 0.8018 - auROC: 0.9300 - val_loss: 0.5753 - val_acc: 0.6800 - val_auROC: 0.8184\n",
      "Epoch 47/331\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4846 - acc: 0.8041 - auROC: 0.9318 - val_loss: 0.5738 - val_acc: 0.6800 - val_auROC: 0.8200\n",
      "Epoch 48/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4825 - acc: 0.8088 - auROC: 0.9346 - val_loss: 0.5741 - val_acc: 0.6800 - val_auROC: 0.8168\n",
      "Epoch 49/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4809 - acc: 0.8065 - auROC: 0.9363 - val_loss: 0.5744 - val_acc: 0.6800 - val_auROC: 0.8144\n",
      "Epoch 50/331\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4796 - acc: 0.8088 - auROC: 0.9375 - val_loss: 0.5741 - val_acc: 0.6600 - val_auROC: 0.8144\n",
      "Epoch 51/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4779 - acc: 0.8157 - auROC: 0.9395 - val_loss: 0.5737 - val_acc: 0.6600 - val_auROC: 0.8184\n",
      "Epoch 52/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4762 - acc: 0.8157 - auROC: 0.9416 - val_loss: 0.5724 - val_acc: 0.6600 - val_auROC: 0.8216\n",
      "Epoch 53/331\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4749 - acc: 0.8180 - auROC: 0.9423 - val_loss: 0.5709 - val_acc: 0.6600 - val_auROC: 0.8224\n",
      "Epoch 54/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4732 - acc: 0.8180 - auROC: 0.9438 - val_loss: 0.5698 - val_acc: 0.6800 - val_auROC: 0.8216\n",
      "Epoch 55/331\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4717 - acc: 0.8111 - auROC: 0.9450 - val_loss: 0.5672 - val_acc: 0.6800 - val_auROC: 0.8248\n",
      "Epoch 56/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4706 - acc: 0.8041 - auROC: 0.9470 - val_loss: 0.5659 - val_acc: 0.6800 - val_auROC: 0.8256\n",
      "Epoch 57/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4689 - acc: 0.8157 - auROC: 0.9488 - val_loss: 0.5661 - val_acc: 0.7000 - val_auROC: 0.8256\n",
      "Epoch 58/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4677 - acc: 0.8157 - auROC: 0.9494 - val_loss: 0.5645 - val_acc: 0.7000 - val_auROC: 0.8272\n",
      "Epoch 59/331\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.4665 - acc: 0.8111 - auROC: 0.9501 - val_loss: 0.5632 - val_acc: 0.7000 - val_auROC: 0.8328\n",
      "Epoch 60/331\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4656 - acc: 0.8041 - auROC: 0.9511 - val_loss: 0.5622 - val_acc: 0.7000 - val_auROC: 0.8416\n",
      "Epoch 61/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4645 - acc: 0.8041 - auROC: 0.9520 - val_loss: 0.5623 - val_acc: 0.7000 - val_auROC: 0.8376\n",
      "Epoch 62/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4634 - acc: 0.8088 - auROC: 0.9523 - val_loss: 0.5617 - val_acc: 0.7000 - val_auROC: 0.8360\n",
      "Epoch 63/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4621 - acc: 0.8111 - auROC: 0.9537 - val_loss: 0.5620 - val_acc: 0.7000 - val_auROC: 0.8384\n",
      "Epoch 64/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4610 - acc: 0.8111 - auROC: 0.9541 - val_loss: 0.5623 - val_acc: 0.7000 - val_auROC: 0.8360\n",
      "Epoch 65/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4600 - acc: 0.8041 - auROC: 0.9547 - val_loss: 0.5619 - val_acc: 0.7000 - val_auROC: 0.8376\n",
      "Epoch 66/331\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4592 - acc: 0.8065 - auROC: 0.9549 - val_loss: 0.5615 - val_acc: 0.7000 - val_auROC: 0.8392\n",
      "Epoch 67/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4583 - acc: 0.8088 - auROC: 0.9552 - val_loss: 0.5607 - val_acc: 0.7000 - val_auROC: 0.8392\n",
      "Epoch 68/331\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.4578 - acc: 0.8018 - auROC: 0.9557 - val_loss: 0.5596 - val_acc: 0.6800 - val_auROC: 0.8392\n",
      "Epoch 69/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4570 - acc: 0.8018 - auROC: 0.9558 - val_loss: 0.5599 - val_acc: 0.7000 - val_auROC: 0.8400\n",
      "Epoch 70/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4564 - acc: 0.8065 - auROC: 0.9561 - val_loss: 0.5592 - val_acc: 0.6800 - val_auROC: 0.8400\n",
      "Epoch 71/331\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4557 - acc: 0.8041 - auROC: 0.9562 - val_loss: 0.5579 - val_acc: 0.6800 - val_auROC: 0.8416\n",
      "Epoch 72/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4552 - acc: 0.8111 - auROC: 0.9566 - val_loss: 0.5567 - val_acc: 0.6800 - val_auROC: 0.8432\n",
      "Epoch 73/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4546 - acc: 0.8157 - auROC: 0.9568 - val_loss: 0.5566 - val_acc: 0.6800 - val_auROC: 0.8424\n",
      "Epoch 74/331\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.4539 - acc: 0.8157 - auROC: 0.9572 - val_loss: 0.5546 - val_acc: 0.6800 - val_auROC: 0.8432\n",
      "Epoch 75/331\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.4533 - acc: 0.8111 - auROC: 0.9574 - val_loss: 0.5527 - val_acc: 0.6800 - val_auROC: 0.8456\n",
      "Epoch 76/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4527 - acc: 0.8111 - auROC: 0.9580 - val_loss: 0.5530 - val_acc: 0.6800 - val_auROC: 0.8448\n",
      "Epoch 77/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4521 - acc: 0.8180 - auROC: 0.9583 - val_loss: 0.5525 - val_acc: 0.6800 - val_auROC: 0.8464\n",
      "Epoch 78/331\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.4515 - acc: 0.8203 - auROC: 0.9587 - val_loss: 0.5517 - val_acc: 0.6800 - val_auROC: 0.8464\n",
      "Epoch 79/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4508 - acc: 0.8226 - auROC: 0.9593 - val_loss: 0.5506 - val_acc: 0.6800 - val_auROC: 0.8472\n",
      "Epoch 80/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4503 - acc: 0.8088 - auROC: 0.9596 - val_loss: 0.5487 - val_acc: 0.6800 - val_auROC: 0.8512\n",
      "Epoch 81/331\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4495 - acc: 0.8065 - auROC: 0.9603 - val_loss: 0.5478 - val_acc: 0.6800 - val_auROC: 0.8552\n",
      "Epoch 82/331\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.4487 - acc: 0.8180 - auROC: 0.9612 - val_loss: 0.5464 - val_acc: 0.6800 - val_auROC: 0.8552\n",
      "Epoch 83/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4481 - acc: 0.8180 - auROC: 0.9615 - val_loss: 0.5462 - val_acc: 0.6800 - val_auROC: 0.8568\n",
      "Epoch 84/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4476 - acc: 0.8134 - auROC: 0.9614 - val_loss: 0.5444 - val_acc: 0.7000 - val_auROC: 0.8592\n",
      "Epoch 85/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4470 - acc: 0.8157 - auROC: 0.9615 - val_loss: 0.5452 - val_acc: 0.7000 - val_auROC: 0.8584\n",
      "Epoch 86/331\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4466 - acc: 0.8249 - auROC: 0.9618 - val_loss: 0.5447 - val_acc: 0.7000 - val_auROC: 0.8584\n",
      "Epoch 87/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4461 - acc: 0.8226 - auROC: 0.9618 - val_loss: 0.5434 - val_acc: 0.7000 - val_auROC: 0.8600\n",
      "Epoch 88/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4457 - acc: 0.8180 - auROC: 0.9621 - val_loss: 0.5429 - val_acc: 0.7000 - val_auROC: 0.8608\n",
      "Epoch 89/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4452 - acc: 0.8203 - auROC: 0.9623 - val_loss: 0.5432 - val_acc: 0.7000 - val_auROC: 0.8600\n",
      "Epoch 90/331\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4448 - acc: 0.8272 - auROC: 0.9624 - val_loss: 0.5422 - val_acc: 0.7200 - val_auROC: 0.8608\n",
      "Epoch 91/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4446 - acc: 0.8272 - auROC: 0.9625 - val_loss: 0.5402 - val_acc: 0.7200 - val_auROC: 0.8624\n",
      "Epoch 92/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4441 - acc: 0.8226 - auROC: 0.9628 - val_loss: 0.5411 - val_acc: 0.7200 - val_auROC: 0.8608\n",
      "Epoch 93/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4437 - acc: 0.8249 - auROC: 0.9632 - val_loss: 0.5415 - val_acc: 0.7200 - val_auROC: 0.8616\n",
      "Epoch 94/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4433 - acc: 0.8249 - auROC: 0.9633 - val_loss: 0.5406 - val_acc: 0.7200 - val_auROC: 0.8624\n",
      "Epoch 95/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4429 - acc: 0.8272 - auROC: 0.9631 - val_loss: 0.5392 - val_acc: 0.7200 - val_auROC: 0.8624\n",
      "Epoch 96/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4425 - acc: 0.8272 - auROC: 0.9631 - val_loss: 0.5388 - val_acc: 0.7200 - val_auROC: 0.8632\n",
      "Epoch 97/331\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4420 - acc: 0.8295 - auROC: 0.9635 - val_loss: 0.5394 - val_acc: 0.7200 - val_auROC: 0.8624\n",
      "Epoch 98/331\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.4414 - acc: 0.8272 - auROC: 0.9637 - val_loss: 0.5386 - val_acc: 0.7200 - val_auROC: 0.8624\n",
      "Epoch 99/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4405 - acc: 0.8295 - auROC: 0.9644 - val_loss: 0.5394 - val_acc: 0.7200 - val_auROC: 0.8576\n",
      "Epoch 100/331\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4392 - acc: 0.8295 - auROC: 0.9658 - val_loss: 0.5391 - val_acc: 0.7200 - val_auROC: 0.8576\n",
      "Epoch 101/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4387 - acc: 0.8295 - auROC: 0.9660 - val_loss: 0.5374 - val_acc: 0.7200 - val_auROC: 0.8600\n",
      "Epoch 102/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4383 - acc: 0.8272 - auROC: 0.9664 - val_loss: 0.5363 - val_acc: 0.7200 - val_auROC: 0.8600\n",
      "Epoch 103/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4378 - acc: 0.8295 - auROC: 0.9667 - val_loss: 0.5370 - val_acc: 0.7200 - val_auROC: 0.8640\n",
      "Epoch 104/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4374 - acc: 0.8295 - auROC: 0.9675 - val_loss: 0.5361 - val_acc: 0.7200 - val_auROC: 0.8688\n",
      "Epoch 105/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4370 - acc: 0.8203 - auROC: 0.9678 - val_loss: 0.5331 - val_acc: 0.7200 - val_auROC: 0.8712\n",
      "Epoch 106/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4364 - acc: 0.8203 - auROC: 0.9680 - val_loss: 0.5321 - val_acc: 0.7200 - val_auROC: 0.8704\n",
      "Epoch 107/331\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.4358 - acc: 0.8272 - auROC: 0.9682 - val_loss: 0.5315 - val_acc: 0.7200 - val_auROC: 0.8696\n",
      "Epoch 108/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4352 - acc: 0.8272 - auROC: 0.9683 - val_loss: 0.5282 - val_acc: 0.7200 - val_auROC: 0.8752\n",
      "Epoch 109/331\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.4348 - acc: 0.8180 - auROC: 0.9680 - val_loss: 0.5269 - val_acc: 0.7200 - val_auROC: 0.8784\n",
      "Epoch 110/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4342 - acc: 0.8249 - auROC: 0.9684 - val_loss: 0.5255 - val_acc: 0.7200 - val_auROC: 0.8792\n",
      "Epoch 111/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4338 - acc: 0.8249 - auROC: 0.9684 - val_loss: 0.5258 - val_acc: 0.7200 - val_auROC: 0.8792\n",
      "Epoch 112/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4333 - acc: 0.8226 - auROC: 0.9687 - val_loss: 0.5264 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 113/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4328 - acc: 0.8203 - auROC: 0.9682 - val_loss: 0.5273 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 114/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4323 - acc: 0.8226 - auROC: 0.9689 - val_loss: 0.5263 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 115/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4319 - acc: 0.8249 - auROC: 0.9689 - val_loss: 0.5256 - val_acc: 0.7200 - val_auROC: 0.8816\n",
      "Epoch 116/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4314 - acc: 0.8295 - auROC: 0.9691 - val_loss: 0.5260 - val_acc: 0.7200 - val_auROC: 0.8824\n",
      "Epoch 117/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4310 - acc: 0.8295 - auROC: 0.9699 - val_loss: 0.5269 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 118/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4305 - acc: 0.8295 - auROC: 0.9703 - val_loss: 0.5268 - val_acc: 0.7200 - val_auROC: 0.8816\n",
      "Epoch 119/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4300 - acc: 0.8295 - auROC: 0.9704 - val_loss: 0.5251 - val_acc: 0.7200 - val_auROC: 0.8824\n",
      "Epoch 120/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4295 - acc: 0.8249 - auROC: 0.9705 - val_loss: 0.5258 - val_acc: 0.7200 - val_auROC: 0.8816\n",
      "Epoch 121/331\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4291 - acc: 0.8272 - auROC: 0.9706 - val_loss: 0.5266 - val_acc: 0.7200 - val_auROC: 0.8816\n",
      "Epoch 122/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4287 - acc: 0.8272 - auROC: 0.9709 - val_loss: 0.5266 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 123/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4284 - acc: 0.8249 - auROC: 0.9710 - val_loss: 0.5255 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 124/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4280 - acc: 0.8272 - auROC: 0.9711 - val_loss: 0.5260 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 125/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4277 - acc: 0.8295 - auROC: 0.9711 - val_loss: 0.5268 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 126/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4274 - acc: 0.8180 - auROC: 0.9712 - val_loss: 0.5256 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 127/331\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.4272 - acc: 0.8041 - auROC: 0.9713 - val_loss: 0.5244 - val_acc: 0.7200 - val_auROC: 0.8816\n",
      "Epoch 128/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4268 - acc: 0.8111 - auROC: 0.9714 - val_loss: 0.5275 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 129/331\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4265 - acc: 0.8249 - auROC: 0.9715 - val_loss: 0.5303 - val_acc: 0.7200 - val_auROC: 0.8784\n",
      "Epoch 130/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4262 - acc: 0.8272 - auROC: 0.9716 - val_loss: 0.5247 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 131/331\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.4259 - acc: 0.8065 - auROC: 0.9717 - val_loss: 0.5235 - val_acc: 0.7200 - val_auROC: 0.8816\n",
      "Epoch 132/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4255 - acc: 0.8041 - auROC: 0.9713 - val_loss: 0.5252 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 133/331\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4251 - acc: 0.8157 - auROC: 0.9715 - val_loss: 0.5282 - val_acc: 0.7200 - val_auROC: 0.8792\n",
      "Epoch 134/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4248 - acc: 0.8226 - auROC: 0.9713 - val_loss: 0.5284 - val_acc: 0.7200 - val_auROC: 0.8792\n",
      "Epoch 135/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4244 - acc: 0.8203 - auROC: 0.9713 - val_loss: 0.5259 - val_acc: 0.7200 - val_auROC: 0.8792\n",
      "Epoch 136/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4242 - acc: 0.8065 - auROC: 0.9712 - val_loss: 0.5248 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 137/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4238 - acc: 0.8157 - auROC: 0.9713 - val_loss: 0.5279 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 138/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4236 - acc: 0.8226 - auROC: 0.9714 - val_loss: 0.5267 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 139/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4233 - acc: 0.8180 - auROC: 0.9715 - val_loss: 0.5235 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 140/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4232 - acc: 0.8088 - auROC: 0.9715 - val_loss: 0.5249 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 141/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4228 - acc: 0.8272 - auROC: 0.9716 - val_loss: 0.5291 - val_acc: 0.7200 - val_auROC: 0.8808\n",
      "Epoch 142/331\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4227 - acc: 0.8295 - auROC: 0.9718 - val_loss: 0.5281 - val_acc: 0.7200 - val_auROC: 0.8824\n",
      "Epoch 143/331\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4224 - acc: 0.8226 - auROC: 0.9718 - val_loss: 0.5253 - val_acc: 0.7200 - val_auROC: 0.8824\n",
      "Epoch 144/331\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4221 - acc: 0.8249 - auROC: 0.9715 - val_loss: 0.5271 - val_acc: 0.7200 - val_auROC: 0.8800\n",
      "Epoch 145/331\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4218 - acc: 0.8295 - auROC: 0.9719 - val_loss: 0.5267 - val_acc: 0.7200 - val_auROC: 0.8832\n",
      "Epoch 146/331\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4228 - acc: 0.8255 - auROC: 0.9695Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.4212 - acc: 0.8295 - auROC: 0.9727 - val_loss: 0.5267 - val_acc: 0.7200 - val_auROC: 0.8824\n",
      "Epoch 00146: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "1     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "2     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "3     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "4     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "5     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "6     0.0    0.0    0.0 -0.174273  ...    0.0 -0.152733 -0.033740    0.0\n",
      "7     0.0    0.0    0.0 -0.174273  ...    0.0  0.124427 -0.031988    0.0\n",
      "8     0.0    0.0    0.0 -0.174273  ...    0.0 -0.121518 -0.074554    0.0\n",
      "9     0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "10    0.0    0.0    0.0 -0.174273  ...    0.0  0.075860 -0.074554    0.0\n",
      "11    0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "12    0.0    0.0    0.0 -0.173550  ...    0.0 -0.218494 -0.074554    0.0\n",
      "13    0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "14    0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "15    0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "16    0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "17    0.0    0.0    0.0 -0.174273  ...    0.0 -0.218494 -0.074554    0.0\n",
      "\n",
      "[18 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC   F-max\n",
      "t                                 ...                                          \n",
      "0.00   0  10   0   6  0.375  1.0  ...  1.0  1.0  0.375  0.5455   0.9889  0.9091\n",
      "0.01   0  10   0   6  0.375  1.0  ...  1.0  1.0  0.375  0.5455   0.9889  0.9091\n",
      "0.02   0  10   0   6  0.375  1.0  ...  1.0  1.0  0.375  0.5455   0.9889  0.9091\n",
      "0.03   0  10   0   6  0.375  1.0  ...  1.0  1.0  0.375  0.5455   0.9889  0.9091\n",
      "0.04   0  10   0   6  0.375  1.0  ...  1.0  1.0  0.375  0.5455   0.9889  0.9091\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...     ...\n",
      "0.97  10   0   6   0  0.625  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9091\n",
      "0.98  10   0   6   0  0.625  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9091\n",
      "0.99  10   0   6   0  0.625  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9091\n",
      "1.00  10   0   6   0  0.625  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9091\n",
      "1.01  10   0   6   0  0.625  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9091\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC   F-max\n",
      "t                                 ...                                          \n",
      "0.00   0   6   0  10  0.625  1.0  ...  1.0  1.0  0.625  0.7692   0.9889  0.9474\n",
      "0.01   0   6   0  10  0.625  1.0  ...  1.0  1.0  0.625  0.7692   0.9889  0.9474\n",
      "0.02   0   6   0  10  0.625  1.0  ...  1.0  1.0  0.625  0.7692   0.9889  0.9474\n",
      "0.03   0   6   0  10  0.625  1.0  ...  1.0  1.0  0.625  0.7692   0.9889  0.9474\n",
      "0.04   0   6   0  10  0.625  1.0  ...  1.0  1.0  0.625  0.7692   0.9889  0.9474\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...     ...\n",
      "0.97   6   0  10   0  0.375  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9474\n",
      "0.98   6   0  10   0  0.375  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9474\n",
      "0.99   6   0  10   0  0.375  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9474\n",
      "1.00   6   0  10   0  0.375  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9474\n",
      "1.01   6   0  10   0  0.375  0.0  ...  0.0  0.0  0.000     NaN   0.9889  0.9474\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  15   0   1  0.0625  1.0  ...  1.0  1.0  0.0625  0.1176   0.7143   0.25\n",
      "0.01   0  15   0   1  0.0625  1.0  ...  1.0  1.0  0.0625  0.1176   0.7143   0.25\n",
      "0.02   0  15   0   1  0.0625  1.0  ...  1.0  1.0  0.0625  0.1176   0.7143   0.25\n",
      "0.03   0  15   0   1  0.0625  1.0  ...  1.0  1.0  0.0625  0.1176   0.7143   0.25\n",
      "0.04   0  15   0   1  0.0625  1.0  ...  1.0  1.0  0.0625  0.1176   0.7143   0.25\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  15   0   1   0  0.9375  0.0  ...  0.0  0.0  0.0000     NaN   0.7143   0.25\n",
      "0.98  15   0   1   0  0.9375  0.0  ...  0.0  0.0  0.0000     NaN   0.7143   0.25\n",
      "0.99  15   0   1   0  0.9375  0.0  ...  0.0  0.0  0.0000     NaN   0.7143   0.25\n",
      "1.00  15   0   1   0  0.9375  0.0  ...  0.0  0.0  0.0000     NaN   0.7143   0.25\n",
      "1.01  15   0   1   0  0.9375  0.0  ...  0.0  0.0  0.0000     NaN   0.7143   0.25\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  13   0   3  0.1875  1.0  ...  1.0  1.0  0.1875  0.3158   0.4167    0.4\n",
      "0.01   0  13   0   3  0.1875  1.0  ...  1.0  1.0  0.1875  0.3158   0.4167    0.4\n",
      "0.02   0  13   0   3  0.1875  1.0  ...  1.0  1.0  0.1875  0.3158   0.4167    0.4\n",
      "0.03   0  13   0   3  0.1875  1.0  ...  1.0  1.0  0.1875  0.3158   0.4167    0.4\n",
      "0.04   0  13   0   3  0.1875  1.0  ...  1.0  1.0  0.1875  0.3158   0.4167    0.4\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  13   0   3   0  0.8125  0.0  ...  0.0  0.0  0.0000     NaN   0.4167    0.4\n",
      "0.98  13   0   3   0  0.8125  0.0  ...  0.0  0.0  0.0000     NaN   0.4167    0.4\n",
      "0.99  13   0   3   0  0.8125  0.0  ...  0.0  0.0  0.0000     NaN   0.4167    0.4\n",
      "1.00  13   0   3   0  0.8125  0.0  ...  0.0  0.0  0.0000     NaN   0.4167    0.4\n",
      "1.01  13   0   3   0  0.8125  0.0  ...  0.0  0.0  0.0000     NaN   0.4167    0.4\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  16   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  16   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  16   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  16   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  16   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  16   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  16   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  16   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  16   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  16   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC   F-max\n",
      "t                                 ...                                          \n",
      "0.00   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      0.0  0.2222\n",
      "0.01   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      0.0  0.2222\n",
      "0.02   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      0.0  0.2222\n",
      "0.03   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      0.0  0.2222\n",
      "0.04   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      0.0  0.2222\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...     ...\n",
      "0.97  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      0.0  0.2222\n",
      "0.98  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      0.0  0.2222\n",
      "0.99  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      0.0  0.2222\n",
      "1.00  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      0.0  0.2222\n",
      "1.01  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      0.0  0.2222\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP   Acc   Sn   Sp  TPR  FPR   Rc    Pr   F1  ROC-AUC   F-max\n",
      "t                                                                              \n",
      "0.00   0  12   0   4  0.25  1.0  0.0  1.0  1.0  1.0  0.25  0.4   0.8333  0.7273\n",
      "0.01   0  12   0   4  0.25  1.0  0.0  1.0  1.0  1.0  0.25  0.4   0.8333  0.7273\n",
      "0.02   0  12   0   4  0.25  1.0  0.0  1.0  1.0  1.0  0.25  0.4   0.8333  0.7273\n",
      "0.03   0  12   0   4  0.25  1.0  0.0  1.0  1.0  1.0  0.25  0.4   0.8333  0.7273\n",
      "0.04   0  12   0   4  0.25  1.0  0.0  1.0  1.0  1.0  0.25  0.4   0.8333  0.7273\n",
      "...   ..  ..  ..  ..   ...  ...  ...  ...  ...  ...   ...  ...      ...     ...\n",
      "0.97  12   0   4   0  0.75  0.0  1.0  0.0  0.0  0.0  0.00  NaN   0.8333  0.7273\n",
      "0.98  12   0   4   0  0.75  0.0  1.0  0.0  0.0  0.0  0.00  NaN   0.8333  0.7273\n",
      "0.99  12   0   4   0  0.75  0.0  1.0  0.0  0.0  0.0  0.00  NaN   0.8333  0.7273\n",
      "1.00  12   0   4   0  0.75  0.0  1.0  0.0  0.0  0.0  0.00  NaN   0.8333  0.7273\n",
      "1.01  12   0   4   0  0.75  0.0  1.0  0.0  0.0  0.0  0.00  NaN   0.8333  0.7273\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC  F-max\n",
      "t                                 ...                                         \n",
      "0.00   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      1.0    1.0\n",
      "0.01   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      1.0    1.0\n",
      "0.02   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      1.0    1.0\n",
      "0.03   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      1.0    1.0\n",
      "0.04   0  14   0   2  0.125  1.0  ...  1.0  1.0  0.125  0.2222      1.0    1.0\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...    ...\n",
      "0.97  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "0.98  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "0.99  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "1.00  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "1.01  14   0   2   0  0.875  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 240\n",
      "Total correct samples: 240?240\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006084  0.034770\n",
      "4      0.003650  0.020862\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000056\n",
      "18016  0.000003  0.000045\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.8368 - acc: 0.5046 - auROC: 0.5763 - val_loss: 0.7132 - val_acc: 0.5625 - val_auROC: 0.5729\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.7276 - acc: 0.5255 - auROC: 0.5343 - val_loss: 0.6631 - val_acc: 0.6250 - val_auROC: 0.6042\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6872 - acc: 0.5579 - auROC: 0.5843 - val_loss: 0.6723 - val_acc: 0.6250 - val_auROC: 0.6901\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6468 - acc: 0.5880 - auROC: 0.7535 - val_loss: 0.6743 - val_acc: 0.6875 - val_auROC: 0.7300\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6283 - acc: 0.6157 - auROC: 0.7934 - val_loss: 0.6533 - val_acc: 0.7083 - val_auROC: 0.7682\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6138 - acc: 0.6782 - auROC: 0.8221 - val_loss: 0.6125 - val_acc: 0.7708 - val_auROC: 0.8559\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6010 - acc: 0.7083 - auROC: 0.8329 - val_loss: 0.5953 - val_acc: 0.7708 - val_auROC: 0.9010\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5913 - acc: 0.7292 - auROC: 0.8530 - val_loss: 0.5908 - val_acc: 0.7917 - val_auROC: 0.9089\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5850 - acc: 0.7454 - auROC: 0.8595 - val_loss: 0.5834 - val_acc: 0.7917 - val_auROC: 0.9054\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5760 - acc: 0.7593 - auROC: 0.8682 - val_loss: 0.5722 - val_acc: 0.8333 - val_auROC: 0.8854\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5662 - acc: 0.7778 - auROC: 0.8776 - val_loss: 0.5680 - val_acc: 0.8125 - val_auROC: 0.8845\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5546 - acc: 0.7917 - auROC: 0.8913 - val_loss: 0.5705 - val_acc: 0.7708 - val_auROC: 0.8767\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5446 - acc: 0.7801 - auROC: 0.8930 - val_loss: 0.5677 - val_acc: 0.7500 - val_auROC: 0.8646\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5394 - acc: 0.7755 - auROC: 0.8946 - val_loss: 0.5645 - val_acc: 0.7917 - val_auROC: 0.8559\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5268 - acc: 0.7708 - auROC: 0.9020 - val_loss: 0.5572 - val_acc: 0.7917 - val_auROC: 0.8681\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5171 - acc: 0.7847 - auROC: 0.9055 - val_loss: 0.5614 - val_acc: 0.7500 - val_auROC: 0.8628\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5079 - acc: 0.7593 - auROC: 0.9148 - val_loss: 0.5586 - val_acc: 0.7500 - val_auROC: 0.8594\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4975 - acc: 0.7778 - auROC: 0.9225 - val_loss: 0.5457 - val_acc: 0.7500 - val_auROC: 0.8698\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4926 - acc: 0.7731 - auROC: 0.9179 - val_loss: 0.5393 - val_acc: 0.7500 - val_auROC: 0.8672\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4826 - acc: 0.7616 - auROC: 0.9223 - val_loss: 0.5390 - val_acc: 0.7083 - val_auROC: 0.8628\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4785 - acc: 0.7639 - auROC: 0.9235 - val_loss: 0.5184 - val_acc: 0.7292 - val_auROC: 0.8733\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4669 - acc: 0.7801 - auROC: 0.9283 - val_loss: 0.5147 - val_acc: 0.7500 - val_auROC: 0.8863\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4588 - acc: 0.7940 - auROC: 0.9357 - val_loss: 0.5250 - val_acc: 0.7500 - val_auROC: 0.8585\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4541 - acc: 0.8403 - auROC: 0.9366 - val_loss: 0.5233 - val_acc: 0.8542 - val_auROC: 0.8611\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4449 - acc: 0.8912 - auROC: 0.9373 - val_loss: 0.4978 - val_acc: 0.8750 - val_auROC: 0.8889\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4468 - acc: 0.8843 - auROC: 0.9332 - val_loss: 0.5002 - val_acc: 0.8542 - val_auROC: 0.8828\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4312 - acc: 0.9074 - auROC: 0.9437 - val_loss: 0.5219 - val_acc: 0.8333 - val_auROC: 0.8455\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4253 - acc: 0.9097 - auROC: 0.9420 - val_loss: 0.4916 - val_acc: 0.8542 - val_auROC: 0.8811\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4224 - acc: 0.8935 - auROC: 0.9419 - val_loss: 0.4864 - val_acc: 0.8542 - val_auROC: 0.8819\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4056 - acc: 0.9167 - auROC: 0.9503 - val_loss: 0.5060 - val_acc: 0.8333 - val_auROC: 0.8498\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3997 - acc: 0.9144 - auROC: 0.9531 - val_loss: 0.4880 - val_acc: 0.8542 - val_auROC: 0.8733\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3923 - acc: 0.9167 - auROC: 0.9541 - val_loss: 0.4773 - val_acc: 0.8542 - val_auROC: 0.8793\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3797 - acc: 0.9236 - auROC: 0.9577 - val_loss: 0.4916 - val_acc: 0.8333 - val_auROC: 0.8516\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3755 - acc: 0.9259 - auROC: 0.9590 - val_loss: 0.4690 - val_acc: 0.8542 - val_auROC: 0.8776\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3679 - acc: 0.9259 - auROC: 0.9621 - val_loss: 0.4519 - val_acc: 0.8750 - val_auROC: 0.8924\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3608 - acc: 0.9259 - auROC: 0.9640 - val_loss: 0.4616 - val_acc: 0.8333 - val_auROC: 0.8741\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3571 - acc: 0.9352 - auROC: 0.9602 - val_loss: 0.4368 - val_acc: 0.8750 - val_auROC: 0.8993\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3489 - acc: 0.9329 - auROC: 0.9641 - val_loss: 0.4308 - val_acc: 0.8750 - val_auROC: 0.9028\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3401 - acc: 0.9375 - auROC: 0.9667 - val_loss: 0.4348 - val_acc: 0.8542 - val_auROC: 0.9045\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3368 - acc: 0.9444 - auROC: 0.9669 - val_loss: 0.4273 - val_acc: 0.8750 - val_auROC: 0.9097\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3313 - acc: 0.9444 - auROC: 0.9664 - val_loss: 0.4182 - val_acc: 0.8750 - val_auROC: 0.9167\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3242 - acc: 0.9421 - auROC: 0.9704 - val_loss: 0.4088 - val_acc: 0.8958 - val_auROC: 0.9184\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3189 - acc: 0.9421 - auROC: 0.9715 - val_loss: 0.4104 - val_acc: 0.8958 - val_auROC: 0.9149\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3146 - acc: 0.9444 - auROC: 0.9720 - val_loss: 0.4036 - val_acc: 0.8958 - val_auROC: 0.9167\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3089 - acc: 0.9444 - auROC: 0.9725 - val_loss: 0.3912 - val_acc: 0.9167 - val_auROC: 0.9175\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3039 - acc: 0.9491 - auROC: 0.9731 - val_loss: 0.3814 - val_acc: 0.9167 - val_auROC: 0.9201\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2989 - acc: 0.9491 - auROC: 0.9735 - val_loss: 0.3786 - val_acc: 0.9167 - val_auROC: 0.9201\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2942 - acc: 0.9514 - auROC: 0.9744 - val_loss: 0.3703 - val_acc: 0.9167 - val_auROC: 0.9184\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2909 - acc: 0.9514 - auROC: 0.9742 - val_loss: 0.3685 - val_acc: 0.9167 - val_auROC: 0.9210\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2868 - acc: 0.9514 - auROC: 0.9739 - val_loss: 0.3635 - val_acc: 0.9167 - val_auROC: 0.9245\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2834 - acc: 0.9514 - auROC: 0.9748 - val_loss: 0.3573 - val_acc: 0.9167 - val_auROC: 0.9253\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2803 - acc: 0.9491 - auROC: 0.9755 - val_loss: 0.3545 - val_acc: 0.9167 - val_auROC: 0.9340\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2761 - acc: 0.9537 - auROC: 0.9771 - val_loss: 0.3517 - val_acc: 0.9167 - val_auROC: 0.9349\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2747 - acc: 0.9514 - auROC: 0.9781 - val_loss: 0.3478 - val_acc: 0.9167 - val_auROC: 0.9358\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2693 - acc: 0.9537 - auROC: 0.9787 - val_loss: 0.3426 - val_acc: 0.9167 - val_auROC: 0.9384\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2626 - acc: 0.9583 - auROC: 0.9804 - val_loss: 0.3432 - val_acc: 0.8958 - val_auROC: 0.9401\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2568 - acc: 0.9630 - auROC: 0.9826 - val_loss: 0.3432 - val_acc: 0.8958 - val_auROC: 0.9392\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2534 - acc: 0.9630 - auROC: 0.9845 - val_loss: 0.3422 - val_acc: 0.8958 - val_auROC: 0.9366\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2506 - acc: 0.9630 - auROC: 0.9851 - val_loss: 0.3390 - val_acc: 0.8958 - val_auROC: 0.9392\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2474 - acc: 0.9630 - auROC: 0.9840 - val_loss: 0.3345 - val_acc: 0.8958 - val_auROC: 0.9410\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2442 - acc: 0.9630 - auROC: 0.9855 - val_loss: 0.3338 - val_acc: 0.8958 - val_auROC: 0.9366\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2417 - acc: 0.9630 - auROC: 0.9871 - val_loss: 0.3334 - val_acc: 0.8958 - val_auROC: 0.9392\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2390 - acc: 0.9630 - auROC: 0.9870 - val_loss: 0.3298 - val_acc: 0.8958 - val_auROC: 0.9401\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2365 - acc: 0.9630 - auROC: 0.9867 - val_loss: 0.3282 - val_acc: 0.8958 - val_auROC: 0.9375\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2341 - acc: 0.9630 - auROC: 0.9871 - val_loss: 0.3272 - val_acc: 0.8958 - val_auROC: 0.9384\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2315 - acc: 0.9630 - auROC: 0.9875 - val_loss: 0.3252 - val_acc: 0.9167 - val_auROC: 0.9392\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2293 - acc: 0.9630 - auROC: 0.9873 - val_loss: 0.3236 - val_acc: 0.8958 - val_auROC: 0.9375\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2265 - acc: 0.9630 - auROC: 0.9876 - val_loss: 0.3224 - val_acc: 0.8958 - val_auROC: 0.9384\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2242 - acc: 0.9630 - auROC: 0.9882 - val_loss: 0.3214 - val_acc: 0.9167 - val_auROC: 0.9401\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2224 - acc: 0.9630 - auROC: 0.9877 - val_loss: 0.3201 - val_acc: 0.8958 - val_auROC: 0.9418\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2198 - acc: 0.9630 - auROC: 0.9888 - val_loss: 0.3189 - val_acc: 0.8958 - val_auROC: 0.9392\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2179 - acc: 0.9630 - auROC: 0.9884 - val_loss: 0.3169 - val_acc: 0.8958 - val_auROC: 0.9427\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2152 - acc: 0.9630 - auROC: 0.9889 - val_loss: 0.3151 - val_acc: 0.8958 - val_auROC: 0.9401\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2132 - acc: 0.9630 - auROC: 0.9896 - val_loss: 0.3130 - val_acc: 0.8958 - val_auROC: 0.9410\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2108 - acc: 0.9630 - auROC: 0.9898 - val_loss: 0.3107 - val_acc: 0.8958 - val_auROC: 0.9392\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2084 - acc: 0.9630 - auROC: 0.9902 - val_loss: 0.3091 - val_acc: 0.9167 - val_auROC: 0.9401\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2063 - acc: 0.9653 - auROC: 0.9905 - val_loss: 0.3079 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2041 - acc: 0.9630 - auROC: 0.9897 - val_loss: 0.3058 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2020 - acc: 0.9630 - auROC: 0.9901 - val_loss: 0.3055 - val_acc: 0.9167 - val_auROC: 0.9436\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2002 - acc: 0.9630 - auROC: 0.9911 - val_loss: 0.3046 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1981 - acc: 0.9653 - auROC: 0.9913 - val_loss: 0.3030 - val_acc: 0.9167 - val_auROC: 0.9427\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1960 - acc: 0.9653 - auROC: 0.9911 - val_loss: 0.3025 - val_acc: 0.9167 - val_auROC: 0.9392\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.1941 - acc: 0.9653 - auROC: 0.9908 - val_loss: 0.3018 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 84/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1923 - acc: 0.9653 - auROC: 0.9917 - val_loss: 0.3006 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 85/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1901 - acc: 0.9653 - auROC: 0.9922 - val_loss: 0.3007 - val_acc: 0.9167 - val_auROC: 0.9384\n",
      "Epoch 86/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1883 - acc: 0.9653 - auROC: 0.9917 - val_loss: 0.3005 - val_acc: 0.9167 - val_auROC: 0.9401\n",
      "Epoch 87/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1863 - acc: 0.9653 - auROC: 0.9934 - val_loss: 0.2998 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 88/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1843 - acc: 0.9653 - auROC: 0.9920 - val_loss: 0.2990 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 89/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1824 - acc: 0.9676 - auROC: 0.9925 - val_loss: 0.2970 - val_acc: 0.9167 - val_auROC: 0.9392\n",
      "Epoch 90/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1789 - acc: 0.9676 - auROC: 0.9934 - val_loss: 0.2962 - val_acc: 0.9167 - val_auROC: 0.9366\n",
      "Epoch 91/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1774 - acc: 0.9676 - auROC: 0.9942 - val_loss: 0.2897 - val_acc: 0.9167 - val_auROC: 0.9462\n",
      "Epoch 92/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.1781 - acc: 0.9676 - auROC: 0.9911 - val_loss: 0.2859 - val_acc: 0.9167 - val_auROC: 0.9392\n",
      "Epoch 93/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1800 - acc: 0.9630 - auROC: 0.9929 - val_loss: 0.2856 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 94/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1744 - acc: 0.9676 - auROC: 0.9938 - val_loss: 0.2897 - val_acc: 0.9167 - val_auROC: 0.9436\n",
      "Epoch 95/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1855 - acc: 0.9630 - auROC: 0.9895 - val_loss: 0.2892 - val_acc: 0.8958 - val_auROC: 0.9384\n",
      "Epoch 96/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1930 - acc: 0.9583 - auROC: 0.9896 - val_loss: 0.2967 - val_acc: 0.8958 - val_auROC: 0.9340\n",
      "Epoch 97/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1916 - acc: 0.9630 - auROC: 0.9889 - val_loss: 0.3147 - val_acc: 0.8958 - val_auROC: 0.9332\n",
      "Epoch 98/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1950 - acc: 0.9688 - auROC: 0.9927\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1910 - acc: 0.9630 - auROC: 0.9914 - val_loss: 0.3112 - val_acc: 0.8542 - val_auROC: 0.9332\n",
      "Epoch 99/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1735 - acc: 0.9630 - auROC: 0.9936 - val_loss: 0.3168 - val_acc: 0.8542 - val_auROC: 0.9297\n",
      "Epoch 100/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1718 - acc: 0.9630 - auROC: 0.9939 - val_loss: 0.3130 - val_acc: 0.8542 - val_auROC: 0.9306\n",
      "Epoch 101/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1700 - acc: 0.9630 - auROC: 0.9941 - val_loss: 0.3052 - val_acc: 0.8958 - val_auROC: 0.9375\n",
      "Epoch 102/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1683 - acc: 0.9630 - auROC: 0.9945 - val_loss: 0.2968 - val_acc: 0.8958 - val_auROC: 0.9410\n",
      "Epoch 103/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1616 - acc: 0.9688 - auROC: 0.9984\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1664 - acc: 0.9653 - auROC: 0.9944 - val_loss: 0.2925 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 104/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1653 - acc: 0.9676 - auROC: 0.9945 - val_loss: 0.2922 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 105/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1651 - acc: 0.9676 - auROC: 0.9945 - val_loss: 0.2919 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 106/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1651 - acc: 0.9676 - auROC: 0.9946 - val_loss: 0.2917 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 107/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1649 - acc: 0.9676 - auROC: 0.9946 - val_loss: 0.2914 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 108/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1715 - acc: 0.9531 - auROC: 0.9951\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1647 - acc: 0.9676 - auROC: 0.9946 - val_loss: 0.2912 - val_acc: 0.9167 - val_auROC: 0.9418\n",
      "Epoch 00108: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 108/407\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.1729 - acc: 0.9676 - auROC: 0.9933 - val_loss: 0.2875 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 109/407\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1692 - acc: 0.9676 - auROC: 0.9939 - val_loss: 0.2884 - val_acc: 0.9167 - val_auROC: 0.9410\n",
      "Epoch 110/407\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.1665 - acc: 0.9699 - auROC: 0.9940 - val_loss: 0.2908 - val_acc: 0.9167 - val_auROC: 0.9392\n",
      "Epoch 111/407\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1636 - acc: 0.9745 - auROC: 0.9943 - val_loss: 0.3006 - val_acc: 0.9167 - val_auROC: 0.9392\n",
      "Epoch 112/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1625 - acc: 0.9745 - auROC: 0.9945 - val_loss: 0.3241 - val_acc: 0.8750 - val_auROC: 0.9358\n",
      "Epoch 113/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1620 - acc: 0.9745 - auROC: 0.9946 - val_loss: 0.3377 - val_acc: 0.8750 - val_auROC: 0.9280\n",
      "Epoch 114/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1616 - acc: 0.9745 - auROC: 0.9947 - val_loss: 0.3402 - val_acc: 0.8750 - val_auROC: 0.9280\n",
      "Epoch 115/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1609 - acc: 0.9769 - auROC: 0.9948 - val_loss: 0.3401 - val_acc: 0.8750 - val_auROC: 0.9175\n",
      "Epoch 116/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1605 - acc: 0.9769 - auROC: 0.9948 - val_loss: 0.3394 - val_acc: 0.8750 - val_auROC: 0.9184\n",
      "Epoch 117/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1600 - acc: 0.9769 - auROC: 0.9949 - val_loss: 0.3387 - val_acc: 0.8750 - val_auROC: 0.9184\n",
      "Epoch 118/407\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.1597 - acc: 0.9769 - auROC: 0.9949 - val_loss: 0.3368 - val_acc: 0.8750 - val_auROC: 0.9175\n",
      "Epoch 119/407\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.1593 - acc: 0.9769 - auROC: 0.9950 - val_loss: 0.3364 - val_acc: 0.8750 - val_auROC: 0.9175\n",
      "Epoch 120/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1588 - acc: 0.9769 - auROC: 0.9951 - val_loss: 0.3367 - val_acc: 0.8750 - val_auROC: 0.9123\n",
      "Epoch 121/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1562 - acc: 0.9838 - auROC: 0.9953 - val_loss: 0.3393 - val_acc: 0.8750 - val_auROC: 0.9132\n",
      "Epoch 122/407\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.1547 - acc: 0.9838 - auROC: 0.9954 - val_loss: 0.3432 - val_acc: 0.8750 - val_auROC: 0.9123\n",
      "Epoch 123/407\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.1570 - acc: 0.9818 - auROC: 0.9952Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.1539 - acc: 0.9838 - auROC: 0.9959 - val_loss: 0.3456 - val_acc: 0.8750 - val_auROC: 0.9123\n",
      "Epoch 00123: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "1     0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "2     0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "3     0.0    0.0    0.0 -0.174985  ...    0.0 -0.186311 -0.032912    0.0\n",
      "4     0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "5     0.0    0.0    0.0 -0.174985  ...    0.0 -0.195973 -0.044903    0.0\n",
      "6     0.0    0.0    0.0 -0.173743  ...    0.0 -0.220155 -0.074912    0.0\n",
      "7     0.0    0.0    0.0 -0.174669  ...    0.0 -0.220155 -0.074912    0.0\n",
      "8     0.0    0.0    0.0 -0.173338  ...    0.0 -0.174396 -0.074912    0.0\n",
      "9     0.0    0.0    0.0 -0.173736  ...    0.0  0.196351 -0.074912    0.0\n",
      "10    0.0    0.0    0.0 -0.174985  ...    0.0 -0.187976 -0.074912    0.0\n",
      "11    0.0    0.0    0.0 -0.172712  ...    0.0 -0.220155 -0.074912    0.0\n",
      "12    0.0    0.0    0.0 -0.174985  ...    0.0 -0.201839 -0.074912    0.0\n",
      "13    0.0    0.0    0.0 -0.173821  ...    0.0 -0.201969 -0.074912    0.0\n",
      "14    0.0    0.0    0.0 -0.174150  ...    0.0 -0.178387 -0.074912    0.0\n",
      "15    0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "16    0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "17    0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "18    0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "19    0.0    0.0    0.0 -0.174985  ...    0.0 -0.220155 -0.074912    0.0\n",
      "\n",
      "[20 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC   F-max\n",
      "t                                  ...                                        \n",
      "0.00   0  12   0   6  0.3333  1.0  ...  1.0  1.0  0.3333  0.5   0.9636  0.9091\n",
      "0.01   0  12   0   6  0.3333  1.0  ...  1.0  1.0  0.3333  0.5   0.9636  0.9091\n",
      "0.02   0  12   0   6  0.3333  1.0  ...  1.0  1.0  0.3333  0.5   0.9636  0.9091\n",
      "0.03   0  12   0   6  0.3333  1.0  ...  1.0  1.0  0.3333  0.5   0.9636  0.9091\n",
      "0.04   0  12   0   6  0.3333  1.0  ...  1.0  1.0  0.3333  0.5   0.9636  0.9091\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...     ...\n",
      "0.97  12   0   6   0  0.6667  0.0  ...  0.0  0.0  0.0000  NaN   0.9636  0.9091\n",
      "0.98  12   0   6   0  0.6667  0.0  ...  0.0  0.0  0.0000  NaN   0.9636  0.9091\n",
      "0.99  12   0   6   0  0.6667  0.0  ...  0.0  0.0  0.0000  NaN   0.9636  0.9091\n",
      "1.00  12   0   6   0  0.6667  0.0  ...  0.0  0.0  0.0000  NaN   0.9636  0.9091\n",
      "1.01  12   0   6   0  0.6667  0.0  ...  0.0  0.0  0.0000  NaN   0.9636  0.9091\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC   F-max\n",
      "t                                  ...                                        \n",
      "0.00   0   6   0  12  0.6667  1.0  ...  1.0  1.0  0.6667  0.8   0.8909  0.9167\n",
      "0.01   0   6   0  12  0.6667  1.0  ...  1.0  1.0  0.6667  0.8   0.8909  0.9167\n",
      "0.02   0   6   0  12  0.6667  1.0  ...  1.0  1.0  0.6667  0.8   0.8909  0.9167\n",
      "0.03   0   6   0  12  0.6667  1.0  ...  1.0  1.0  0.6667  0.8   0.8909  0.9167\n",
      "0.04   0   6   0  12  0.6667  1.0  ...  1.0  1.0  0.6667  0.8   0.8909  0.9167\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...     ...\n",
      "0.97   6   0  12   0  0.3333  0.0  ...  0.0  0.0  0.0000  NaN   0.8909  0.9167\n",
      "0.98   6   0  12   0  0.3333  0.0  ...  0.0  0.0  0.0000  NaN   0.8909  0.9167\n",
      "0.99   6   0  12   0  0.3333  0.0  ...  0.0  0.0  0.0000  NaN   0.8909  0.9167\n",
      "1.00   6   0  12   0  0.3333  0.0  ...  0.0  0.0  0.0000  NaN   0.8909  0.9167\n",
      "1.01   6   0  12   0  0.3333  0.0  ...  0.0  0.0  0.0000  NaN   0.8909  0.9167\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  17   0   1  0.0556  1.0  ...  1.0  1.0  0.0556  0.1053   0.4375  0.1818\n",
      "0.01   0  17   0   1  0.0556  1.0  ...  1.0  1.0  0.0556  0.1053   0.4375  0.1818\n",
      "0.02   0  17   0   1  0.0556  1.0  ...  1.0  1.0  0.0556  0.1053   0.4375  0.1818\n",
      "0.03   0  17   0   1  0.0556  1.0  ...  1.0  1.0  0.0556  0.1053   0.4375  0.1818\n",
      "0.04   0  17   0   1  0.0556  1.0  ...  1.0  1.0  0.0556  0.1053   0.4375  0.1818\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  17   0   1   0  0.9444  0.0  ...  0.0  0.0  0.0000     NaN   0.4375  0.1818\n",
      "0.98  17   0   1   0  0.9444  0.0  ...  0.0  0.0  0.0000     NaN   0.4375  0.1818\n",
      "0.99  17   0   1   0  0.9444  0.0  ...  0.0  0.0  0.0000     NaN   0.4375  0.1818\n",
      "1.00  17   0   1   0  0.9444  0.0  ...  0.0  0.0  0.0000     NaN   0.4375  0.1818\n",
      "1.01  17   0   1   0  0.9444  0.0  ...  0.0  0.0  0.0000     NaN   0.4375  0.1818\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.3393  0.3158\n",
      "0.01   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.3393  0.3158\n",
      "0.02   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.3393  0.3158\n",
      "0.03   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.3393  0.3158\n",
      "0.04   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.3393  0.3158\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.3393  0.3158\n",
      "0.98  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.3393  0.3158\n",
      "0.99  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.3393  0.3158\n",
      "1.00  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.3393  0.3158\n",
      "1.01  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.3393  0.3158\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  18   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  18   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  18   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  18   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  18   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  18   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  18   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  18   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  18   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  18   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.1429  0.2857\n",
      "0.01   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.1429  0.2857\n",
      "0.02   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.1429  0.2857\n",
      "0.03   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.1429  0.2857\n",
      "0.04   0  15   0   3  0.1667  1.0  ...  1.0  1.0  0.1667  0.2857   0.1429  0.2857\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.1429  0.2857\n",
      "0.98  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.1429  0.2857\n",
      "0.99  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.1429  0.2857\n",
      "1.00  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.1429  0.2857\n",
      "1.01  15   0   3   0  0.8333  0.0  ...  0.0  0.0  0.0000     NaN   0.1429  0.2857\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  13   0   5  0.2778  1.0  ...  1.0  1.0  0.2778  0.4348   0.7604  0.6667\n",
      "0.01   0  13   0   5  0.2778  1.0  ...  1.0  1.0  0.2778  0.4348   0.7604  0.6667\n",
      "0.02   0  13   0   5  0.2778  1.0  ...  1.0  1.0  0.2778  0.4348   0.7604  0.6667\n",
      "0.03   0  13   0   5  0.2778  1.0  ...  1.0  1.0  0.2778  0.4348   0.7604  0.6667\n",
      "0.04   0  13   0   5  0.2778  1.0  ...  1.0  1.0  0.2778  0.4348   0.7604  0.6667\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  13   0   5   0  0.7222  0.0  ...  0.0  0.0  0.0000     NaN   0.7604  0.6667\n",
      "0.98  13   0   5   0  0.7222  0.0  ...  0.0  0.0  0.0000     NaN   0.7604  0.6667\n",
      "0.99  13   0   5   0  0.7222  0.0  ...  0.0  0.0  0.0000     NaN   0.7604  0.6667\n",
      "1.00  13   0   5   0  0.7222  0.0  ...  0.0  0.0  0.0000     NaN   0.7604  0.6667\n",
      "1.01  13   0   5   0  0.7222  0.0  ...  0.0  0.0  0.0000     NaN   0.7604  0.6667\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC   F-max\n",
      "t                                  ...                                        \n",
      "0.00   0  16   0   2  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3333  0.2353\n",
      "0.01   0  16   0   2  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3333  0.2353\n",
      "0.02   0  16   0   2  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3333  0.2353\n",
      "0.03   0  16   0   2  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3333  0.2353\n",
      "0.04   0  16   0   2  0.1111  1.0  ...  1.0  1.0  0.1111  0.2   0.3333  0.2353\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...     ...\n",
      "0.97  16   0   2   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3333  0.2353\n",
      "0.98  16   0   2   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3333  0.2353\n",
      "0.99  16   0   2   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3333  0.2353\n",
      "1.00  16   0   2   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3333  0.2353\n",
      "1.01  16   0   2   0  0.8889  0.0  ...  0.0  0.0  0.0000  NaN   0.3333  0.2353\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 247\n",
      "Total correct samples: 247?247\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.005913  0.034288\n",
      "4      0.003548  0.020573\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000055\n",
      "18016  0.000003  0.000044\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 196ms/step - loss: 0.8112 - acc: 0.5158 - auROC: 0.6033 - val_loss: 0.7933 - val_acc: 0.5200 - val_auROC: 0.4376\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.7124 - acc: 0.5315 - auROC: 0.5455 - val_loss: 0.7502 - val_acc: 0.5000 - val_auROC: 0.3472\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.6734 - acc: 0.5901 - auROC: 0.6119 - val_loss: 0.7376 - val_acc: 0.4200 - val_auROC: 0.4448\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6496 - acc: 0.6351 - auROC: 0.7470 - val_loss: 0.6809 - val_acc: 0.4400 - val_auROC: 0.6160\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6256 - acc: 0.6509 - auROC: 0.8079 - val_loss: 0.6286 - val_acc: 0.5200 - val_auROC: 0.7872\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6215 - acc: 0.6802 - auROC: 0.8017 - val_loss: 0.6187 - val_acc: 0.5600 - val_auROC: 0.8280\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6078 - acc: 0.7140 - auROC: 0.8248 - val_loss: 0.6296 - val_acc: 0.5800 - val_auROC: 0.7568\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5934 - acc: 0.7410 - auROC: 0.8528 - val_loss: 0.6242 - val_acc: 0.6400 - val_auROC: 0.7968\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5859 - acc: 0.7658 - auROC: 0.8603 - val_loss: 0.6150 - val_acc: 0.6800 - val_auROC: 0.8136\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5802 - acc: 0.7815 - auROC: 0.8645 - val_loss: 0.6004 - val_acc: 0.6600 - val_auROC: 0.8408\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5753 - acc: 0.8063 - auROC: 0.8635 - val_loss: 0.5925 - val_acc: 0.6600 - val_auROC: 0.8568\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5700 - acc: 0.8153 - auROC: 0.8669 - val_loss: 0.5922 - val_acc: 0.6800 - val_auROC: 0.8552\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5645 - acc: 0.8198 - auROC: 0.8768 - val_loss: 0.5894 - val_acc: 0.6800 - val_auROC: 0.8616\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5594 - acc: 0.8221 - auROC: 0.8805 - val_loss: 0.5821 - val_acc: 0.7000 - val_auROC: 0.8624\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5535 - acc: 0.8266 - auROC: 0.8783 - val_loss: 0.5723 - val_acc: 0.7200 - val_auROC: 0.8696\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5475 - acc: 0.8288 - auROC: 0.8801 - val_loss: 0.5631 - val_acc: 0.7400 - val_auROC: 0.8736\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5404 - acc: 0.8221 - auROC: 0.8818 - val_loss: 0.5514 - val_acc: 0.8000 - val_auROC: 0.8848\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5329 - acc: 0.8176 - auROC: 0.8883 - val_loss: 0.5457 - val_acc: 0.7800 - val_auROC: 0.8896\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5237 - acc: 0.8311 - auROC: 0.8901 - val_loss: 0.5374 - val_acc: 0.8000 - val_auROC: 0.8920\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5168 - acc: 0.8243 - auROC: 0.8871 - val_loss: 0.5313 - val_acc: 0.8200 - val_auROC: 0.8952\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5083 - acc: 0.8243 - auROC: 0.8876 - val_loss: 0.5298 - val_acc: 0.8000 - val_auROC: 0.8848\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.5008 - acc: 0.8311 - auROC: 0.8926 - val_loss: 0.5261 - val_acc: 0.8000 - val_auROC: 0.8824\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4950 - acc: 0.8333 - auROC: 0.8935 - val_loss: 0.5328 - val_acc: 0.7600 - val_auROC: 0.8704\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4842 - acc: 0.8266 - auROC: 0.8980 - val_loss: 0.5101 - val_acc: 0.8200 - val_auROC: 0.8952\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4779 - acc: 0.8266 - auROC: 0.9072 - val_loss: 0.5094 - val_acc: 0.8000 - val_auROC: 0.8992\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4722 - acc: 0.8266 - auROC: 0.9115 - val_loss: 0.5082 - val_acc: 0.7800 - val_auROC: 0.8808\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4637 - acc: 0.8536 - auROC: 0.9157 - val_loss: 0.5001 - val_acc: 0.8000 - val_auROC: 0.8896\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4561 - acc: 0.8919 - auROC: 0.9207 - val_loss: 0.4849 - val_acc: 0.8400 - val_auROC: 0.9088\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4538 - acc: 0.8986 - auROC: 0.9218 - val_loss: 0.4815 - val_acc: 0.8800 - val_auROC: 0.9104\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4475 - acc: 0.8964 - auROC: 0.9255 - val_loss: 0.4832 - val_acc: 0.8400 - val_auROC: 0.9072\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4389 - acc: 0.9009 - auROC: 0.9305 - val_loss: 0.4890 - val_acc: 0.8400 - val_auROC: 0.8952\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4327 - acc: 0.9054 - auROC: 0.9323 - val_loss: 0.4789 - val_acc: 0.8400 - val_auROC: 0.9000\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4243 - acc: 0.9009 - auROC: 0.9356 - val_loss: 0.4712 - val_acc: 0.8600 - val_auROC: 0.9008\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4183 - acc: 0.9032 - auROC: 0.9368 - val_loss: 0.4699 - val_acc: 0.8400 - val_auROC: 0.9024\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4098 - acc: 0.9122 - auROC: 0.9406 - val_loss: 0.4590 - val_acc: 0.8600 - val_auROC: 0.9136\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4060 - acc: 0.9122 - auROC: 0.9408 - val_loss: 0.4626 - val_acc: 0.8400 - val_auROC: 0.9144\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3990 - acc: 0.9189 - auROC: 0.9433 - val_loss: 0.4550 - val_acc: 0.8400 - val_auROC: 0.9120\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3940 - acc: 0.9144 - auROC: 0.9438 - val_loss: 0.4561 - val_acc: 0.8400 - val_auROC: 0.9128\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3895 - acc: 0.9234 - auROC: 0.9432 - val_loss: 0.4731 - val_acc: 0.8400 - val_auROC: 0.8712\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3828 - acc: 0.9279 - auROC: 0.9451 - val_loss: 0.4587 - val_acc: 0.8200 - val_auROC: 0.8944\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3792 - acc: 0.9212 - auROC: 0.9460 - val_loss: 0.4460 - val_acc: 0.8200 - val_auROC: 0.9152\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3743 - acc: 0.9324 - auROC: 0.9451 - val_loss: 0.4543 - val_acc: 0.8400 - val_auROC: 0.8912\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3682 - acc: 0.9347 - auROC: 0.9479 - val_loss: 0.4557 - val_acc: 0.8200 - val_auROC: 0.8888\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3623 - acc: 0.9369 - auROC: 0.9470 - val_loss: 0.4474 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3577 - acc: 0.9324 - auROC: 0.9485 - val_loss: 0.4465 - val_acc: 0.8400 - val_auROC: 0.8944\n",
      "Epoch 46/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3502 - acc: 0.9375 - auROC: 0.9614\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3522 - acc: 0.9369 - auROC: 0.9483 - val_loss: 0.4460 - val_acc: 0.8200 - val_auROC: 0.9016\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3476 - acc: 0.9369 - auROC: 0.9491 - val_loss: 0.4443 - val_acc: 0.8200 - val_auROC: 0.9024\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3467 - acc: 0.9369 - auROC: 0.9496 - val_loss: 0.4423 - val_acc: 0.8200 - val_auROC: 0.9032\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3462 - acc: 0.9369 - auROC: 0.9497 - val_loss: 0.4423 - val_acc: 0.8200 - val_auROC: 0.9032\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3457 - acc: 0.9369 - auROC: 0.9500 - val_loss: 0.4404 - val_acc: 0.8200 - val_auROC: 0.8992\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3451 - acc: 0.9392 - auROC: 0.9506 - val_loss: 0.4397 - val_acc: 0.8200 - val_auROC: 0.8992\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3445 - acc: 0.9369 - auROC: 0.9503 - val_loss: 0.4396 - val_acc: 0.8200 - val_auROC: 0.8984\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3438 - acc: 0.9392 - auROC: 0.9492 - val_loss: 0.4375 - val_acc: 0.8200 - val_auROC: 0.8992\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3433 - acc: 0.9392 - auROC: 0.9495 - val_loss: 0.4371 - val_acc: 0.8200 - val_auROC: 0.8992\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3427 - acc: 0.9392 - auROC: 0.9500 - val_loss: 0.4376 - val_acc: 0.8200 - val_auROC: 0.8992\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3422 - acc: 0.9392 - auROC: 0.9502 - val_loss: 0.4377 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3417 - acc: 0.9392 - auROC: 0.9503 - val_loss: 0.4374 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3411 - acc: 0.9392 - auROC: 0.9506 - val_loss: 0.4380 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 59/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3479 - acc: 0.9297 - auROC: 0.9432\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3406 - acc: 0.9392 - auROC: 0.9510 - val_loss: 0.4386 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3403 - acc: 0.9392 - auROC: 0.9512 - val_loss: 0.4386 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3402 - acc: 0.9392 - auROC: 0.9513 - val_loss: 0.4386 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3402 - acc: 0.9392 - auROC: 0.9513 - val_loss: 0.4385 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3401 - acc: 0.9392 - auROC: 0.9513 - val_loss: 0.4383 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 64/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3588 - acc: 0.9219 - auROC: 0.9414\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3401 - acc: 0.9392 - auROC: 0.9514 - val_loss: 0.4381 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3400 - acc: 0.9392 - auROC: 0.9508 - val_loss: 0.4378 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3400 - acc: 0.9392 - auROC: 0.9508 - val_loss: 0.4377 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3399 - acc: 0.9392 - auROC: 0.9509 - val_loss: 0.4376 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3399 - acc: 0.9392 - auROC: 0.9509 - val_loss: 0.4376 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 69/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3208 - acc: 0.9531 - auROC: 0.9667Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3398 - acc: 0.9392 - auROC: 0.9509 - val_loss: 0.4375 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 00069: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 69/368\n",
      "4/4 [==============================] - 1s 190ms/step - loss: 0.3438 - acc: 0.9392 - auROC: 0.9497 - val_loss: 0.4344 - val_acc: 0.8200 - val_auROC: 0.9016\n",
      "Epoch 70/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3410 - acc: 0.9392 - auROC: 0.9511 - val_loss: 0.4351 - val_acc: 0.8200 - val_auROC: 0.9000\n",
      "Epoch 71/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3392 - acc: 0.9392 - auROC: 0.9509 - val_loss: 0.4346 - val_acc: 0.8200 - val_auROC: 0.8984\n",
      "Epoch 72/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3377 - acc: 0.9392 - auROC: 0.9520 - val_loss: 0.4330 - val_acc: 0.8400 - val_auROC: 0.9024\n",
      "Epoch 73/368\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3365 - acc: 0.9392 - auROC: 0.9522 - val_loss: 0.4301 - val_acc: 0.8400 - val_auROC: 0.9056\n",
      "Epoch 74/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3352 - acc: 0.9414 - auROC: 0.9524 - val_loss: 0.4264 - val_acc: 0.8400 - val_auROC: 0.9064\n",
      "Epoch 75/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3340 - acc: 0.9414 - auROC: 0.9523 - val_loss: 0.4227 - val_acc: 0.8400 - val_auROC: 0.9096\n",
      "Epoch 76/368\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3331 - acc: 0.9414 - auROC: 0.9523 - val_loss: 0.4200 - val_acc: 0.8600 - val_auROC: 0.9112\n",
      "Epoch 77/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3322 - acc: 0.9414 - auROC: 0.9527 - val_loss: 0.4175 - val_acc: 0.8600 - val_auROC: 0.9136\n",
      "Epoch 78/368\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3311 - acc: 0.9414 - auROC: 0.9535 - val_loss: 0.4157 - val_acc: 0.8800 - val_auROC: 0.9136\n",
      "Epoch 79/368\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3300 - acc: 0.9414 - auROC: 0.9539 - val_loss: 0.4133 - val_acc: 0.8800 - val_auROC: 0.9184\n",
      "Epoch 80/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3287 - acc: 0.9414 - auROC: 0.9546 - val_loss: 0.4090 - val_acc: 0.8600 - val_auROC: 0.9320\n",
      "Epoch 81/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3267 - acc: 0.9437 - auROC: 0.9562 - val_loss: 0.4050 - val_acc: 0.8800 - val_auROC: 0.9368\n",
      "Epoch 82/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3252 - acc: 0.9459 - auROC: 0.9564 - val_loss: 0.4019 - val_acc: 0.8800 - val_auROC: 0.9336\n",
      "Epoch 83/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3239 - acc: 0.9459 - auROC: 0.9569 - val_loss: 0.3993 - val_acc: 0.8800 - val_auROC: 0.9336\n",
      "Epoch 84/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3235 - acc: 0.9459 - auROC: 0.9572 - val_loss: 0.3967 - val_acc: 0.9000 - val_auROC: 0.9368\n",
      "Epoch 85/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3228 - acc: 0.9459 - auROC: 0.9575 - val_loss: 0.3949 - val_acc: 0.9200 - val_auROC: 0.9360\n",
      "Epoch 86/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3222 - acc: 0.9459 - auROC: 0.9580 - val_loss: 0.3938 - val_acc: 0.9200 - val_auROC: 0.9416\n",
      "Epoch 87/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3217 - acc: 0.9459 - auROC: 0.9580 - val_loss: 0.3934 - val_acc: 0.9200 - val_auROC: 0.9416\n",
      "Epoch 88/368\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.3214 - acc: 0.9459 - auROC: 0.9583 - val_loss: 0.3928 - val_acc: 0.9200 - val_auROC: 0.9400\n",
      "Epoch 89/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3210 - acc: 0.9459 - auROC: 0.9584 - val_loss: 0.3907 - val_acc: 0.9200 - val_auROC: 0.9360\n",
      "Epoch 90/368\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3206 - acc: 0.9459 - auROC: 0.9586 - val_loss: 0.3892 - val_acc: 0.9200 - val_auROC: 0.9352\n",
      "Epoch 91/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3200 - acc: 0.9459 - auROC: 0.9589 - val_loss: 0.3885 - val_acc: 0.9200 - val_auROC: 0.9408\n",
      "Epoch 92/368\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3193 - acc: 0.9459 - auROC: 0.9611 - val_loss: 0.3878 - val_acc: 0.9200 - val_auROC: 0.9416\n",
      "Epoch 93/368\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3188 - acc: 0.9459 - auROC: 0.9618 - val_loss: 0.3877 - val_acc: 0.9200 - val_auROC: 0.9360\n",
      "Epoch 94/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3172 - acc: 0.9505 - auROC: 0.9621 - val_loss: 0.3890 - val_acc: 0.9200 - val_auROC: 0.9432\n",
      "Epoch 95/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3164 - acc: 0.9505 - auROC: 0.9626 - val_loss: 0.3900 - val_acc: 0.9200 - val_auROC: 0.9416\n",
      "Epoch 96/368\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3154 - acc: 0.9505 - auROC: 0.9630 - val_loss: 0.3897 - val_acc: 0.9200 - val_auROC: 0.9424\n",
      "Epoch 97/368\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3147 - acc: 0.9505 - auROC: 0.9635 - val_loss: 0.3879 - val_acc: 0.9200 - val_auROC: 0.9424\n",
      "Epoch 98/368\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.3139 - acc: 0.9505 - auROC: 0.9648 - val_loss: 0.3876 - val_acc: 0.9200 - val_auROC: 0.9424\n",
      "Epoch 99/368\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3128 - acc: 0.9505 - auROC: 0.9658 - val_loss: 0.3884 - val_acc: 0.9200 - val_auROC: 0.9424\n",
      "Epoch 100/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3113 - acc: 0.9527 - auROC: 0.9665 - val_loss: 0.3892 - val_acc: 0.9200 - val_auROC: 0.9408\n",
      "Epoch 101/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3094 - acc: 0.9550 - auROC: 0.9686 - val_loss: 0.3908 - val_acc: 0.9200 - val_auROC: 0.9416\n",
      "Epoch 102/368\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3077 - acc: 0.9550 - auROC: 0.9707 - val_loss: 0.3932 - val_acc: 0.9200 - val_auROC: 0.9424\n",
      "Epoch 103/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3047 - acc: 0.9595 - auROC: 0.9708 - val_loss: 0.3920 - val_acc: 0.9200 - val_auROC: 0.9408\n",
      "Epoch 104/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3030 - acc: 0.9595 - auROC: 0.9718 - val_loss: 0.3895 - val_acc: 0.9200 - val_auROC: 0.9424\n",
      "Epoch 105/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3014 - acc: 0.9640 - auROC: 0.9732 - val_loss: 0.3876 - val_acc: 0.9000 - val_auROC: 0.9432\n",
      "Epoch 106/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3002 - acc: 0.9640 - auROC: 0.9733 - val_loss: 0.3862 - val_acc: 0.9000 - val_auROC: 0.9440\n",
      "Epoch 107/368\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.2984 - acc: 0.9640 - auROC: 0.9741 - val_loss: 0.3849 - val_acc: 0.9000 - val_auROC: 0.9440\n",
      "Epoch 108/368\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2962 - acc: 0.9662 - auROC: 0.9757 - val_loss: 0.3841 - val_acc: 0.8800 - val_auROC: 0.9456\n",
      "Epoch 109/368\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2942 - acc: 0.9707 - auROC: 0.9759 - val_loss: 0.3820 - val_acc: 0.8800 - val_auROC: 0.9464\n",
      "Epoch 110/368\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2926 - acc: 0.9707 - auROC: 0.9762 - val_loss: 0.3820 - val_acc: 0.8800 - val_auROC: 0.9488\n",
      "Epoch 111/368\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2916 - acc: 0.9730 - auROC: 0.9762 - val_loss: 0.3824 - val_acc: 0.8800 - val_auROC: 0.9520\n",
      "Epoch 112/368\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2902 - acc: 0.9730 - auROC: 0.9765 - val_loss: 0.3825 - val_acc: 0.8800 - val_auROC: 0.9512\n",
      "Epoch 113/368\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2895 - acc: 0.9730 - auROC: 0.9768 - val_loss: 0.3830 - val_acc: 0.8800 - val_auROC: 0.9512\n",
      "Epoch 114/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2889 - acc: 0.9730 - auROC: 0.9769 - val_loss: 0.3838 - val_acc: 0.8800 - val_auROC: 0.9488\n",
      "Epoch 115/368\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2884 - acc: 0.9730 - auROC: 0.9776 - val_loss: 0.3846 - val_acc: 0.8800 - val_auROC: 0.9488\n",
      "Epoch 116/368\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2880 - acc: 0.9730 - auROC: 0.9778 - val_loss: 0.3854 - val_acc: 0.8800 - val_auROC: 0.9496\n",
      "Epoch 117/368\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2877 - acc: 0.9730 - auROC: 0.9780 - val_loss: 0.3860 - val_acc: 0.8800 - val_auROC: 0.9448\n",
      "Epoch 118/368\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2873 - acc: 0.9730 - auROC: 0.9781 - val_loss: 0.3870 - val_acc: 0.8800 - val_auROC: 0.9424\n",
      "Epoch 119/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2872 - acc: 0.9730 - auROC: 0.9781 - val_loss: 0.3874 - val_acc: 0.8800 - val_auROC: 0.9392\n",
      "Epoch 120/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2869 - acc: 0.9730 - auROC: 0.9781 - val_loss: 0.3870 - val_acc: 0.8800 - val_auROC: 0.9408\n",
      "Epoch 121/368\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2867 - acc: 0.9730 - auROC: 0.9782 - val_loss: 0.3866 - val_acc: 0.8800 - val_auROC: 0.9456\n",
      "Epoch 122/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2865 - acc: 0.9730 - auROC: 0.9783 - val_loss: 0.3861 - val_acc: 0.8800 - val_auROC: 0.9448\n",
      "Epoch 123/368\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2863 - acc: 0.9730 - auROC: 0.9783 - val_loss: 0.3865 - val_acc: 0.8800 - val_auROC: 0.9440\n",
      "Epoch 124/368\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2860 - acc: 0.9730 - auROC: 0.9784 - val_loss: 0.3861 - val_acc: 0.8800 - val_auROC: 0.9384\n",
      "Epoch 125/368\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2742 - acc: 0.9844 - auROC: 0.9865Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.2859 - acc: 0.9730 - auROC: 0.9784 - val_loss: 0.3855 - val_acc: 0.8800 - val_auROC: 0.9392\n",
      "Epoch 00125: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "1     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "2     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "3     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "4     0.0    0.0    0.0 -0.172447  ...    0.0 -0.088015 -0.074134    0.0\n",
      "5     0.0    0.0    0.0 -0.171890  ...    0.0 -0.172228 -0.074134    0.0\n",
      "6     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "7     0.0    0.0    0.0 -0.172447  ...    0.0 -0.198904 -0.074134    0.0\n",
      "8     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "9     0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "10    0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "11    0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "12    0.0    0.0    0.0 -0.172447  ...    0.0 -0.218668 -0.074134    0.0\n",
      "\n",
      "[13 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0   5   0   6  0.5455  1.0  ...  1.0  1.0  0.5455  0.7059      1.0    1.0\n",
      "0.01   0   5   0   6  0.5455  1.0  ...  1.0  1.0  0.5455  0.7059      1.0    1.0\n",
      "0.02   0   5   0   6  0.5455  1.0  ...  1.0  1.0  0.5455  0.7059      1.0    1.0\n",
      "0.03   0   5   0   6  0.5455  1.0  ...  1.0  1.0  0.5455  0.7059      1.0    1.0\n",
      "0.04   0   5   0   6  0.5455  1.0  ...  1.0  1.0  0.5455  0.7059      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97   5   0   6   0  0.4545  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98   5   0   6   0  0.4545  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99   5   0   6   0  0.4545  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00   5   0   6   0  0.4545  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01   5   0   6   0  0.4545  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr     F1  ROC-AUC  F-max\n",
      "t                                  ...                                         \n",
      "0.00   0   6   0   5  0.4545  1.0  ...  1.0  1.0  0.4545  0.625      1.0    1.0\n",
      "0.01   0   6   0   5  0.4545  1.0  ...  1.0  1.0  0.4545  0.625      1.0    1.0\n",
      "0.02   0   6   0   5  0.4545  1.0  ...  1.0  1.0  0.4545  0.625      1.0    1.0\n",
      "0.03   0   6   0   5  0.4545  1.0  ...  1.0  1.0  0.4545  0.625      1.0    1.0\n",
      "0.04   0   6   0   5  0.4545  1.0  ...  1.0  1.0  0.4545  0.625      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...    ...      ...    ...\n",
      "0.97   6   0   5   0  0.5455  0.0  ...  0.0  0.0  0.0000    NaN      1.0    1.0\n",
      "0.98   6   0   5   0  0.5455  0.0  ...  0.0  0.0  0.0000    NaN      1.0    1.0\n",
      "0.99   6   0   5   0  0.5455  0.0  ...  0.0  0.0  0.0000    NaN      1.0    1.0\n",
      "1.00   6   0   5   0  0.5455  0.0  ...  0.0  0.0  0.0000    NaN      1.0    1.0\n",
      "1.01   6   0   5   0  0.5455  0.0  ...  0.0  0.0  0.0000    NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  10   0   1  0.0909  1.0  ...  1.0  1.0  0.0909  0.1667   0.4444  0.2857\n",
      "0.01   0  10   0   1  0.0909  1.0  ...  1.0  1.0  0.0909  0.1667   0.4444  0.2857\n",
      "0.02   0  10   0   1  0.0909  1.0  ...  1.0  1.0  0.0909  0.1667   0.4444  0.2857\n",
      "0.03   0  10   0   1  0.0909  1.0  ...  1.0  1.0  0.0909  0.1667   0.4444  0.2857\n",
      "0.04   0  10   0   1  0.0909  1.0  ...  1.0  1.0  0.0909  0.1667   0.4444  0.2857\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  10   0   1   0  0.9091  0.0  ...  0.0  0.0  0.0000     NaN   0.4444  0.2857\n",
      "0.98  10   0   1   0  0.9091  0.0  ...  0.0  0.0  0.0000     NaN   0.4444  0.2857\n",
      "0.99  10   0   1   0  0.9091  0.0  ...  0.0  0.0  0.0000     NaN   0.4444  0.2857\n",
      "1.00  10   0   1   0  0.9091  0.0  ...  0.0  0.0  0.0000     NaN   0.4444  0.2857\n",
      "1.01  10   0   1   0  0.9091  0.0  ...  0.0  0.0  0.0000     NaN   0.4444  0.2857\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0   8   0   3  0.2727  1.0  ...  1.0  1.0  0.2727  0.4286   0.3214  0.4286\n",
      "0.01   0   8   0   3  0.2727  1.0  ...  1.0  1.0  0.2727  0.4286   0.3214  0.4286\n",
      "0.02   0   8   0   3  0.2727  1.0  ...  1.0  1.0  0.2727  0.4286   0.3214  0.4286\n",
      "0.03   0   8   0   3  0.2727  1.0  ...  1.0  1.0  0.2727  0.4286   0.3214  0.4286\n",
      "0.04   0   8   0   3  0.2727  1.0  ...  1.0  1.0  0.2727  0.4286   0.3214  0.4286\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97   8   0   3   0  0.7273  0.0  ...  0.0  0.0  0.0000     NaN   0.3214  0.4286\n",
      "0.98   8   0   3   0  0.7273  0.0  ...  0.0  0.0  0.0000     NaN   0.3214  0.4286\n",
      "0.99   8   0   3   0  0.7273  0.0  ...  0.0  0.0  0.0000     NaN   0.3214  0.4286\n",
      "1.00   8   0   3   0  0.7273  0.0  ...  0.0  0.0  0.0000     NaN   0.3214  0.4286\n",
      "1.01   8   0   3   0  0.7273  0.0  ...  0.0  0.0  0.0000     NaN   0.3214  0.4286\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0   7   0   4  0.3636  1.0  ...  1.0  1.0  0.3636  0.5333      1.0    1.0\n",
      "0.01   0   7   0   4  0.3636  1.0  ...  1.0  1.0  0.3636  0.5333      1.0    1.0\n",
      "0.02   0   7   0   4  0.3636  1.0  ...  1.0  1.0  0.3636  0.5333      1.0    1.0\n",
      "0.03   0   7   0   4  0.3636  1.0  ...  1.0  1.0  0.3636  0.5333      1.0    1.0\n",
      "0.04   0   7   0   4  0.3636  1.0  ...  1.0  1.0  0.3636  0.5333      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97   7   0   4   0  0.6364  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98   7   0   4   0  0.6364  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99   7   0   4   0  0.6364  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00   7   0   4   0  0.6364  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01   7   0   4   0  0.6364  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  11   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  11   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 241\n",
      "Total correct samples: 241?241\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006060  0.034699\n",
      "4      0.003636  0.020820\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000055\n",
      "18016  0.000003  0.000045\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.8358 - acc: 0.5046 - auROC: 0.6050 - val_loss: 0.7491 - val_acc: 0.5000 - val_auROC: 0.5936\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.7350 - acc: 0.5023 - auROC: 0.5618 - val_loss: 0.6935 - val_acc: 0.5200 - val_auROC: 0.4312\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6937 - acc: 0.5185 - auROC: 0.5232 - val_loss: 0.6670 - val_acc: 0.5800 - val_auROC: 0.5936\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6751 - acc: 0.5417 - auROC: 0.6082 - val_loss: 0.6536 - val_acc: 0.5600 - val_auROC: 0.7112\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6511 - acc: 0.5602 - auROC: 0.7392 - val_loss: 0.6555 - val_acc: 0.5600 - val_auROC: 0.7032\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6339 - acc: 0.5556 - auROC: 0.7958 - val_loss: 0.6481 - val_acc: 0.5600 - val_auROC: 0.7032\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6203 - acc: 0.5856 - auROC: 0.8152 - val_loss: 0.6306 - val_acc: 0.5600 - val_auROC: 0.7160\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6014 - acc: 0.6319 - auROC: 0.8227 - val_loss: 0.6204 - val_acc: 0.6400 - val_auROC: 0.7360\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5841 - acc: 0.7014 - auROC: 0.8431 - val_loss: 0.6182 - val_acc: 0.6800 - val_auROC: 0.7400\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5712 - acc: 0.7130 - auROC: 0.8530 - val_loss: 0.6131 - val_acc: 0.6600 - val_auROC: 0.7640\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5622 - acc: 0.7199 - auROC: 0.8605 - val_loss: 0.5934 - val_acc: 0.7000 - val_auROC: 0.8264\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5578 - acc: 0.7106 - auROC: 0.8671 - val_loss: 0.5851 - val_acc: 0.6800 - val_auROC: 0.8304\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5450 - acc: 0.7222 - auROC: 0.8757 - val_loss: 0.5943 - val_acc: 0.6800 - val_auROC: 0.7976\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5372 - acc: 0.7269 - auROC: 0.8803 - val_loss: 0.5872 - val_acc: 0.6800 - val_auROC: 0.8016\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5251 - acc: 0.7269 - auROC: 0.8893 - val_loss: 0.5736 - val_acc: 0.6600 - val_auROC: 0.8312\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5140 - acc: 0.7014 - auROC: 0.8917 - val_loss: 0.5705 - val_acc: 0.6400 - val_auROC: 0.8120\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5039 - acc: 0.7130 - auROC: 0.8957 - val_loss: 0.5650 - val_acc: 0.6600 - val_auROC: 0.8264\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4937 - acc: 0.7106 - auROC: 0.9043 - val_loss: 0.5512 - val_acc: 0.7000 - val_auROC: 0.8368\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4822 - acc: 0.7245 - auROC: 0.9090 - val_loss: 0.5573 - val_acc: 0.6800 - val_auROC: 0.8328\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4738 - acc: 0.7315 - auROC: 0.9114 - val_loss: 0.5336 - val_acc: 0.7000 - val_auROC: 0.8648\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4628 - acc: 0.7477 - auROC: 0.9219 - val_loss: 0.5249 - val_acc: 0.8000 - val_auROC: 0.8704\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4542 - acc: 0.8727 - auROC: 0.9237 - val_loss: 0.5327 - val_acc: 0.7800 - val_auROC: 0.8464\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4431 - acc: 0.8912 - auROC: 0.9317 - val_loss: 0.5244 - val_acc: 0.8000 - val_auROC: 0.8504\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4338 - acc: 0.9051 - auROC: 0.9354 - val_loss: 0.5160 - val_acc: 0.8000 - val_auROC: 0.8608\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4223 - acc: 0.9120 - auROC: 0.9446 - val_loss: 0.5047 - val_acc: 0.8000 - val_auROC: 0.8648\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4124 - acc: 0.9282 - auROC: 0.9456 - val_loss: 0.5131 - val_acc: 0.8200 - val_auROC: 0.8536\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4039 - acc: 0.9282 - auROC: 0.9466 - val_loss: 0.5009 - val_acc: 0.8000 - val_auROC: 0.8568\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3921 - acc: 0.9306 - auROC: 0.9526 - val_loss: 0.4856 - val_acc: 0.8200 - val_auROC: 0.8720\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3829 - acc: 0.9306 - auROC: 0.9570 - val_loss: 0.5015 - val_acc: 0.8200 - val_auROC: 0.8480\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3734 - acc: 0.9329 - auROC: 0.9582 - val_loss: 0.4856 - val_acc: 0.8200 - val_auROC: 0.8656\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3677 - acc: 0.9329 - auROC: 0.9601 - val_loss: 0.4937 - val_acc: 0.8400 - val_auROC: 0.8392\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3576 - acc: 0.9329 - auROC: 0.9632 - val_loss: 0.4867 - val_acc: 0.8400 - val_auROC: 0.8408\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3472 - acc: 0.9375 - auROC: 0.9653 - val_loss: 0.4711 - val_acc: 0.8200 - val_auROC: 0.8728\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3407 - acc: 0.9398 - auROC: 0.9657 - val_loss: 0.4923 - val_acc: 0.8200 - val_auROC: 0.8296\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3383 - acc: 0.9375 - auROC: 0.9664 - val_loss: 0.4786 - val_acc: 0.8400 - val_auROC: 0.8336\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3323 - acc: 0.9398 - auROC: 0.9693 - val_loss: 0.4790 - val_acc: 0.8400 - val_auROC: 0.8352\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3322 - acc: 0.9421 - auROC: 0.9681 - val_loss: 0.4703 - val_acc: 0.8400 - val_auROC: 0.8456\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3223 - acc: 0.9444 - auROC: 0.9712 - val_loss: 0.4883 - val_acc: 0.8400 - val_auROC: 0.8192\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3206 - acc: 0.9398 - auROC: 0.9718 - val_loss: 0.4750 - val_acc: 0.8400 - val_auROC: 0.8376\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3147 - acc: 0.9514 - auROC: 0.9709 - val_loss: 0.4725 - val_acc: 0.8400 - val_auROC: 0.8480\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3094 - acc: 0.9468 - auROC: 0.9752 - val_loss: 0.4839 - val_acc: 0.8400 - val_auROC: 0.8264\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3066 - acc: 0.9491 - auROC: 0.9763 - val_loss: 0.4663 - val_acc: 0.8400 - val_auROC: 0.8416\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3022 - acc: 0.9537 - auROC: 0.9766 - val_loss: 0.4667 - val_acc: 0.8400 - val_auROC: 0.8456\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2983 - acc: 0.9560 - auROC: 0.9772 - val_loss: 0.4672 - val_acc: 0.8400 - val_auROC: 0.8448\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2945 - acc: 0.9583 - auROC: 0.9764 - val_loss: 0.4676 - val_acc: 0.8400 - val_auROC: 0.8312\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2918 - acc: 0.9560 - auROC: 0.9770 - val_loss: 0.4669 - val_acc: 0.8400 - val_auROC: 0.8304\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2884 - acc: 0.9583 - auROC: 0.9764 - val_loss: 0.4654 - val_acc: 0.8400 - val_auROC: 0.8280\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2848 - acc: 0.9583 - auROC: 0.9765 - val_loss: 0.4655 - val_acc: 0.8400 - val_auROC: 0.8368\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2822 - acc: 0.9606 - auROC: 0.9767 - val_loss: 0.4630 - val_acc: 0.8400 - val_auROC: 0.8376\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2794 - acc: 0.9606 - auROC: 0.9782 - val_loss: 0.4666 - val_acc: 0.8400 - val_auROC: 0.8224\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2769 - acc: 0.9653 - auROC: 0.9791 - val_loss: 0.4668 - val_acc: 0.8400 - val_auROC: 0.8280\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2739 - acc: 0.9653 - auROC: 0.9791 - val_loss: 0.4607 - val_acc: 0.8400 - val_auROC: 0.8344\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2715 - acc: 0.9653 - auROC: 0.9787 - val_loss: 0.4614 - val_acc: 0.8400 - val_auROC: 0.8272\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2694 - acc: 0.9653 - auROC: 0.9794 - val_loss: 0.4609 - val_acc: 0.8400 - val_auROC: 0.8304\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2676 - acc: 0.9630 - auROC: 0.9789 - val_loss: 0.4606 - val_acc: 0.8400 - val_auROC: 0.8304\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2676 - acc: 0.9630 - auROC: 0.9787 - val_loss: 0.4582 - val_acc: 0.8400 - val_auROC: 0.8320\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2623 - acc: 0.9676 - auROC: 0.9787 - val_loss: 0.4858 - val_acc: 0.8200 - val_auROC: 0.8248\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2600 - acc: 0.9676 - auROC: 0.9806 - val_loss: 0.4657 - val_acc: 0.8400 - val_auROC: 0.8288\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2580 - acc: 0.9676 - auROC: 0.9809 - val_loss: 0.4744 - val_acc: 0.8400 - val_auROC: 0.8216\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2596 - acc: 0.9653 - auROC: 0.9797 - val_loss: 0.4718 - val_acc: 0.8400 - val_auROC: 0.8192\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2518 - acc: 0.9676 - auROC: 0.9810 - val_loss: 0.4574 - val_acc: 0.8400 - val_auROC: 0.8264\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2505 - acc: 0.9676 - auROC: 0.9814 - val_loss: 0.4658 - val_acc: 0.8400 - val_auROC: 0.8256\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2495 - acc: 0.9653 - auROC: 0.9821 - val_loss: 0.4626 - val_acc: 0.8400 - val_auROC: 0.8232\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2570 - acc: 0.9606 - auROC: 0.9809 - val_loss: 0.4565 - val_acc: 0.8400 - val_auROC: 0.8264\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2454 - acc: 0.9653 - auROC: 0.9817 - val_loss: 0.5423 - val_acc: 0.7600 - val_auROC: 0.7920\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2469 - acc: 0.9606 - auROC: 0.9812 - val_loss: 0.4435 - val_acc: 0.8400 - val_auROC: 0.8560\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2431 - acc: 0.9676 - auROC: 0.9820 - val_loss: 0.4573 - val_acc: 0.8400 - val_auROC: 0.8296\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2355 - acc: 0.9722 - auROC: 0.9807 - val_loss: 0.5590 - val_acc: 0.7600 - val_auROC: 0.7768\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2456 - acc: 0.9606 - auROC: 0.9799 - val_loss: 0.4396 - val_acc: 0.8400 - val_auROC: 0.8520\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2407 - acc: 0.9630 - auROC: 0.9806 - val_loss: 0.4485 - val_acc: 0.8400 - val_auROC: 0.8368\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2270 - acc: 0.9699 - auROC: 0.9825 - val_loss: 0.5189 - val_acc: 0.8000 - val_auROC: 0.8048\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2267 - acc: 0.9699 - auROC: 0.9823 - val_loss: 0.4534 - val_acc: 0.8400 - val_auROC: 0.8456\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2322 - acc: 0.9630 - auROC: 0.9728 - val_loss: 0.4975 - val_acc: 0.8200 - val_auROC: 0.8248\n",
      "Epoch 74/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2031 - acc: 0.9844 - auROC: 0.9973\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2624 - acc: 0.9444 - auROC: 0.9707 - val_loss: 0.4813 - val_acc: 0.8200 - val_auROC: 0.8376\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2512 - acc: 0.9398 - auROC: 0.9744 - val_loss: 0.4671 - val_acc: 0.8400 - val_auROC: 0.8328\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2448 - acc: 0.9468 - auROC: 0.9789 - val_loss: 0.4600 - val_acc: 0.8400 - val_auROC: 0.8320\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2397 - acc: 0.9537 - auROC: 0.9825 - val_loss: 0.4559 - val_acc: 0.8400 - val_auROC: 0.8368\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2357 - acc: 0.9583 - auROC: 0.9824 - val_loss: 0.4539 - val_acc: 0.8400 - val_auROC: 0.8440\n",
      "Epoch 79/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2541 - acc: 0.9531 - auROC: 0.9742\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2313 - acc: 0.9630 - auROC: 0.9823 - val_loss: 0.4539 - val_acc: 0.8400 - val_auROC: 0.8416\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2286 - acc: 0.9630 - auROC: 0.9825 - val_loss: 0.4539 - val_acc: 0.8400 - val_auROC: 0.8416\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2283 - acc: 0.9630 - auROC: 0.9832 - val_loss: 0.4540 - val_acc: 0.8400 - val_auROC: 0.8416\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2281 - acc: 0.9630 - auROC: 0.9832 - val_loss: 0.4540 - val_acc: 0.8400 - val_auROC: 0.8424\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2278 - acc: 0.9630 - auROC: 0.9833 - val_loss: 0.4541 - val_acc: 0.8400 - val_auROC: 0.8416\n",
      "Epoch 84/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2216 - acc: 0.9688 - auROC: 0.9888\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2276 - acc: 0.9630 - auROC: 0.9833 - val_loss: 0.4541 - val_acc: 0.8400 - val_auROC: 0.8416\n",
      "Epoch 00084: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 84/383\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 0.2358 - acc: 0.9653 - auROC: 0.9820 - val_loss: 0.4441 - val_acc: 0.8400 - val_auROC: 0.8448\n",
      "Epoch 85/383\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2286 - acc: 0.9699 - auROC: 0.9836 - val_loss: 0.4498 - val_acc: 0.8400 - val_auROC: 0.8360\n",
      "Epoch 86/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2272 - acc: 0.9722 - auROC: 0.9840 - val_loss: 0.4526 - val_acc: 0.8400 - val_auROC: 0.8368\n",
      "Epoch 87/383\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2264 - acc: 0.9722 - auROC: 0.9842 - val_loss: 0.4539 - val_acc: 0.8400 - val_auROC: 0.8296\n",
      "Epoch 88/383\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2255 - acc: 0.9722 - auROC: 0.9842 - val_loss: 0.4528 - val_acc: 0.8400 - val_auROC: 0.8288\n",
      "Epoch 89/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2246 - acc: 0.9722 - auROC: 0.9842 - val_loss: 0.4516 - val_acc: 0.8400 - val_auROC: 0.8360\n",
      "Epoch 90/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2241 - acc: 0.9722 - auROC: 0.9845 - val_loss: 0.4508 - val_acc: 0.8400 - val_auROC: 0.8384\n",
      "Epoch 91/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2237 - acc: 0.9722 - auROC: 0.9846 - val_loss: 0.4502 - val_acc: 0.8400 - val_auROC: 0.8400\n",
      "Epoch 92/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2233 - acc: 0.9722 - auROC: 0.9850 - val_loss: 0.4500 - val_acc: 0.8400 - val_auROC: 0.8320\n",
      "Epoch 93/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2228 - acc: 0.9722 - auROC: 0.9851 - val_loss: 0.4497 - val_acc: 0.8400 - val_auROC: 0.8272\n",
      "Epoch 94/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2225 - acc: 0.9722 - auROC: 0.9851 - val_loss: 0.4494 - val_acc: 0.8400 - val_auROC: 0.8384\n",
      "Epoch 95/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2222 - acc: 0.9722 - auROC: 0.9852 - val_loss: 0.4489 - val_acc: 0.8400 - val_auROC: 0.8392\n",
      "Epoch 96/383\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2218 - acc: 0.9722 - auROC: 0.9852 - val_loss: 0.4485 - val_acc: 0.8400 - val_auROC: 0.8392\n",
      "Epoch 97/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2214 - acc: 0.9722 - auROC: 0.9854 - val_loss: 0.4484 - val_acc: 0.8400 - val_auROC: 0.8392\n",
      "Epoch 98/383\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2210 - acc: 0.9722 - auROC: 0.9855 - val_loss: 0.4483 - val_acc: 0.8400 - val_auROC: 0.8384\n",
      "Epoch 99/383\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2201 - acc: 0.9740 - auROC: 0.9853Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2206 - acc: 0.9722 - auROC: 0.9865 - val_loss: 0.4478 - val_acc: 0.8400 - val_auROC: 0.8392\n",
      "Epoch 00099: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "1     0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "2     0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "3     0.0    0.0    0.0 -0.173618  ...    0.0 -0.184334 -0.073013    0.0\n",
      "4     0.0    0.0    0.0 -0.174639  ...    0.0 -0.109326  0.051544    0.0\n",
      "5     0.0    0.0    0.0 -0.174639  ...    0.0  1.103493 -0.073013    0.0\n",
      "6     0.0    0.0    0.0 -0.174639  ...    0.0  0.030632  0.139716    0.0\n",
      "7     0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "8     0.0    0.0    0.0 -0.173678  ...    0.0 -0.149665 -0.073013    0.0\n",
      "9     0.0    0.0    0.0 -0.174639  ...    0.0  0.155869 -0.073013    0.0\n",
      "10    0.0    0.0    0.0 -0.174639  ...    0.0 -0.071328  0.055685    0.0\n",
      "11    0.0    0.0    0.0 -0.174639  ...    0.0 -0.080042 -0.073013    0.0\n",
      "12    0.0    0.0    0.0 -0.174639  ...    0.0 -0.192804 -0.073013    0.0\n",
      "13    0.0    0.0    0.0 -0.174639  ...    0.0  0.501974 -0.046306    0.0\n",
      "14    0.0    0.0    0.0 -0.174639  ...    0.0 -0.199175 -0.073013    0.0\n",
      "15    0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "16    0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "17    0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "18    0.0    0.0    0.0 -0.174639  ...    0.0 -0.209939 -0.073013    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  11   0   6  0.3529  1.0  ...  1.0  1.0  0.3529  0.5217     0.72   0.75\n",
      "0.01   0  11   0   6  0.3529  1.0  ...  1.0  1.0  0.3529  0.5217     0.72   0.75\n",
      "0.02   0  11   0   6  0.3529  1.0  ...  1.0  1.0  0.3529  0.5217     0.72   0.75\n",
      "0.03   0  11   0   6  0.3529  1.0  ...  1.0  1.0  0.3529  0.5217     0.72   0.75\n",
      "0.04   0  11   0   6  0.3529  1.0  ...  1.0  1.0  0.3529  0.5217     0.72   0.75\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  11   0   6   0  0.6471  0.0  ...  0.0  0.0  0.0000     NaN     0.72   0.75\n",
      "0.98  11   0   6   0  0.6471  0.0  ...  0.0  0.0  0.0000     NaN     0.72   0.75\n",
      "0.99  11   0   6   0  0.6471  0.0  ...  0.0  0.0  0.0000     NaN     0.72   0.75\n",
      "1.00  11   0   6   0  0.6471  0.0  ...  0.0  0.0  0.0000     NaN     0.72   0.75\n",
      "1.01  11   0   6   0  0.6471  0.0  ...  0.0  0.0  0.0000     NaN     0.72   0.75\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0   6   0  11  0.6471  1.0  ...  1.0  1.0  0.6471  0.7857      0.9    0.9\n",
      "0.01   0   6   0  11  0.6471  1.0  ...  1.0  1.0  0.6471  0.7857      0.9    0.9\n",
      "0.02   0   6   0  11  0.6471  1.0  ...  1.0  1.0  0.6471  0.7857      0.9    0.9\n",
      "0.03   0   6   0  11  0.6471  1.0  ...  1.0  1.0  0.6471  0.7857      0.9    0.9\n",
      "0.04   0   6   0  11  0.6471  1.0  ...  1.0  1.0  0.6471  0.7857      0.9    0.9\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97   6   0  11   0  0.3529  0.0  ...  0.0  0.0  0.0000     NaN      0.9    0.9\n",
      "0.98   6   0  11   0  0.3529  0.0  ...  0.0  0.0  0.0000     NaN      0.9    0.9\n",
      "0.99   6   0  11   0  0.3529  0.0  ...  0.0  0.0  0.0000     NaN      0.9    0.9\n",
      "1.00   6   0  11   0  0.3529  0.0  ...  0.0  0.0  0.0000     NaN      0.9    0.9\n",
      "1.01   6   0  11   0  0.3529  0.0  ...  0.0  0.0  0.0000     NaN      0.9    0.9\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.125\n",
      "0.01   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.125\n",
      "0.02   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.125\n",
      "0.03   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.125\n",
      "0.04   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.125\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.125\n",
      "0.98  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.125\n",
      "0.99  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.125\n",
      "1.00  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.125\n",
      "1.01  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.125\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC  F-max\n",
      "t                                  ...                                       \n",
      "0.00   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      1.0    1.0\n",
      "0.01   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      1.0    1.0\n",
      "0.02   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      1.0    1.0\n",
      "0.03   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      1.0    1.0\n",
      "0.04   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...    ...\n",
      "0.97  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      1.0    1.0\n",
      "0.98  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      1.0    1.0\n",
      "0.99  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      1.0    1.0\n",
      "1.00  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      1.0    1.0\n",
      "1.01  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  17   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  17   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  17   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  17   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  17   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  17   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  17   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  17   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  17   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  17   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.7143  0.2857\n",
      "0.01   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.7143  0.2857\n",
      "0.02   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.7143  0.2857\n",
      "0.03   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.7143  0.2857\n",
      "0.04   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.7143  0.2857\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.7143  0.2857\n",
      "0.98  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.7143  0.2857\n",
      "0.99  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.7143  0.2857\n",
      "1.00  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.7143  0.2857\n",
      "1.01  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.7143  0.2857\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  12   0   5  0.2941  1.0  ...  1.0000  1.0  0.2941  0.4545   0.9091  0.7692\n",
      "0.01   0  12   0   5  0.2941  1.0  ...  1.0000  1.0  0.2941  0.4545   0.9091  0.7692\n",
      "0.02   0  12   0   5  0.2941  1.0  ...  1.0000  1.0  0.2941  0.4545   0.9091  0.7692\n",
      "0.03   0  12   0   5  0.2941  1.0  ...  1.0000  1.0  0.2941  0.4545   0.9091  0.7692\n",
      "0.04   2   9   0   5  0.4375  1.0  ...  0.8182  1.0  0.3571  0.5263   0.9091  0.7692\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  12   0   5   0  0.7059  0.0  ...  0.0000  0.0  0.0000     NaN   0.9091  0.7692\n",
      "0.98  12   0   5   0  0.7059  0.0  ...  0.0000  0.0  0.0000     NaN   0.9091  0.7692\n",
      "0.99  12   0   5   0  0.7059  0.0  ...  0.0000  0.0  0.0000     NaN   0.9091  0.7692\n",
      "1.00  12   0   5   0  0.7059  0.0  ...  0.0000  0.0  0.0000     NaN   0.9091  0.7692\n",
      "1.01  12   0   5   0  0.7059  0.0  ...  0.0000  0.0  0.0000     NaN   0.9091  0.7692\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105      0.5  0.3077\n",
      "0.01   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105      0.5  0.3077\n",
      "0.02   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105      0.5  0.3077\n",
      "0.03   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105      0.5  0.3077\n",
      "0.04   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105      0.5  0.3077\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN      0.5  0.3077\n",
      "0.98  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN      0.5  0.3077\n",
      "0.99  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN      0.5  0.3077\n",
      "1.00  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN      0.5  0.3077\n",
      "1.01  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN      0.5  0.3077\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 234\n",
      "Total correct samples: 234?234\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006241  0.035199\n",
      "4      0.003744  0.021119\n",
      "...         ...       ...\n",
      "18013  0.000007  0.000026\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000013  0.000056\n",
      "18016  0.000003  0.000045\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.8165 - acc: 0.5000 - auROC: 0.5893 - val_loss: 0.7058 - val_acc: 0.5208 - val_auROC: 0.6311\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.7223 - acc: 0.5190 - auROC: 0.5426 - val_loss: 0.6530 - val_acc: 0.5833 - val_auROC: 0.7526\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6750 - acc: 0.5667 - auROC: 0.6774 - val_loss: 0.6385 - val_acc: 0.6250 - val_auROC: 0.8168\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6349 - acc: 0.5929 - auROC: 0.7863 - val_loss: 0.6328 - val_acc: 0.6875 - val_auROC: 0.8229\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6176 - acc: 0.6452 - auROC: 0.7984 - val_loss: 0.6167 - val_acc: 0.7708 - val_auROC: 0.8394\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6051 - acc: 0.6667 - auROC: 0.8109 - val_loss: 0.6044 - val_acc: 0.7708 - val_auROC: 0.8437\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5966 - acc: 0.6905 - auROC: 0.8266 - val_loss: 0.5959 - val_acc: 0.8125 - val_auROC: 0.8516\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5847 - acc: 0.7143 - auROC: 0.8386 - val_loss: 0.5921 - val_acc: 0.8333 - val_auROC: 0.8655\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5775 - acc: 0.7405 - auROC: 0.8500 - val_loss: 0.5867 - val_acc: 0.8333 - val_auROC: 0.8568\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5710 - acc: 0.7571 - auROC: 0.8509 - val_loss: 0.5808 - val_acc: 0.8333 - val_auROC: 0.8542\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5636 - acc: 0.7714 - auROC: 0.8675 - val_loss: 0.5715 - val_acc: 0.8125 - val_auROC: 0.8628\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5552 - acc: 0.7762 - auROC: 0.8725 - val_loss: 0.5631 - val_acc: 0.8125 - val_auROC: 0.8715\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5465 - acc: 0.7762 - auROC: 0.8776 - val_loss: 0.5546 - val_acc: 0.8125 - val_auROC: 0.8863\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5342 - acc: 0.7810 - auROC: 0.8928 - val_loss: 0.5513 - val_acc: 0.7917 - val_auROC: 0.8672\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5245 - acc: 0.7810 - auROC: 0.9021 - val_loss: 0.5461 - val_acc: 0.8125 - val_auROC: 0.8646\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5164 - acc: 0.7881 - auROC: 0.8990 - val_loss: 0.5428 - val_acc: 0.8333 - val_auROC: 0.8646\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5088 - acc: 0.7905 - auROC: 0.9024 - val_loss: 0.5361 - val_acc: 0.8542 - val_auROC: 0.8628\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5015 - acc: 0.7929 - auROC: 0.9103 - val_loss: 0.5383 - val_acc: 0.8333 - val_auROC: 0.8715\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4932 - acc: 0.8000 - auROC: 0.9134 - val_loss: 0.5275 - val_acc: 0.8542 - val_auROC: 0.8602\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4874 - acc: 0.7905 - auROC: 0.9048 - val_loss: 0.5260 - val_acc: 0.8125 - val_auROC: 0.8507\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4800 - acc: 0.7690 - auROC: 0.9064 - val_loss: 0.5169 - val_acc: 0.8125 - val_auROC: 0.8628\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4676 - acc: 0.7738 - auROC: 0.9208 - val_loss: 0.5141 - val_acc: 0.8125 - val_auROC: 0.8941\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4632 - acc: 0.8238 - auROC: 0.9221 - val_loss: 0.5114 - val_acc: 0.8542 - val_auROC: 0.8793\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4560 - acc: 0.8881 - auROC: 0.9226 - val_loss: 0.5086 - val_acc: 0.8750 - val_auROC: 0.8611\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4508 - acc: 0.8786 - auROC: 0.9189 - val_loss: 0.5038 - val_acc: 0.8750 - val_auROC: 0.8611\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4466 - acc: 0.8833 - auROC: 0.9166 - val_loss: 0.4911 - val_acc: 0.8750 - val_auROC: 0.8785\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4382 - acc: 0.8833 - auROC: 0.9185 - val_loss: 0.5035 - val_acc: 0.8750 - val_auROC: 0.8585\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4342 - acc: 0.8810 - auROC: 0.9200 - val_loss: 0.4941 - val_acc: 0.8750 - val_auROC: 0.8715\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4260 - acc: 0.9000 - auROC: 0.9285 - val_loss: 0.4744 - val_acc: 0.8750 - val_auROC: 0.9080\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.4198 - acc: 0.9071 - auROC: 0.9311 - val_loss: 0.4735 - val_acc: 0.8542 - val_auROC: 0.9062\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4140 - acc: 0.8929 - auROC: 0.9293 - val_loss: 0.4832 - val_acc: 0.8750 - val_auROC: 0.8785\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4081 - acc: 0.8929 - auROC: 0.9293 - val_loss: 0.4453 - val_acc: 0.8958 - val_auROC: 0.9175\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4028 - acc: 0.8929 - auROC: 0.9309 - val_loss: 0.4329 - val_acc: 0.9167 - val_auROC: 0.9236\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3930 - acc: 0.8976 - auROC: 0.9335 - val_loss: 0.4675 - val_acc: 0.8750 - val_auROC: 0.8819\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3951 - acc: 0.8905 - auROC: 0.9324 - val_loss: 0.4718 - val_acc: 0.8750 - val_auROC: 0.8837\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3893 - acc: 0.8976 - auROC: 0.9361 - val_loss: 0.4433 - val_acc: 0.8958 - val_auROC: 0.9054\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3814 - acc: 0.9095 - auROC: 0.9372 - val_loss: 0.4289 - val_acc: 0.8958 - val_auROC: 0.9184\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3748 - acc: 0.9095 - auROC: 0.9377 - val_loss: 0.4367 - val_acc: 0.8958 - val_auROC: 0.9002\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3709 - acc: 0.9048 - auROC: 0.9380 - val_loss: 0.4173 - val_acc: 0.8958 - val_auROC: 0.9227\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3666 - acc: 0.9095 - auROC: 0.9383 - val_loss: 0.4064 - val_acc: 0.9167 - val_auROC: 0.9297\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3620 - acc: 0.9071 - auROC: 0.9410 - val_loss: 0.4289 - val_acc: 0.8750 - val_auROC: 0.8941\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3561 - acc: 0.9095 - auROC: 0.9455 - val_loss: 0.3963 - val_acc: 0.8958 - val_auROC: 0.9375\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3480 - acc: 0.9167 - auROC: 0.9461 - val_loss: 0.4134 - val_acc: 0.8750 - val_auROC: 0.9210\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3443 - acc: 0.9167 - auROC: 0.9477 - val_loss: 0.4332 - val_acc: 0.8542 - val_auROC: 0.8941\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3412 - acc: 0.9167 - auROC: 0.9495 - val_loss: 0.4203 - val_acc: 0.8542 - val_auROC: 0.9141\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3383 - acc: 0.9167 - auROC: 0.9498 - val_loss: 0.4217 - val_acc: 0.8750 - val_auROC: 0.8880\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3368 - acc: 0.9143 - auROC: 0.9491 - val_loss: 0.3950 - val_acc: 0.8958 - val_auROC: 0.9123\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3377 - acc: 0.9190 - auROC: 0.9502 - val_loss: 0.3823 - val_acc: 0.8958 - val_auROC: 0.9332\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3397 - acc: 0.9095 - auROC: 0.9450 - val_loss: 0.4786 - val_acc: 0.8333 - val_auROC: 0.8507\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3447 - acc: 0.9071 - auROC: 0.9413 - val_loss: 0.4797 - val_acc: 0.7917 - val_auROC: 0.8620\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3415 - acc: 0.9071 - auROC: 0.9421 - val_loss: 0.4383 - val_acc: 0.8333 - val_auROC: 0.8898\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3285 - acc: 0.9167 - auROC: 0.9442 - val_loss: 0.4315 - val_acc: 0.8750 - val_auROC: 0.8802\n",
      "Epoch 53/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3016 - acc: 0.9375 - auROC: 0.9647\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3283 - acc: 0.9119 - auROC: 0.9473 - val_loss: 0.4247 - val_acc: 0.8750 - val_auROC: 0.8776\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3257 - acc: 0.9119 - auROC: 0.9494 - val_loss: 0.4247 - val_acc: 0.8750 - val_auROC: 0.8802\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3246 - acc: 0.9143 - auROC: 0.9495 - val_loss: 0.4254 - val_acc: 0.8750 - val_auROC: 0.8828\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3233 - acc: 0.9143 - auROC: 0.9500 - val_loss: 0.4263 - val_acc: 0.8750 - val_auROC: 0.8872\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3220 - acc: 0.9167 - auROC: 0.9486 - val_loss: 0.4281 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 58/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3130 - acc: 0.9219 - auROC: 0.9531\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3209 - acc: 0.9167 - auROC: 0.9489 - val_loss: 0.4304 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3201 - acc: 0.9167 - auROC: 0.9493 - val_loss: 0.4306 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3200 - acc: 0.9167 - auROC: 0.9494 - val_loss: 0.4308 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3199 - acc: 0.9167 - auROC: 0.9494 - val_loss: 0.4310 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3197 - acc: 0.9190 - auROC: 0.9497 - val_loss: 0.4311 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 63/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3528 - acc: 0.9141 - auROC: 0.9226\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3196 - acc: 0.9190 - auROC: 0.9498 - val_loss: 0.4312 - val_acc: 0.8542 - val_auROC: 0.8802\n",
      "Epoch 00063: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 63/362\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.3379 - acc: 0.9119 - auROC: 0.9471 - val_loss: 0.3963 - val_acc: 0.8958 - val_auROC: 0.9358\n",
      "Epoch 64/362\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3302 - acc: 0.9119 - auROC: 0.9524 - val_loss: 0.4003 - val_acc: 0.8750 - val_auROC: 0.9288\n",
      "Epoch 65/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3257 - acc: 0.9167 - auROC: 0.9538 - val_loss: 0.3976 - val_acc: 0.8958 - val_auROC: 0.9306\n",
      "Epoch 66/362\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.3229 - acc: 0.9190 - auROC: 0.9549 - val_loss: 0.3940 - val_acc: 0.8958 - val_auROC: 0.9323\n",
      "Epoch 67/362\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3206 - acc: 0.9238 - auROC: 0.9557 - val_loss: 0.3943 - val_acc: 0.8958 - val_auROC: 0.9227\n",
      "Epoch 68/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3190 - acc: 0.9238 - auROC: 0.9565 - val_loss: 0.3959 - val_acc: 0.8958 - val_auROC: 0.9219\n",
      "Epoch 69/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3174 - acc: 0.9262 - auROC: 0.9568 - val_loss: 0.3952 - val_acc: 0.8958 - val_auROC: 0.9245\n",
      "Epoch 70/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3156 - acc: 0.9286 - auROC: 0.9578 - val_loss: 0.3954 - val_acc: 0.8958 - val_auROC: 0.9253\n",
      "Epoch 71/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3141 - acc: 0.9286 - auROC: 0.9589 - val_loss: 0.3969 - val_acc: 0.8958 - val_auROC: 0.9245\n",
      "Epoch 72/362\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3114 - acc: 0.9333 - auROC: 0.9598 - val_loss: 0.3969 - val_acc: 0.8958 - val_auROC: 0.9253\n",
      "Epoch 73/362\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3093 - acc: 0.9333 - auROC: 0.9603 - val_loss: 0.3944 - val_acc: 0.8750 - val_auROC: 0.9280\n",
      "Epoch 74/362\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3065 - acc: 0.9357 - auROC: 0.9643 - val_loss: 0.3906 - val_acc: 0.8750 - val_auROC: 0.9288\n",
      "Epoch 75/362\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3042 - acc: 0.9405 - auROC: 0.9642 - val_loss: 0.3902 - val_acc: 0.8750 - val_auROC: 0.9288\n",
      "Epoch 76/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3028 - acc: 0.9405 - auROC: 0.9641 - val_loss: 0.3943 - val_acc: 0.8750 - val_auROC: 0.9280\n",
      "Epoch 77/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3008 - acc: 0.9405 - auROC: 0.9666 - val_loss: 0.3997 - val_acc: 0.8750 - val_auROC: 0.9236\n",
      "Epoch 78/362\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2999 - acc: 0.9405 - auROC: 0.9670 - val_loss: 0.4031 - val_acc: 0.8750 - val_auROC: 0.9219\n",
      "Epoch 79/362\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2985 - acc: 0.9429 - auROC: 0.9673 - val_loss: 0.4040 - val_acc: 0.8750 - val_auROC: 0.9201\n",
      "Epoch 80/362\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2975 - acc: 0.9429 - auROC: 0.9680 - val_loss: 0.4049 - val_acc: 0.8750 - val_auROC: 0.9193\n",
      "Epoch 81/362\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2967 - acc: 0.9429 - auROC: 0.9684 - val_loss: 0.4072 - val_acc: 0.8750 - val_auROC: 0.9158\n",
      "Epoch 82/362\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2961 - acc: 0.9429 - auROC: 0.9690 - val_loss: 0.4086 - val_acc: 0.8750 - val_auROC: 0.9097\n",
      "Epoch 83/362\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2956 - acc: 0.9429 - auROC: 0.9694 - val_loss: 0.4096 - val_acc: 0.8750 - val_auROC: 0.9106\n",
      "Epoch 84/362\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2945 - acc: 0.9429 - auROC: 0.9697 - val_loss: 0.4196 - val_acc: 0.8750 - val_auROC: 0.9010\n",
      "Epoch 85/362\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2932 - acc: 0.9429 - auROC: 0.9705 - val_loss: 0.4318 - val_acc: 0.8542 - val_auROC: 0.8906\n",
      "Epoch 86/362\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2916 - acc: 0.9452 - auROC: 0.9711 - val_loss: 0.4386 - val_acc: 0.8333 - val_auROC: 0.8889\n",
      "Epoch 87/362\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2907 - acc: 0.9452 - auROC: 0.9716 - val_loss: 0.4427 - val_acc: 0.8125 - val_auROC: 0.8872\n",
      "Epoch 88/362\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2897 - acc: 0.9476 - auROC: 0.9726 - val_loss: 0.4404 - val_acc: 0.8333 - val_auROC: 0.8880\n",
      "Epoch 89/362\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2890 - acc: 0.9500 - auROC: 0.9726 - val_loss: 0.4406 - val_acc: 0.8333 - val_auROC: 0.8872\n",
      "Epoch 90/362\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2815 - acc: 0.9557 - auROC: 0.9753Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.2881 - acc: 0.9500 - auROC: 0.9730 - val_loss: 0.4537 - val_acc: 0.8125 - val_auROC: 0.8681\n",
      "Epoch 00090: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "1     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "2     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "3     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "4     0.0    0.0    0.0 -0.177295  ...    0.0 -0.194487 -0.076178    0.0\n",
      "5     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "6     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "7     0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "8     0.0    0.0    0.0 -0.176332  ...    0.0 -0.133659 -0.076178    0.0\n",
      "9     0.0    0.0    0.0 -0.174301  ...    0.0 -0.036804 -0.076178    0.0\n",
      "10    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "11    0.0    0.0    0.0 -0.176888  ...    0.0 -0.223964 -0.076178    0.0\n",
      "12    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "13    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "14    0.0    0.0    0.0 -0.177295  ...    0.0 -0.170814 -0.076178    0.0\n",
      "15    0.0    0.0    0.0 -0.176410  ...    0.0 -0.205531 -0.076178    0.0\n",
      "16    0.0    0.0    0.0 -0.176458  ...    0.0 -0.223964 -0.076178    0.0\n",
      "17    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "18    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "19    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "20    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "21    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "22    0.0    0.0    0.0 -0.177295  ...    0.0 -0.185461 -0.076178    0.0\n",
      "23    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "24    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "25    0.0    0.0    0.0 -0.177295  ...    0.0 -0.223964 -0.076178    0.0\n",
      "\n",
      "[26 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  14   0  10  0.4167  1.0  ...  1.0  1.0  0.4167  0.5882   0.8034  0.8235\n",
      "0.01   0  14   0  10  0.4167  1.0  ...  1.0  1.0  0.4167  0.5882   0.8034  0.8235\n",
      "0.02   0  14   0  10  0.4167  1.0  ...  1.0  1.0  0.4167  0.5882   0.8034  0.8235\n",
      "0.03   0  14   0  10  0.4167  1.0  ...  1.0  1.0  0.4167  0.5882   0.8034  0.8235\n",
      "0.04   0  14   0  10  0.4167  1.0  ...  1.0  1.0  0.4167  0.5882   0.8034  0.8235\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  14   0  10   0  0.5833  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8235\n",
      "0.98  14   0  10   0  0.5833  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8235\n",
      "0.99  14   0  10   0  0.5833  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8235\n",
      "1.00  14   0  10   0  0.5833  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8235\n",
      "1.01  14   0  10   0  0.5833  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8235\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  10   0  14  0.5833  1.0  ...  1.0  1.0  0.5833  0.7368   0.8034  0.8889\n",
      "0.01   0  10   0  14  0.5833  1.0  ...  1.0  1.0  0.5833  0.7368   0.8034  0.8889\n",
      "0.02   0  10   0  14  0.5833  1.0  ...  1.0  1.0  0.5833  0.7368   0.8034  0.8889\n",
      "0.03   0  10   0  14  0.5833  1.0  ...  1.0  1.0  0.5833  0.7368   0.8034  0.8889\n",
      "0.04   0  10   0  14  0.5833  1.0  ...  1.0  1.0  0.5833  0.7368   0.8034  0.8889\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  10   0  14   0  0.4167  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8889\n",
      "0.98  10   0  14   0  0.4167  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8889\n",
      "0.99  10   0  14   0  0.4167  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8889\n",
      "1.00  10   0  14   0  0.4167  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8889\n",
      "1.01  10   0  14   0  0.4167  0.0  ...  0.0  0.0  0.0000     NaN   0.8034  0.8889\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr    F1  ROC-AUC   F-max\n",
      "t                                  ...                                         \n",
      "0.00   0  23   0   1  0.0417  1.0  ...  1.0  1.0  0.0417  0.08   0.3182  0.0952\n",
      "0.01   0  23   0   1  0.0417  1.0  ...  1.0  1.0  0.0417  0.08   0.3182  0.0952\n",
      "0.02   0  23   0   1  0.0417  1.0  ...  1.0  1.0  0.0417  0.08   0.3182  0.0952\n",
      "0.03   0  23   0   1  0.0417  1.0  ...  1.0  1.0  0.0417  0.08   0.3182  0.0952\n",
      "0.04   0  23   0   1  0.0417  1.0  ...  1.0  1.0  0.0417  0.08   0.3182  0.0952\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...   ...      ...     ...\n",
      "0.97  23   0   1   0  0.9583  0.0  ...  0.0  0.0  0.0000   NaN   0.3182  0.0952\n",
      "0.98  23   0   1   0  0.9583  0.0  ...  0.0  0.0  0.0000   NaN   0.3182  0.0952\n",
      "0.99  23   0   1   0  0.9583  0.0  ...  0.0  0.0  0.0000   NaN   0.3182  0.0952\n",
      "1.00  23   0   1   0  0.9583  0.0  ...  0.0  0.0  0.0000   NaN   0.3182  0.0952\n",
      "1.01  23   0   1   0  0.9583  0.0  ...  0.0  0.0  0.0000   NaN   0.3182  0.0952\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC   F-max\n",
      "t                                 ...                                          \n",
      "0.00   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.225  0.2222\n",
      "0.01   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.225  0.2222\n",
      "0.02   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.225  0.2222\n",
      "0.03   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.225  0.2222\n",
      "0.04   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.225  0.2222\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...     ...\n",
      "0.97  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.225  0.2222\n",
      "0.98  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.225  0.2222\n",
      "0.99  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.225  0.2222\n",
      "1.00  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.225  0.2222\n",
      "1.01  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.225  0.2222\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  20   0   4  0.1667  1.0  ...  1.0000  1.0  0.1667  0.2857   0.8772  0.6667\n",
      "0.01   0  20   0   4  0.1667  1.0  ...  1.0000  1.0  0.1667  0.2857   0.8772  0.6667\n",
      "0.02   0  20   0   4  0.1667  1.0  ...  1.0000  1.0  0.1667  0.2857   0.8772  0.6667\n",
      "0.03   0  20   0   4  0.1667  1.0  ...  1.0000  1.0  0.1667  0.2857   0.8772  0.6667\n",
      "0.04   1  18   0   4  0.2174  1.0  ...  0.9474  1.0  0.1818  0.3077   0.8772  0.6667\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  20   0   4   0  0.8333  0.0  ...  0.0000  0.0  0.0000     NaN   0.8772  0.6667\n",
      "0.98  20   0   4   0  0.8333  0.0  ...  0.0000  0.0  0.0000     NaN   0.8772  0.6667\n",
      "0.99  20   0   4   0  0.8333  0.0  ...  0.0000  0.0  0.0000     NaN   0.8772  0.6667\n",
      "1.00  20   0   4   0  0.8333  0.0  ...  0.0000  0.0  0.0000     NaN   0.8772  0.6667\n",
      "1.01  20   0   4   0  0.8333  0.0  ...  0.0000  0.0  0.0000     NaN   0.8772  0.6667\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC   F-max\n",
      "t                                 ...                                          \n",
      "0.00   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.675  0.3333\n",
      "0.01   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.675  0.3333\n",
      "0.02   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.675  0.3333\n",
      "0.03   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.675  0.3333\n",
      "0.04   0  21   0   3  0.125  1.0  ...  1.0  1.0  0.125  0.2222    0.675  0.3333\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...     ...\n",
      "0.97  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.675  0.3333\n",
      "0.98  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.675  0.3333\n",
      "0.99  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.675  0.3333\n",
      "1.00  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.675  0.3333\n",
      "1.01  21   0   3   0  0.875  0.0  ...  0.0  0.0  0.000     NaN    0.675  0.3333\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  17   0   7  0.2917  1.0  ...  1.0  1.0  0.2917  0.4516   0.7135  0.5455\n",
      "0.01   0  17   0   7  0.2917  1.0  ...  1.0  1.0  0.2917  0.4516   0.7135  0.5455\n",
      "0.02   0  17   0   7  0.2917  1.0  ...  1.0  1.0  0.2917  0.4516   0.7135  0.5455\n",
      "0.03   0  17   0   7  0.2917  1.0  ...  1.0  1.0  0.2917  0.4516   0.7135  0.5455\n",
      "0.04   0  17   0   7  0.2917  1.0  ...  1.0  1.0  0.2917  0.4516   0.7135  0.5455\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  17   0   7   0  0.7083  0.0  ...  0.0  0.0  0.0000     NaN   0.7135  0.5455\n",
      "0.98  17   0   7   0  0.7083  0.0  ...  0.0  0.0  0.0000     NaN   0.7135  0.5455\n",
      "0.99  17   0   7   0  0.7083  0.0  ...  0.0  0.0  0.0000     NaN   0.7135  0.5455\n",
      "1.00  17   0   7   0  0.7083  0.0  ...  0.0  0.0  0.0000     NaN   0.7135  0.5455\n",
      "1.01  17   0   7   0  0.7083  0.0  ...  0.0  0.0  0.0000     NaN   0.7135  0.5455\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  22   0   2  0.0833  1.0  ...  1.0  1.0  0.0833  0.1538   0.2619   0.16\n",
      "0.01   0  22   0   2  0.0833  1.0  ...  1.0  1.0  0.0833  0.1538   0.2619   0.16\n",
      "0.02   0  22   0   2  0.0833  1.0  ...  1.0  1.0  0.0833  0.1538   0.2619   0.16\n",
      "0.03   0  22   0   2  0.0833  1.0  ...  1.0  1.0  0.0833  0.1538   0.2619   0.16\n",
      "0.04   0  21   0   2  0.0870  1.0  ...  1.0  1.0  0.0870  0.1600   0.2619   0.16\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  22   0   2   0  0.9167  0.0  ...  0.0  0.0  0.0000     NaN   0.2619   0.16\n",
      "0.98  22   0   2   0  0.9167  0.0  ...  0.0  0.0  0.0000     NaN   0.2619   0.16\n",
      "0.99  22   0   2   0  0.9167  0.0  ...  0.0  0.0  0.0000     NaN   0.2619   0.16\n",
      "1.00  22   0   2   0  0.9167  0.0  ...  0.0  0.0  0.0000     NaN   0.2619   0.16\n",
      "1.01  22   0   2   0  0.9167  0.0  ...  0.0  0.0  0.0000     NaN   0.2619   0.16\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 241\n",
      "Total correct samples: 241?241\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006060  0.034699\n",
      "4      0.003636  0.020820\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000055\n",
      "18016  0.000003  0.000045\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 0.8163 - acc: 0.5069 - auROC: 0.5921 - val_loss: 0.7618 - val_acc: 0.5200 - val_auROC: 0.5136\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.7289 - acc: 0.5185 - auROC: 0.5380 - val_loss: 0.7044 - val_acc: 0.5400 - val_auROC: 0.5712\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6862 - acc: 0.5370 - auROC: 0.6098 - val_loss: 0.6954 - val_acc: 0.5600 - val_auROC: 0.6320\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6519 - acc: 0.5671 - auROC: 0.7319 - val_loss: 0.7025 - val_acc: 0.5200 - val_auROC: 0.6144\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6306 - acc: 0.6412 - auROC: 0.7798 - val_loss: 0.7227 - val_acc: 0.6000 - val_auROC: 0.5920\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6179 - acc: 0.6667 - auROC: 0.7919 - val_loss: 0.6886 - val_acc: 0.6000 - val_auROC: 0.6568\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6040 - acc: 0.6736 - auROC: 0.8121 - val_loss: 0.6618 - val_acc: 0.6200 - val_auROC: 0.6888\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5940 - acc: 0.6968 - auROC: 0.8267 - val_loss: 0.6385 - val_acc: 0.6400 - val_auROC: 0.7008\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5874 - acc: 0.7037 - auROC: 0.8340 - val_loss: 0.6142 - val_acc: 0.6600 - val_auROC: 0.7648\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5798 - acc: 0.7153 - auROC: 0.8441 - val_loss: 0.6263 - val_acc: 0.6800 - val_auROC: 0.7232\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5707 - acc: 0.7269 - auROC: 0.8498 - val_loss: 0.6234 - val_acc: 0.6600 - val_auROC: 0.7312\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5625 - acc: 0.7315 - auROC: 0.8544 - val_loss: 0.6154 - val_acc: 0.6800 - val_auROC: 0.7488\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5547 - acc: 0.7338 - auROC: 0.8601 - val_loss: 0.6143 - val_acc: 0.6400 - val_auROC: 0.7512\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5473 - acc: 0.7384 - auROC: 0.8642 - val_loss: 0.6030 - val_acc: 0.6800 - val_auROC: 0.7784\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5366 - acc: 0.7431 - auROC: 0.8754 - val_loss: 0.5824 - val_acc: 0.6800 - val_auROC: 0.8096\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5254 - acc: 0.7454 - auROC: 0.8858 - val_loss: 0.5734 - val_acc: 0.7000 - val_auROC: 0.8168\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5161 - acc: 0.7523 - auROC: 0.8932 - val_loss: 0.5582 - val_acc: 0.7400 - val_auROC: 0.8440\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5075 - acc: 0.7546 - auROC: 0.8973 - val_loss: 0.5643 - val_acc: 0.7200 - val_auROC: 0.8248\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4966 - acc: 0.7523 - auROC: 0.9093 - val_loss: 0.5404 - val_acc: 0.6800 - val_auROC: 0.8440\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4876 - acc: 0.7593 - auROC: 0.9202 - val_loss: 0.5354 - val_acc: 0.7000 - val_auROC: 0.8464\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4752 - acc: 0.7685 - auROC: 0.9275 - val_loss: 0.5333 - val_acc: 0.7200 - val_auROC: 0.8392\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4657 - acc: 0.8287 - auROC: 0.9313 - val_loss: 0.5182 - val_acc: 0.8000 - val_auROC: 0.8512\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4570 - acc: 0.8796 - auROC: 0.9344 - val_loss: 0.5297 - val_acc: 0.8000 - val_auROC: 0.8520\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4482 - acc: 0.8866 - auROC: 0.9363 - val_loss: 0.5143 - val_acc: 0.8200 - val_auROC: 0.8512\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4364 - acc: 0.9005 - auROC: 0.9453 - val_loss: 0.5241 - val_acc: 0.7800 - val_auROC: 0.8264\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4319 - acc: 0.9028 - auROC: 0.9411 - val_loss: 0.5191 - val_acc: 0.8000 - val_auROC: 0.8296\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4274 - acc: 0.9028 - auROC: 0.9432 - val_loss: 0.5062 - val_acc: 0.8000 - val_auROC: 0.8384\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4083 - acc: 0.9236 - auROC: 0.9506 - val_loss: 0.5275 - val_acc: 0.8000 - val_auROC: 0.8160\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4024 - acc: 0.9190 - auROC: 0.9540 - val_loss: 0.5027 - val_acc: 0.8000 - val_auROC: 0.8344\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4018 - acc: 0.9213 - auROC: 0.9503 - val_loss: 0.5155 - val_acc: 0.7800 - val_auROC: 0.8296\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3864 - acc: 0.9213 - auROC: 0.9588 - val_loss: 0.5237 - val_acc: 0.7800 - val_auROC: 0.8136\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3799 - acc: 0.9329 - auROC: 0.9621 - val_loss: 0.4876 - val_acc: 0.8000 - val_auROC: 0.8448\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3728 - acc: 0.9306 - auROC: 0.9603 - val_loss: 0.5092 - val_acc: 0.7800 - val_auROC: 0.8264\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3652 - acc: 0.9282 - auROC: 0.9626 - val_loss: 0.5093 - val_acc: 0.7600 - val_auROC: 0.8272\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3569 - acc: 0.9375 - auROC: 0.9652 - val_loss: 0.4820 - val_acc: 0.8000 - val_auROC: 0.8520\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3552 - acc: 0.9398 - auROC: 0.9657 - val_loss: 0.4875 - val_acc: 0.8000 - val_auROC: 0.8368\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3450 - acc: 0.9491 - auROC: 0.9679 - val_loss: 0.4985 - val_acc: 0.7800 - val_auROC: 0.8320\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3408 - acc: 0.9491 - auROC: 0.9663 - val_loss: 0.4987 - val_acc: 0.7600 - val_auROC: 0.8352\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3360 - acc: 0.9444 - auROC: 0.9657 - val_loss: 0.5170 - val_acc: 0.7600 - val_auROC: 0.8200\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3289 - acc: 0.9444 - auROC: 0.9692 - val_loss: 0.4505 - val_acc: 0.8000 - val_auROC: 0.8760\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3267 - acc: 0.9537 - auROC: 0.9696 - val_loss: 0.4726 - val_acc: 0.8000 - val_auROC: 0.8600\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3282 - acc: 0.9468 - auROC: 0.9694 - val_loss: 0.5088 - val_acc: 0.7200 - val_auROC: 0.8296\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3218 - acc: 0.9468 - auROC: 0.9705 - val_loss: 0.4563 - val_acc: 0.8000 - val_auROC: 0.8736\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3095 - acc: 0.9537 - auROC: 0.9733 - val_loss: 0.5379 - val_acc: 0.7200 - val_auROC: 0.8136\n",
      "Epoch 45/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3132 - acc: 0.9297 - auROC: 0.9683\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3125 - acc: 0.9421 - auROC: 0.9710 - val_loss: 0.4509 - val_acc: 0.7800 - val_auROC: 0.8800\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3069 - acc: 0.9514 - auROC: 0.9713 - val_loss: 0.4477 - val_acc: 0.7800 - val_auROC: 0.8840\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3055 - acc: 0.9514 - auROC: 0.9722 - val_loss: 0.4536 - val_acc: 0.7800 - val_auROC: 0.8800\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3013 - acc: 0.9537 - auROC: 0.9740 - val_loss: 0.4630 - val_acc: 0.7800 - val_auROC: 0.8760\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2980 - acc: 0.9583 - auROC: 0.9730 - val_loss: 0.4715 - val_acc: 0.7800 - val_auROC: 0.8584\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2969 - acc: 0.9583 - auROC: 0.9734 - val_loss: 0.4757 - val_acc: 0.7800 - val_auROC: 0.8584\n",
      "Epoch 51/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2690 - acc: 0.9688 - auROC: 0.9927\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2965 - acc: 0.9583 - auROC: 0.9742 - val_loss: 0.4761 - val_acc: 0.7800 - val_auROC: 0.8584\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2956 - acc: 0.9606 - auROC: 0.9746 - val_loss: 0.4758 - val_acc: 0.7800 - val_auROC: 0.8584\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2954 - acc: 0.9606 - auROC: 0.9746 - val_loss: 0.4755 - val_acc: 0.7800 - val_auROC: 0.8592\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2953 - acc: 0.9606 - auROC: 0.9746 - val_loss: 0.4749 - val_acc: 0.7800 - val_auROC: 0.8592\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2951 - acc: 0.9606 - auROC: 0.9748 - val_loss: 0.4744 - val_acc: 0.7800 - val_auROC: 0.8600\n",
      "Epoch 56/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3087 - acc: 0.9453 - auROC: 0.9773\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2950 - acc: 0.9606 - auROC: 0.9749 - val_loss: 0.4737 - val_acc: 0.7800 - val_auROC: 0.8600\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2948 - acc: 0.9583 - auROC: 0.9750 - val_loss: 0.4730 - val_acc: 0.7800 - val_auROC: 0.8600\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2947 - acc: 0.9583 - auROC: 0.9754 - val_loss: 0.4723 - val_acc: 0.7800 - val_auROC: 0.8600\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2946 - acc: 0.9583 - auROC: 0.9754 - val_loss: 0.4717 - val_acc: 0.7800 - val_auROC: 0.8600\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2944 - acc: 0.9583 - auROC: 0.9758 - val_loss: 0.4711 - val_acc: 0.7800 - val_auROC: 0.8608\n",
      "Epoch 61/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3081 - acc: 0.9531 - auROC: 0.9569Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2944 - acc: 0.9583 - auROC: 0.9759 - val_loss: 0.4703 - val_acc: 0.7800 - val_auROC: 0.8600\n",
      "Epoch 00061: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 61/360\n",
      "4/4 [==============================] - 1s 188ms/step - loss: 0.3053 - acc: 0.9514 - auROC: 0.9722 - val_loss: 0.4568 - val_acc: 0.7800 - val_auROC: 0.8816\n",
      "Epoch 62/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2954 - acc: 0.9583 - auROC: 0.9760 - val_loss: 0.4605 - val_acc: 0.7800 - val_auROC: 0.8832\n",
      "Epoch 63/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2918 - acc: 0.9606 - auROC: 0.9772 - val_loss: 0.4641 - val_acc: 0.7800 - val_auROC: 0.8760\n",
      "Epoch 64/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2894 - acc: 0.9630 - auROC: 0.9777 - val_loss: 0.4655 - val_acc: 0.7800 - val_auROC: 0.8720\n",
      "Epoch 65/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2874 - acc: 0.9630 - auROC: 0.9781 - val_loss: 0.4654 - val_acc: 0.7800 - val_auROC: 0.8736\n",
      "Epoch 66/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2860 - acc: 0.9630 - auROC: 0.9786 - val_loss: 0.4649 - val_acc: 0.7800 - val_auROC: 0.8744\n",
      "Epoch 67/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2847 - acc: 0.9653 - auROC: 0.9792 - val_loss: 0.4640 - val_acc: 0.7800 - val_auROC: 0.8760\n",
      "Epoch 68/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2836 - acc: 0.9653 - auROC: 0.9794 - val_loss: 0.4642 - val_acc: 0.7800 - val_auROC: 0.8760\n",
      "Epoch 69/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2828 - acc: 0.9653 - auROC: 0.9798 - val_loss: 0.4636 - val_acc: 0.7800 - val_auROC: 0.8768\n",
      "Epoch 70/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2817 - acc: 0.9653 - auROC: 0.9803 - val_loss: 0.4631 - val_acc: 0.7800 - val_auROC: 0.8768\n",
      "Epoch 71/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2808 - acc: 0.9653 - auROC: 0.9818 - val_loss: 0.4623 - val_acc: 0.7800 - val_auROC: 0.8792\n",
      "Epoch 72/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2792 - acc: 0.9653 - auROC: 0.9835 - val_loss: 0.4612 - val_acc: 0.7800 - val_auROC: 0.8800\n",
      "Epoch 73/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2784 - acc: 0.9653 - auROC: 0.9839 - val_loss: 0.4602 - val_acc: 0.8000 - val_auROC: 0.8808\n",
      "Epoch 74/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2757 - acc: 0.9699 - auROC: 0.9844 - val_loss: 0.4592 - val_acc: 0.8000 - val_auROC: 0.8816\n",
      "Epoch 75/360\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2745 - acc: 0.9676 - auROC: 0.9851 - val_loss: 0.4559 - val_acc: 0.8000 - val_auROC: 0.8808\n",
      "Epoch 76/360\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.2733 - acc: 0.9699 - auROC: 0.9857 - val_loss: 0.4533 - val_acc: 0.8000 - val_auROC: 0.8848\n",
      "Epoch 77/360\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.2722 - acc: 0.9699 - auROC: 0.9858 - val_loss: 0.4495 - val_acc: 0.8000 - val_auROC: 0.8888\n",
      "Epoch 78/360\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.2709 - acc: 0.9745 - auROC: 0.9859 - val_loss: 0.4460 - val_acc: 0.8000 - val_auROC: 0.8888\n",
      "Epoch 79/360\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.2698 - acc: 0.9769 - auROC: 0.9861 - val_loss: 0.4438 - val_acc: 0.8000 - val_auROC: 0.8904\n",
      "Epoch 80/360\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.2693 - acc: 0.9769 - auROC: 0.9862 - val_loss: 0.4426 - val_acc: 0.8000 - val_auROC: 0.8896\n",
      "Epoch 81/360\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2688 - acc: 0.9769 - auROC: 0.9863 - val_loss: 0.4435 - val_acc: 0.8000 - val_auROC: 0.8880\n",
      "Epoch 82/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2682 - acc: 0.9769 - auROC: 0.9864 - val_loss: 0.4434 - val_acc: 0.8000 - val_auROC: 0.8920\n",
      "Epoch 83/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2676 - acc: 0.9769 - auROC: 0.9865 - val_loss: 0.4427 - val_acc: 0.8000 - val_auROC: 0.8912\n",
      "Epoch 84/360\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2669 - acc: 0.9769 - auROC: 0.9866 - val_loss: 0.4439 - val_acc: 0.8000 - val_auROC: 0.8936\n",
      "Epoch 85/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2666 - acc: 0.9769 - auROC: 0.9866 - val_loss: 0.4451 - val_acc: 0.8000 - val_auROC: 0.8928\n",
      "Epoch 86/360\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2663 - acc: 0.9769 - auROC: 0.9867 - val_loss: 0.4464 - val_acc: 0.8000 - val_auROC: 0.8928\n",
      "Epoch 87/360\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2660 - acc: 0.9769 - auROC: 0.9867 - val_loss: 0.4471 - val_acc: 0.8000 - val_auROC: 0.8928\n",
      "Epoch 88/360\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2656 - acc: 0.9769 - auROC: 0.9868 - val_loss: 0.4472 - val_acc: 0.8000 - val_auROC: 0.8904\n",
      "Epoch 89/360\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2654 - acc: 0.9769 - auROC: 0.9869 - val_loss: 0.4482 - val_acc: 0.8000 - val_auROC: 0.8896\n",
      "Epoch 90/360\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2651 - acc: 0.9769 - auROC: 0.9869 - val_loss: 0.4492 - val_acc: 0.8000 - val_auROC: 0.8904\n",
      "Epoch 91/360\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2650 - acc: 0.9769 - auROC: 0.9869 - val_loss: 0.4487 - val_acc: 0.8000 - val_auROC: 0.8936\n",
      "Epoch 92/360\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.2646 - acc: 0.9769 - auROC: 0.9870 - val_loss: 0.4486 - val_acc: 0.8000 - val_auROC: 0.8936\n",
      "Epoch 93/360\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.2644 - acc: 0.9769 - auROC: 0.9871 - val_loss: 0.4483 - val_acc: 0.8000 - val_auROC: 0.8944\n",
      "Epoch 94/360\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.2642 - acc: 0.9769 - auROC: 0.9872 - val_loss: 0.4488 - val_acc: 0.8000 - val_auROC: 0.8920\n",
      "Epoch 95/360\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.2674 - acc: 0.9740 - auROC: 0.9855Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.2639 - acc: 0.9769 - auROC: 0.9872 - val_loss: 0.4493 - val_acc: 0.8000 - val_auROC: 0.8928\n",
      "Epoch 00095: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "1     0.0    0.0    0.0 -0.174494  ...    0.0 -0.213220 -0.072319    0.0\n",
      "2     0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "3     0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "4     0.0    0.0    0.0 -0.174644  ...    0.0 -0.188526 -0.072319    0.0\n",
      "5     0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "6     0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "7     0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "8     0.0    0.0    0.0 -0.174451  ...    0.0  0.510950 -0.072319    0.0\n",
      "9     0.0    0.0    0.0 -0.174644  ...    0.0 -0.122744 -0.072319    0.0\n",
      "10    0.0    0.0    0.0 -0.174644  ...    0.0 -0.191434 -0.045295    0.0\n",
      "11    0.0    0.0    0.0 -0.174644  ...    0.0 -0.051497  0.128290    0.0\n",
      "12    0.0    0.0    0.0 -0.174644  ...    0.0  0.350610  0.364807    0.0\n",
      "13    0.0    0.0    0.0 -0.174644  ...    0.0  0.407145 -0.072319    0.0\n",
      "14    0.0    0.0    0.0 -0.174064  ...    0.0 -0.189063 -0.072319    0.0\n",
      "15    0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "16    0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "17    0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "18    0.0    0.0    0.0 -0.174644  ...    0.0 -0.213220 -0.072319    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.9318  0.8571\n",
      "0.01   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.9318  0.8571\n",
      "0.02   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.9318  0.8571\n",
      "0.03   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.9318  0.8571\n",
      "0.04   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.9318  0.8571\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.8571\n",
      "0.98  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.8571\n",
      "0.99  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.8571\n",
      "1.00  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.8571\n",
      "1.01  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.8571\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0   5   0  12  0.7059  1.0  ...  1.0  1.0  0.7059  0.8276   0.9318  0.9565\n",
      "0.01   0   5   0  12  0.7059  1.0  ...  1.0  1.0  0.7059  0.8276   0.9318  0.9565\n",
      "0.02   0   5   0  12  0.7059  1.0  ...  1.0  1.0  0.7059  0.8276   0.9318  0.9565\n",
      "0.03   0   5   0  12  0.7059  1.0  ...  1.0  1.0  0.7059  0.8276   0.9318  0.9565\n",
      "0.04   0   5   0  12  0.7059  1.0  ...  1.0  1.0  0.7059  0.8276   0.9318  0.9565\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97   5   0  12   0  0.2941  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.9565\n",
      "0.98   5   0  12   0  0.2941  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.9565\n",
      "0.99   5   0  12   0  0.2941  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.9565\n",
      "1.00   5   0  12   0  0.2941  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.9565\n",
      "1.01   5   0  12   0  0.2941  0.0  ...  0.0  0.0  0.0000     NaN   0.9318  0.9565\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.1176\n",
      "0.01   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.1176\n",
      "0.02   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.1176\n",
      "0.03   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.1176\n",
      "0.04   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.0667  0.1176\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.1176\n",
      "0.98  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.1176\n",
      "0.99  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.1176\n",
      "1.00  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.1176\n",
      "1.01  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.0667  0.1176\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC  F-max\n",
      "t                                  ...                                       \n",
      "0.00   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      0.5    0.4\n",
      "0.01   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      0.5    0.4\n",
      "0.02   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      0.5    0.4\n",
      "0.03   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      0.5    0.4\n",
      "0.04   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3      0.5    0.4\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...    ...\n",
      "0.97  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      0.5    0.4\n",
      "0.98  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      0.5    0.4\n",
      "0.99  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      0.5    0.4\n",
      "1.00  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      0.5    0.4\n",
      "1.01  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN      0.5    0.4\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC   F-max\n",
      "t                                  ...                                        \n",
      "0.00   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.0769  0.3158\n",
      "0.01   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.0769  0.3158\n",
      "0.02   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.0769  0.3158\n",
      "0.03   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.0769  0.3158\n",
      "0.04   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.0769  0.3158\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...     ...\n",
      "0.97  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.0769  0.3158\n",
      "0.98  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.0769  0.3158\n",
      "0.99  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.0769  0.3158\n",
      "1.00  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.0769  0.3158\n",
      "1.01  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.0769  0.3158\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.4886  0.5333\n",
      "0.01   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.4886  0.5333\n",
      "0.02   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.4886  0.5333\n",
      "0.03   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.4886  0.5333\n",
      "0.04   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.4886  0.5333\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.4886  0.5333\n",
      "0.98  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.4886  0.5333\n",
      "0.99  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.4886  0.5333\n",
      "1.00  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.4886  0.5333\n",
      "1.01  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.4886  0.5333\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.5714  0.2222\n",
      "0.01   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.5714  0.2222\n",
      "0.02   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.5714  0.2222\n",
      "0.03   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.5714  0.2222\n",
      "0.04   0  15   0   2  0.1176  1.0  ...  1.0  1.0  0.1176  0.2105   0.5714  0.2222\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.5714  0.2222\n",
      "0.98  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.5714  0.2222\n",
      "0.99  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.5714  0.2222\n",
      "1.00  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.5714  0.2222\n",
      "1.01  15   0   2   0  0.8824  0.0  ...  0.0  0.0  0.0000     NaN   0.5714  0.2222\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 238\n",
      "Total correct samples: 238?238\n",
      "               mean       std\n",
      "0      0.000000e+00  0.000000\n",
      "1      0.000000e+00  0.000000\n",
      "2      0.000000e+00  0.000000\n",
      "3      6.136386e-03  0.034911\n",
      "4      3.681832e-03  0.020946\n",
      "...             ...       ...\n",
      "18013  8.212785e-06  0.000029\n",
      "18014  0.000000e+00  0.000000\n",
      "18015  9.506790e-06  0.000034\n",
      "18016  4.524404e-07  0.000002\n",
      "18017  0.000000e+00  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.8054 - acc: 0.5117 - auROC: 0.6081 - val_loss: 0.7579 - val_acc: 0.5000 - val_auROC: 0.4531\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.7056 - acc: 0.5350 - auROC: 0.5320 - val_loss: 0.7098 - val_acc: 0.5833 - val_auROC: 0.5252\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6742 - acc: 0.5794 - auROC: 0.6319 - val_loss: 0.6618 - val_acc: 0.5833 - val_auROC: 0.7483\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6389 - acc: 0.6379 - auROC: 0.7891 - val_loss: 0.6246 - val_acc: 0.6458 - val_auROC: 0.8325\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6308 - acc: 0.6425 - auROC: 0.8026 - val_loss: 0.6061 - val_acc: 0.6458 - val_auROC: 0.8507\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6103 - acc: 0.6636 - auROC: 0.8345 - val_loss: 0.5814 - val_acc: 0.6875 - val_auROC: 0.9089\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5953 - acc: 0.6916 - auROC: 0.8638 - val_loss: 0.5838 - val_acc: 0.6875 - val_auROC: 0.8967\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5883 - acc: 0.7383 - auROC: 0.8723 - val_loss: 0.5844 - val_acc: 0.7500 - val_auROC: 0.9010\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5792 - acc: 0.7850 - auROC: 0.8774 - val_loss: 0.5755 - val_acc: 0.7917 - val_auROC: 0.8993\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5728 - acc: 0.8154 - auROC: 0.8801 - val_loss: 0.5721 - val_acc: 0.7917 - val_auROC: 0.9175\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5663 - acc: 0.8294 - auROC: 0.8839 - val_loss: 0.5661 - val_acc: 0.7708 - val_auROC: 0.9149\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5597 - acc: 0.8341 - auROC: 0.8986 - val_loss: 0.5672 - val_acc: 0.7917 - val_auROC: 0.9002\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5575 - acc: 0.8294 - auROC: 0.9022 - val_loss: 0.5556 - val_acc: 0.8125 - val_auROC: 0.9089\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5500 - acc: 0.8505 - auROC: 0.8971 - val_loss: 0.5400 - val_acc: 0.8333 - val_auROC: 0.9410\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.5448 - acc: 0.8621 - auROC: 0.8936 - val_loss: 0.5355 - val_acc: 0.8542 - val_auROC: 0.9427\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5376 - acc: 0.8692 - auROC: 0.9021 - val_loss: 0.5412 - val_acc: 0.8333 - val_auROC: 0.9358\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5317 - acc: 0.8621 - auROC: 0.9032 - val_loss: 0.5327 - val_acc: 0.8542 - val_auROC: 0.9323\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5241 - acc: 0.8645 - auROC: 0.9088 - val_loss: 0.5149 - val_acc: 0.8542 - val_auROC: 0.9418\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.5146 - acc: 0.8481 - auROC: 0.9056 - val_loss: 0.4970 - val_acc: 0.8333 - val_auROC: 0.9549\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.5075 - acc: 0.8248 - auROC: 0.9020 - val_loss: 0.4826 - val_acc: 0.8125 - val_auROC: 0.9549\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4984 - acc: 0.8248 - auROC: 0.9047 - val_loss: 0.4838 - val_acc: 0.8333 - val_auROC: 0.9505\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4911 - acc: 0.8294 - auROC: 0.9079 - val_loss: 0.4793 - val_acc: 0.8542 - val_auROC: 0.9497\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4850 - acc: 0.8388 - auROC: 0.9096 - val_loss: 0.4706 - val_acc: 0.8750 - val_auROC: 0.9523\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4773 - acc: 0.8341 - auROC: 0.9091 - val_loss: 0.4615 - val_acc: 0.8333 - val_auROC: 0.9401\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4705 - acc: 0.8037 - auROC: 0.9128 - val_loss: 0.4543 - val_acc: 0.8125 - val_auROC: 0.9410\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4641 - acc: 0.8107 - auROC: 0.9126 - val_loss: 0.4521 - val_acc: 0.8333 - val_auROC: 0.9505\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4591 - acc: 0.8411 - auROC: 0.9182 - val_loss: 0.4530 - val_acc: 0.8333 - val_auROC: 0.9514\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4558 - acc: 0.8808 - auROC: 0.9203 - val_loss: 0.4594 - val_acc: 0.8958 - val_auROC: 0.9332\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4494 - acc: 0.8925 - auROC: 0.9237 - val_loss: 0.4562 - val_acc: 0.9167 - val_auROC: 0.9262\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4427 - acc: 0.8925 - auROC: 0.9252 - val_loss: 0.4371 - val_acc: 0.9167 - val_auROC: 0.9314\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4362 - acc: 0.8995 - auROC: 0.9216 - val_loss: 0.4291 - val_acc: 0.9167 - val_auROC: 0.9306\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4306 - acc: 0.9019 - auROC: 0.9278 - val_loss: 0.4354 - val_acc: 0.9167 - val_auROC: 0.9297\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4252 - acc: 0.9042 - auROC: 0.9311 - val_loss: 0.4344 - val_acc: 0.9167 - val_auROC: 0.9280\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4204 - acc: 0.9042 - auROC: 0.9335 - val_loss: 0.4338 - val_acc: 0.9167 - val_auROC: 0.9297\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4146 - acc: 0.9042 - auROC: 0.9344 - val_loss: 0.4253 - val_acc: 0.9167 - val_auROC: 0.9306\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4127 - acc: 0.9065 - auROC: 0.9347 - val_loss: 0.4209 - val_acc: 0.9167 - val_auROC: 0.9271\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.4036 - acc: 0.9089 - auROC: 0.9389 - val_loss: 0.4170 - val_acc: 0.9167 - val_auROC: 0.9245\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3950 - acc: 0.9159 - auROC: 0.9457 - val_loss: 0.4079 - val_acc: 0.9167 - val_auROC: 0.9323\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3848 - acc: 0.9229 - auROC: 0.9506 - val_loss: 0.4042 - val_acc: 0.9167 - val_auROC: 0.9314\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3838 - acc: 0.9206 - auROC: 0.9455 - val_loss: 0.4408 - val_acc: 0.8542 - val_auROC: 0.9045\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3776 - acc: 0.9229 - auROC: 0.9510 - val_loss: 0.4665 - val_acc: 0.8333 - val_auROC: 0.8880\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3719 - acc: 0.9299 - auROC: 0.9527 - val_loss: 0.4393 - val_acc: 0.8750 - val_auROC: 0.9028\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3587 - acc: 0.9322 - auROC: 0.9587 - val_loss: 0.4451 - val_acc: 0.8750 - val_auROC: 0.8837\n",
      "Epoch 44/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3359 - acc: 0.9688 - auROC: 0.9720\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3577 - acc: 0.9416 - auROC: 0.9567 - val_loss: 0.4189 - val_acc: 0.9167 - val_auROC: 0.9071\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3431 - acc: 0.9463 - auROC: 0.9591 - val_loss: 0.4166 - val_acc: 0.9167 - val_auROC: 0.9080\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3426 - acc: 0.9463 - auROC: 0.9608 - val_loss: 0.4162 - val_acc: 0.9167 - val_auROC: 0.9080\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3415 - acc: 0.9463 - auROC: 0.9610 - val_loss: 0.4179 - val_acc: 0.9167 - val_auROC: 0.9089\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3403 - acc: 0.9463 - auROC: 0.9610 - val_loss: 0.4222 - val_acc: 0.8958 - val_auROC: 0.9080\n",
      "Epoch 49/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3488 - acc: 0.9375 - auROC: 0.9557\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3402 - acc: 0.9486 - auROC: 0.9607 - val_loss: 0.4256 - val_acc: 0.8750 - val_auROC: 0.9062\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3396 - acc: 0.9486 - auROC: 0.9615 - val_loss: 0.4256 - val_acc: 0.8750 - val_auROC: 0.9062\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3394 - acc: 0.9486 - auROC: 0.9616 - val_loss: 0.4256 - val_acc: 0.8750 - val_auROC: 0.9062\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3393 - acc: 0.9486 - auROC: 0.9617 - val_loss: 0.4254 - val_acc: 0.8750 - val_auROC: 0.9054\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3391 - acc: 0.9486 - auROC: 0.9617 - val_loss: 0.4254 - val_acc: 0.8750 - val_auROC: 0.9054\n",
      "Epoch 54/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3557 - acc: 0.9375 - auROC: 0.9465\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3390 - acc: 0.9486 - auROC: 0.9618 - val_loss: 0.4254 - val_acc: 0.8750 - val_auROC: 0.9054\n",
      "Epoch 00054: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 54/353\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.3856 - acc: 0.9182 - auROC: 0.9450 - val_loss: 0.4075 - val_acc: 0.8958 - val_auROC: 0.9323\n",
      "Epoch 55/353\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3790 - acc: 0.9252 - auROC: 0.9498 - val_loss: 0.4096 - val_acc: 0.8958 - val_auROC: 0.9297\n",
      "Epoch 56/353\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3754 - acc: 0.9299 - auROC: 0.9536 - val_loss: 0.4126 - val_acc: 0.8958 - val_auROC: 0.9280\n",
      "Epoch 57/353\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3723 - acc: 0.9322 - auROC: 0.9555 - val_loss: 0.4152 - val_acc: 0.8958 - val_auROC: 0.9271\n",
      "Epoch 58/353\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3684 - acc: 0.9369 - auROC: 0.9578 - val_loss: 0.4178 - val_acc: 0.8958 - val_auROC: 0.9245\n",
      "Epoch 59/353\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3635 - acc: 0.9439 - auROC: 0.9627 - val_loss: 0.4219 - val_acc: 0.8750 - val_auROC: 0.9245\n",
      "Epoch 60/353\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3604 - acc: 0.9439 - auROC: 0.9637 - val_loss: 0.4252 - val_acc: 0.8750 - val_auROC: 0.9245\n",
      "Epoch 61/353\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3585 - acc: 0.9509 - auROC: 0.9641 - val_loss: 0.4277 - val_acc: 0.8750 - val_auROC: 0.9123\n",
      "Epoch 62/353\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3563 - acc: 0.9509 - auROC: 0.9643 - val_loss: 0.4282 - val_acc: 0.8750 - val_auROC: 0.9123\n",
      "Epoch 63/353\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3553 - acc: 0.9509 - auROC: 0.9647 - val_loss: 0.4279 - val_acc: 0.8750 - val_auROC: 0.9123\n",
      "Epoch 64/353\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3545 - acc: 0.9556 - auROC: 0.9646 - val_loss: 0.4268 - val_acc: 0.8750 - val_auROC: 0.9175\n",
      "Epoch 65/353\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3532 - acc: 0.9556 - auROC: 0.9648 - val_loss: 0.4247 - val_acc: 0.8750 - val_auROC: 0.9253\n",
      "Epoch 66/353\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3519 - acc: 0.9579 - auROC: 0.9651 - val_loss: 0.4221 - val_acc: 0.8750 - val_auROC: 0.9253\n",
      "Epoch 67/353\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3509 - acc: 0.9579 - auROC: 0.9653 - val_loss: 0.4199 - val_acc: 0.8958 - val_auROC: 0.9262\n",
      "Epoch 68/353\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3501 - acc: 0.9579 - auROC: 0.9657 - val_loss: 0.4193 - val_acc: 0.8958 - val_auROC: 0.9262\n",
      "Epoch 69/353\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3542 - acc: 0.9531 - auROC: 0.9621Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3494 - acc: 0.9579 - auROC: 0.9659 - val_loss: 0.4186 - val_acc: 0.8958 - val_auROC: 0.9262\n",
      "Epoch 00069: early stopping\n",
      "    0      1      2         3      ...  18014      18015       18016  18017\n",
      "0     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "1     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "2     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "3     0.0    0.0    0.0 -0.175773  ...    0.0  -0.169117    1.693825    0.0\n",
      "4     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "5     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "6     0.0    0.0    0.0 -0.175773  ...    0.0  -0.220459    0.829100    0.0\n",
      "7     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "8     0.0    0.0    0.0 -0.174587  ...    0.0  -0.106873   -0.226968    0.0\n",
      "9     0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "10    0.0    0.0    0.0 -0.175773  ...    0.0  20.544883  348.613521    0.0\n",
      "11    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "12    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "13    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "14    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "15    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "16    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "17    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "18    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "19    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "20    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "21    0.0    0.0    0.0 -0.175773  ...    0.0  -0.283162   -0.226968    0.0\n",
      "\n",
      "[22 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP   Acc   Sn  ...  FPR   Rc    Pr      F1  ROC-AUC   F-max\n",
      "t                                ...                                         \n",
      "0.00   0   9   0  11  0.55  1.0  ...  1.0  1.0  0.55  0.7097    0.675  0.7692\n",
      "0.01   0   9   0  11  0.55  1.0  ...  1.0  1.0  0.55  0.7097    0.675  0.7692\n",
      "0.02   0   9   0  11  0.55  1.0  ...  1.0  1.0  0.55  0.7097    0.675  0.7692\n",
      "0.03   0   9   0  11  0.55  1.0  ...  1.0  1.0  0.55  0.7097    0.675  0.7692\n",
      "0.04   0   9   0  11  0.55  1.0  ...  1.0  1.0  0.55  0.7097    0.675  0.7692\n",
      "...   ..  ..  ..  ..   ...  ...  ...  ...  ...   ...     ...      ...     ...\n",
      "0.97   9   0  11   0  0.45  0.0  ...  0.0  0.0  0.00     NaN    0.675  0.7692\n",
      "0.98   9   0  11   0  0.45  0.0  ...  0.0  0.0  0.00     NaN    0.675  0.7692\n",
      "0.99   9   0  11   0  0.45  0.0  ...  0.0  0.0  0.00     NaN    0.675  0.7692\n",
      "1.00   9   0  11   0  0.45  0.0  ...  0.0  0.0  0.00     NaN    0.675  0.7692\n",
      "1.01   9   0  11   0  0.45  0.0  ...  0.0  0.0  0.00     NaN    0.675  0.7692\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP   Acc   Sn  ...  FPR   Rc    Pr      F1  ROC-AUC  F-max\n",
      "t                                ...                                        \n",
      "0.00   0  11   0   9  0.45  1.0  ...  1.0  1.0  0.45  0.6207   0.6312    0.7\n",
      "0.01   0  11   0   9  0.45  1.0  ...  1.0  1.0  0.45  0.6207   0.6312    0.7\n",
      "0.02   0  11   0   9  0.45  1.0  ...  1.0  1.0  0.45  0.6207   0.6312    0.7\n",
      "0.03   0  11   0   9  0.45  1.0  ...  1.0  1.0  0.45  0.6207   0.6312    0.7\n",
      "0.04   0  11   0   9  0.45  1.0  ...  1.0  1.0  0.45  0.6207   0.6312    0.7\n",
      "...   ..  ..  ..  ..   ...  ...  ...  ...  ...   ...     ...      ...    ...\n",
      "0.97  11   0   9   0  0.55  0.0  ...  0.0  0.0  0.00     NaN   0.6312    0.7\n",
      "0.98  11   0   9   0  0.55  0.0  ...  0.0  0.0  0.00     NaN   0.6312    0.7\n",
      "0.99  11   0   9   0  0.55  0.0  ...  0.0  0.0  0.00     NaN   0.6312    0.7\n",
      "1.00  11   0   9   0  0.55  0.0  ...  0.0  0.0  0.00     NaN   0.6312    0.7\n",
      "1.01  11   0   9   0  0.55  0.0  ...  0.0  0.0  0.00     NaN   0.6312    0.7\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP   Acc   Sn  ...  FPR   Rc    Pr      F1  ROC-AUC   F-max\n",
      "t                                ...                                         \n",
      "0.00   0  19   0   1  0.05  1.0  ...  1.0  1.0  0.05  0.0952   0.5556  0.1818\n",
      "0.01   0  19   0   1  0.05  1.0  ...  1.0  1.0  0.05  0.0952   0.5556  0.1818\n",
      "0.02   0  19   0   1  0.05  1.0  ...  1.0  1.0  0.05  0.0952   0.5556  0.1818\n",
      "0.03   0  19   0   1  0.05  1.0  ...  1.0  1.0  0.05  0.0952   0.5556  0.1818\n",
      "0.04   0  19   0   1  0.05  1.0  ...  1.0  1.0  0.05  0.0952   0.5556  0.1818\n",
      "...   ..  ..  ..  ..   ...  ...  ...  ...  ...   ...     ...      ...     ...\n",
      "0.97  19   0   1   0  0.95  0.0  ...  0.0  0.0  0.00     NaN   0.5556  0.1818\n",
      "0.98  19   0   1   0  0.95  0.0  ...  0.0  0.0  0.00     NaN   0.5556  0.1818\n",
      "0.99  19   0   1   0  0.95  0.0  ...  0.0  0.0  0.00     NaN   0.5556  0.1818\n",
      "1.00  19   0   1   0  0.95  0.0  ...  0.0  0.0  0.00     NaN   0.5556  0.1818\n",
      "1.01  19   0   1   0  0.95  0.0  ...  0.0  0.0  0.00     NaN   0.5556  0.1818\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP   Acc   Sn  ...  FPR   Rc    Pr      F1  ROC-AUC   F-max\n",
      "t                                ...                                         \n",
      "0.00   0  17   0   3  0.15  1.0  ...  1.0  1.0  0.15  0.2609    0.625  0.4286\n",
      "0.01   0  17   0   3  0.15  1.0  ...  1.0  1.0  0.15  0.2609    0.625  0.4286\n",
      "0.02   0  17   0   3  0.15  1.0  ...  1.0  1.0  0.15  0.2609    0.625  0.4286\n",
      "0.03   0  17   0   3  0.15  1.0  ...  1.0  1.0  0.15  0.2609    0.625  0.4286\n",
      "0.04   0  17   0   3  0.15  1.0  ...  1.0  1.0  0.15  0.2609    0.625  0.4286\n",
      "...   ..  ..  ..  ..   ...  ...  ...  ...  ...   ...     ...      ...     ...\n",
      "0.97  17   0   3   0  0.85  0.0  ...  0.0  0.0  0.00     NaN    0.625  0.4286\n",
      "0.98  17   0   3   0  0.85  0.0  ...  0.0  0.0  0.00     NaN    0.625  0.4286\n",
      "0.99  17   0   3   0  0.85  0.0  ...  0.0  0.0  0.00     NaN    0.625  0.4286\n",
      "1.00  17   0   3   0  0.85  0.0  ...  0.0  0.0  0.00     NaN    0.625  0.4286\n",
      "1.01  17   0   3   0  0.85  0.0  ...  0.0  0.0  0.00     NaN    0.625  0.4286\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  15   0   5  0.2500  1.0  ...  1.0000  1.0  0.2500  0.4000   0.4196  0.4211\n",
      "0.01   0  15   0   5  0.2500  1.0  ...  1.0000  1.0  0.2500  0.4000   0.4196  0.4211\n",
      "0.02   0  14   0   5  0.2632  1.0  ...  1.0000  1.0  0.2632  0.4167   0.4196  0.4211\n",
      "0.03   0  14   0   5  0.2632  1.0  ...  1.0000  1.0  0.2632  0.4167   0.4196  0.4211\n",
      "0.04   3  11   0   4  0.3889  1.0  ...  0.7857  1.0  0.2667  0.4211   0.4196  0.4211\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.4196  0.4211\n",
      "0.98  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.4196  0.4211\n",
      "0.99  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.4196  0.4211\n",
      "1.00  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.4196  0.4211\n",
      "1.01  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.4196  0.4211\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr      F1  ROC-AUC  F-max\n",
      "t                                                                              \n",
      "0.00   0  18   0   2  0.1  1.0  0.0  1.0  1.0  1.0  0.1  0.1818   0.7647    0.4\n",
      "0.01   0  18   0   2  0.1  1.0  0.0  1.0  1.0  1.0  0.1  0.1818   0.7647    0.4\n",
      "0.02   0  18   0   2  0.1  1.0  0.0  1.0  1.0  1.0  0.1  0.1818   0.7647    0.4\n",
      "0.03   0  18   0   2  0.1  1.0  0.0  1.0  1.0  1.0  0.1  0.1818   0.7647    0.4\n",
      "0.04   0  18   0   2  0.1  1.0  0.0  1.0  1.0  1.0  0.1  0.1818   0.7647    0.4\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...     ...      ...    ...\n",
      "0.97  18   0   2   0  0.9  0.0  1.0  0.0  0.0  0.0  0.0     NaN   0.7647    0.4\n",
      "0.98  18   0   2   0  0.9  0.0  1.0  0.0  0.0  0.0  0.0     NaN   0.7647    0.4\n",
      "0.99  18   0   2   0  0.9  0.0  1.0  0.0  0.0  0.0  0.0     NaN   0.7647    0.4\n",
      "1.00  18   0   2   0  0.9  0.0  1.0  0.0  0.0  0.0  0.0     NaN   0.7647    0.4\n",
      "1.01  18   0   2   0  0.9  0.0  1.0  0.0  0.0  0.0  0.0     NaN   0.7647    0.4\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  15   0   5  0.2500  1.0  ...  1.0000  1.0  0.2500  0.4000   0.8304   0.75\n",
      "0.01   0  15   0   5  0.2500  1.0  ...  1.0000  1.0  0.2500  0.4000   0.8304   0.75\n",
      "0.02   0  15   0   5  0.2500  1.0  ...  1.0000  1.0  0.2500  0.4000   0.8304   0.75\n",
      "0.03   0  14   0   4  0.2222  1.0  ...  1.0000  1.0  0.2222  0.3636   0.8304   0.75\n",
      "0.04   2  12   0   4  0.3333  1.0  ...  0.8571  1.0  0.2500  0.4000   0.8304   0.75\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.8304   0.75\n",
      "0.98  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.8304   0.75\n",
      "0.99  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.8304   0.75\n",
      "1.00  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.8304   0.75\n",
      "1.01  15   0   5   0  0.7500  0.0  ...  0.0000  0.0  0.0000     NaN   0.8304   0.75\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                  ...                                      \n",
      "0.00   0  20   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  20   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  20   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  20   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   3  16   0   0  0.1579  0.0  ...  0.8421  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...  ...  ..      ...    ...\n",
      "0.97  20   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  20   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  20   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  20   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  20   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 241\n",
      "Total correct samples: 241?241\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.006060  0.034699\n",
      "4      0.003636  0.020820\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000028\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000012  0.000055\n",
      "18016  0.000003  0.000045\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.7958 - acc: 0.4954 - auROC: 0.6508 - val_loss: 0.8254 - val_acc: 0.5000 - val_auROC: 0.3744\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.7082 - acc: 0.5231 - auROC: 0.5536 - val_loss: 0.7564 - val_acc: 0.4400 - val_auROC: 0.3096\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6832 - acc: 0.5509 - auROC: 0.5572 - val_loss: 0.7357 - val_acc: 0.4000 - val_auROC: 0.3336\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6620 - acc: 0.6088 - auROC: 0.6549 - val_loss: 0.6800 - val_acc: 0.4200 - val_auROC: 0.5864\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6328 - acc: 0.6111 - auROC: 0.7816 - val_loss: 0.6197 - val_acc: 0.5000 - val_auROC: 0.7784\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6243 - acc: 0.6343 - auROC: 0.7973 - val_loss: 0.6074 - val_acc: 0.5200 - val_auROC: 0.7808\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6095 - acc: 0.6852 - auROC: 0.8115 - val_loss: 0.6140 - val_acc: 0.6200 - val_auROC: 0.7504\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5944 - acc: 0.7060 - auROC: 0.8194 - val_loss: 0.6278 - val_acc: 0.6000 - val_auROC: 0.7256\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5870 - acc: 0.7153 - auROC: 0.8443 - val_loss: 0.6163 - val_acc: 0.5800 - val_auROC: 0.7424\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5757 - acc: 0.7130 - auROC: 0.8563 - val_loss: 0.6031 - val_acc: 0.6000 - val_auROC: 0.7552\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5673 - acc: 0.7593 - auROC: 0.8704 - val_loss: 0.5878 - val_acc: 0.6600 - val_auROC: 0.7816\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5599 - acc: 0.7477 - auROC: 0.8719 - val_loss: 0.5969 - val_acc: 0.7200 - val_auROC: 0.7976\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5512 - acc: 0.7384 - auROC: 0.8763 - val_loss: 0.6003 - val_acc: 0.7400 - val_auROC: 0.8032\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5407 - acc: 0.7407 - auROC: 0.8859 - val_loss: 0.5889 - val_acc: 0.7400 - val_auROC: 0.8112\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5323 - acc: 0.7384 - auROC: 0.8907 - val_loss: 0.5807 - val_acc: 0.7400 - val_auROC: 0.8144\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5229 - acc: 0.7384 - auROC: 0.8933 - val_loss: 0.5716 - val_acc: 0.7400 - val_auROC: 0.8200\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.5141 - acc: 0.7292 - auROC: 0.8948 - val_loss: 0.5607 - val_acc: 0.7400 - val_auROC: 0.8320\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5062 - acc: 0.7361 - auROC: 0.8920 - val_loss: 0.5499 - val_acc: 0.7600 - val_auROC: 0.8408\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4976 - acc: 0.7407 - auROC: 0.8956 - val_loss: 0.5586 - val_acc: 0.7600 - val_auROC: 0.8328\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4876 - acc: 0.7292 - auROC: 0.8996 - val_loss: 0.5620 - val_acc: 0.7600 - val_auROC: 0.8264\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4775 - acc: 0.7407 - auROC: 0.9114 - val_loss: 0.5378 - val_acc: 0.7600 - val_auROC: 0.8336\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4695 - acc: 0.7731 - auROC: 0.9151 - val_loss: 0.5499 - val_acc: 0.8000 - val_auROC: 0.8320\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4613 - acc: 0.8727 - auROC: 0.9171 - val_loss: 0.5377 - val_acc: 0.8000 - val_auROC: 0.8360\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4522 - acc: 0.8819 - auROC: 0.9201 - val_loss: 0.5386 - val_acc: 0.8000 - val_auROC: 0.8456\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4491 - acc: 0.8843 - auROC: 0.9171 - val_loss: 0.5433 - val_acc: 0.8000 - val_auROC: 0.8304\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.4375 - acc: 0.8912 - auROC: 0.9205 - val_loss: 0.5321 - val_acc: 0.8000 - val_auROC: 0.8432\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4356 - acc: 0.8866 - auROC: 0.9216 - val_loss: 0.5349 - val_acc: 0.8000 - val_auROC: 0.8320\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4247 - acc: 0.8935 - auROC: 0.9281 - val_loss: 0.5615 - val_acc: 0.7800 - val_auROC: 0.8200\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4247 - acc: 0.8935 - auROC: 0.9291 - val_loss: 0.5381 - val_acc: 0.8000 - val_auROC: 0.8352\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4133 - acc: 0.8935 - auROC: 0.9308 - val_loss: 0.5169 - val_acc: 0.8000 - val_auROC: 0.8416\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4090 - acc: 0.8935 - auROC: 0.9315 - val_loss: 0.5516 - val_acc: 0.8000 - val_auROC: 0.8080\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3963 - acc: 0.9120 - auROC: 0.9387 - val_loss: 0.5876 - val_acc: 0.7600 - val_auROC: 0.7832\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3952 - acc: 0.9074 - auROC: 0.9381 - val_loss: 0.5483 - val_acc: 0.8000 - val_auROC: 0.8088\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3812 - acc: 0.9097 - auROC: 0.9438 - val_loss: 0.5417 - val_acc: 0.8000 - val_auROC: 0.8160\n",
      "Epoch 35/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3462 - acc: 0.9453 - auROC: 0.9685\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3747 - acc: 0.9051 - auROC: 0.9455 - val_loss: 0.5495 - val_acc: 0.8000 - val_auROC: 0.7936\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3692 - acc: 0.9167 - auROC: 0.9487 - val_loss: 0.5500 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3685 - acc: 0.9167 - auROC: 0.9506 - val_loss: 0.5505 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3675 - acc: 0.9167 - auROC: 0.9507 - val_loss: 0.5507 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3665 - acc: 0.9167 - auROC: 0.9501 - val_loss: 0.5513 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 40/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3383 - acc: 0.9375 - auROC: 0.9723\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3656 - acc: 0.9167 - auROC: 0.9504 - val_loss: 0.5525 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3649 - acc: 0.9167 - auROC: 0.9506 - val_loss: 0.5526 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3649 - acc: 0.9167 - auROC: 0.9504 - val_loss: 0.5527 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3648 - acc: 0.9167 - auROC: 0.9504 - val_loss: 0.5528 - val_acc: 0.8000 - val_auROC: 0.7968\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3647 - acc: 0.9167 - auROC: 0.9504 - val_loss: 0.5529 - val_acc: 0.8000 - val_auROC: 0.7976\n",
      "Epoch 45/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3900 - acc: 0.8906 - auROC: 0.9299\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3646 - acc: 0.9167 - auROC: 0.9505 - val_loss: 0.5530 - val_acc: 0.8000 - val_auROC: 0.7976\n",
      "Epoch 00045: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 45/344\n",
      "4/4 [==============================] - 1s 195ms/step - loss: 0.4127 - acc: 0.8958 - auROC: 0.9313 - val_loss: 0.5254 - val_acc: 0.8000 - val_auROC: 0.8432\n",
      "Epoch 46/344\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.4031 - acc: 0.9028 - auROC: 0.9376 - val_loss: 0.5321 - val_acc: 0.8000 - val_auROC: 0.8304\n",
      "Epoch 47/344\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3988 - acc: 0.9097 - auROC: 0.9392 - val_loss: 0.5463 - val_acc: 0.8000 - val_auROC: 0.8104\n",
      "Epoch 48/344\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3963 - acc: 0.9120 - auROC: 0.9412 - val_loss: 0.5560 - val_acc: 0.8000 - val_auROC: 0.8088\n",
      "Epoch 49/344\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3938 - acc: 0.9144 - auROC: 0.9422 - val_loss: 0.5566 - val_acc: 0.8000 - val_auROC: 0.8112\n",
      "Epoch 50/344\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3912 - acc: 0.9144 - auROC: 0.9430 - val_loss: 0.5551 - val_acc: 0.8000 - val_auROC: 0.8136\n",
      "Epoch 51/344\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.3895 - acc: 0.9144 - auROC: 0.9458 - val_loss: 0.5540 - val_acc: 0.8000 - val_auROC: 0.8160\n",
      "Epoch 52/344\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3882 - acc: 0.9144 - auROC: 0.9466 - val_loss: 0.5537 - val_acc: 0.8000 - val_auROC: 0.8160\n",
      "Epoch 53/344\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.3870 - acc: 0.9167 - auROC: 0.9468 - val_loss: 0.5535 - val_acc: 0.8000 - val_auROC: 0.8136\n",
      "Epoch 54/344\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3859 - acc: 0.9190 - auROC: 0.9471 - val_loss: 0.5538 - val_acc: 0.8000 - val_auROC: 0.8128\n",
      "Epoch 55/344\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3847 - acc: 0.9190 - auROC: 0.9467 - val_loss: 0.5543 - val_acc: 0.8000 - val_auROC: 0.8128\n",
      "Epoch 56/344\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3837 - acc: 0.9190 - auROC: 0.9470 - val_loss: 0.5545 - val_acc: 0.8000 - val_auROC: 0.8120\n",
      "Epoch 57/344\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3827 - acc: 0.9190 - auROC: 0.9472 - val_loss: 0.5545 - val_acc: 0.8000 - val_auROC: 0.8128\n",
      "Epoch 58/344\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3820 - acc: 0.9190 - auROC: 0.9471 - val_loss: 0.5544 - val_acc: 0.8000 - val_auROC: 0.8128\n",
      "Epoch 59/344\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3812 - acc: 0.9190 - auROC: 0.9474 - val_loss: 0.5544 - val_acc: 0.8000 - val_auROC: 0.8128\n",
      "Epoch 60/344\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3814 - acc: 0.9219 - auROC: 0.9483Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.3805 - acc: 0.9190 - auROC: 0.9488 - val_loss: 0.5548 - val_acc: 0.8000 - val_auROC: 0.8096\n",
      "Epoch 00060: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "1     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "2     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "3     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "4     0.0    0.0    0.0 -0.174644  ...    0.0 -0.096915 -0.074509    0.0\n",
      "5     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "6     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "7     0.0    0.0    0.0 -0.174262  ...    0.0  1.345111 -0.074509    0.0\n",
      "8     0.0    0.0    0.0 -0.174428  ...    0.0 -0.188032 -0.074509    0.0\n",
      "9     0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "10    0.0    0.0    0.0 -0.174445  ...    0.0 -0.190100 -0.074509    0.0\n",
      "11    0.0    0.0    0.0 -0.174644  ...    0.0 -0.193311 -0.074509    0.0\n",
      "12    0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "13    0.0    0.0    0.0 -0.174644  ...    0.0 -0.142324 -0.029482    0.0\n",
      "14    0.0    0.0    0.0 -0.174644  ...    0.0 -0.182967 -0.034716    0.0\n",
      "15    0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "16    0.0    0.0    0.0 -0.174644  ...    0.0 -0.215145 -0.074509    0.0\n",
      "17    0.0    0.0    0.0 -0.174644  ...    0.0 -0.195568 -0.074509    0.0\n",
      "18    0.0    0.0    0.0 -0.174644  ...    0.0 -0.177526 -0.027987    0.0\n",
      "\n",
      "[19 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:China\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr    F1  ROC-AUC  F-max\n",
      "t                                  ...                                        \n",
      "0.00   0   9   0   8  0.4706  1.0  ...  1.0  1.0  0.4706  0.64   0.9196  0.875\n",
      "0.01   0   9   0   8  0.4706  1.0  ...  1.0  1.0  0.4706  0.64   0.9196  0.875\n",
      "0.02   0   9   0   8  0.4706  1.0  ...  1.0  1.0  0.4706  0.64   0.9196  0.875\n",
      "0.03   0   9   0   8  0.4706  1.0  ...  1.0  1.0  0.4706  0.64   0.9196  0.875\n",
      "0.04   0   9   0   8  0.4706  1.0  ...  1.0  1.0  0.4706  0.64   0.9196  0.875\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...   ...      ...    ...\n",
      "0.97   9   0   8   0  0.5294  0.0  ...  0.0  0.0  0.0000   NaN   0.9196  0.875\n",
      "0.98   9   0   8   0  0.5294  0.0  ...  0.0  0.0  0.0000   NaN   0.9196  0.875\n",
      "0.99   9   0   8   0  0.5294  0.0  ...  0.0  0.0  0.0000   NaN   0.9196  0.875\n",
      "1.00   9   0   8   0  0.5294  0.0  ...  0.0  0.0  0.0000   NaN   0.9196  0.875\n",
      "1.01   9   0   8   0  0.5294  0.0  ...  0.0  0.0  0.0000   NaN   0.9196  0.875\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0   8   0   9  0.5294  1.0  ...  1.0  1.0  0.5294  0.6923    0.875  0.8421\n",
      "0.01   0   8   0   9  0.5294  1.0  ...  1.0  1.0  0.5294  0.6923    0.875  0.8421\n",
      "0.02   0   8   0   9  0.5294  1.0  ...  1.0  1.0  0.5294  0.6923    0.875  0.8421\n",
      "0.03   0   8   0   9  0.5294  1.0  ...  1.0  1.0  0.5294  0.6923    0.875  0.8421\n",
      "0.04   0   8   0   9  0.5294  1.0  ...  1.0  1.0  0.5294  0.6923    0.875  0.8421\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97   8   0   9   0  0.4706  0.0  ...  0.0  0.0  0.0000     NaN    0.875  0.8421\n",
      "0.98   8   0   9   0  0.4706  0.0  ...  0.0  0.0  0.0000     NaN    0.875  0.8421\n",
      "0.99   8   0   9   0  0.4706  0.0  ...  0.0  0.0  0.0000     NaN    0.875  0.8421\n",
      "1.00   8   0   9   0  0.4706  0.0  ...  0.0  0.0  0.0000     NaN    0.875  0.8421\n",
      "1.01   8   0   9   0  0.4706  0.0  ...  0.0  0.0  0.0000     NaN    0.875  0.8421\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T1\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111      0.0  0.1111\n",
      "0.01   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111      0.0  0.1111\n",
      "0.02   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111      0.0  0.1111\n",
      "0.03   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111      0.0  0.1111\n",
      "0.04   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111      0.0  0.1111\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN      0.0  0.1111\n",
      "0.98  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN      0.0  0.1111\n",
      "0.99  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN      0.0  0.1111\n",
      "1.00  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN      0.0  0.1111\n",
      "1.01  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN      0.0  0.1111\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T5\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr   F1  ROC-AUC   F-max\n",
      "t                                  ...                                        \n",
      "0.00   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.6923  0.4444\n",
      "0.01   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.6923  0.4444\n",
      "0.02   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.6923  0.4444\n",
      "0.03   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.6923  0.4444\n",
      "0.04   0  14   0   3  0.1765  1.0  ...  1.0  1.0  0.1765  0.3   0.6923  0.4444\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...  ...      ...     ...\n",
      "0.97  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.6923  0.4444\n",
      "0.98  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.6923  0.4444\n",
      "0.99  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.6923  0.4444\n",
      "1.00  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.6923  0.4444\n",
      "1.01  14   0   3   0  0.8235  0.0  ...  0.0  0.0  0.0000  NaN   0.6923  0.4444\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:China:T6\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  15   0   2  0.1176  1.0  ...  1.0000  1.0  0.1176  0.2105   0.3214  0.2667\n",
      "0.01   0  15   0   2  0.1176  1.0  ...  1.0000  1.0  0.1176  0.2105   0.3214  0.2667\n",
      "0.02   0  15   0   2  0.1176  1.0  ...  1.0000  1.0  0.1176  0.2105   0.3214  0.2667\n",
      "0.03   0  14   0   2  0.1250  1.0  ...  1.0000  1.0  0.1250  0.2222   0.3214  0.2667\n",
      "0.04   1  13   0   2  0.1875  1.0  ...  0.9286  1.0  0.1333  0.2353   0.3214  0.2667\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  15   0   2   0  0.8824  0.0  ...  0.0000  0.0  0.0000     NaN   0.3214  0.2667\n",
      "0.98  15   0   2   0  0.8824  0.0  ...  0.0000  0.0  0.0000     NaN   0.3214  0.2667\n",
      "0.99  15   0   2   0  0.8824  0.0  ...  0.0000  0.0  0.0000     NaN   0.3214  0.2667\n",
      "1.00  15   0   2   0  0.8824  0.0  ...  0.0000  0.0  0.0000     NaN   0.3214  0.2667\n",
      "1.01  15   0   2   0  0.8824  0.0  ...  0.0000  0.0  0.0000     NaN   0.3214  0.2667\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T2\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.2333  0.1333\n",
      "0.01   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.2333  0.1333\n",
      "0.02   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.2333  0.1333\n",
      "0.03   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.2333  0.1333\n",
      "0.04   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.2333  0.1333\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.2333  0.1333\n",
      "0.98  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.2333  0.1333\n",
      "0.99  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.2333  0.1333\n",
      "1.00  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.2333  0.1333\n",
      "1.01  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.2333  0.1333\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T3\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.5455  0.5263\n",
      "0.01   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.5455  0.5263\n",
      "0.02   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.5455  0.5263\n",
      "0.03   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.5455  0.5263\n",
      "0.04   0  12   0   5  0.2941  1.0  ...  1.0  1.0  0.2941  0.4545   0.5455  0.5263\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.5455  0.5263\n",
      "0.98  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.5455  0.5263\n",
      "0.99  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.5455  0.5263\n",
      "1.00  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.5455  0.5263\n",
      "1.01  12   0   5   0  0.7059  0.0  ...  0.0  0.0  0.0000     NaN   0.5455  0.5263\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:Trinidad and Tobago:T4\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.5333  0.2222\n",
      "0.01   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.5333  0.2222\n",
      "0.02   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.5333  0.2222\n",
      "0.03   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.5333  0.2222\n",
      "0.04   0  16   0   1  0.0588  1.0  ...  1.0  1.0  0.0588  0.1111   0.5333  0.2222\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.5333  0.2222\n",
      "0.98  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.5333  0.2222\n",
      "0.99  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.5333  0.2222\n",
      "1.00  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.5333  0.2222\n",
      "1.01  16   0   1   0  0.9412  0.0  ...  0.0  0.0  0.0000     NaN   0.5333  0.2222\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 260\n",
      "Total correct samples: 260?260\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.005617  0.033445\n",
      "4      0.003370  0.020067\n",
      "...         ...       ...\n",
      "18013  0.000008  0.000027\n",
      "18014  0.000000  0.000000\n",
      "18015  0.000011  0.000054\n",
      "18016  0.000003  0.000043\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 0.8194 - acc: 0.5043 - auROC: 0.6061 - val_loss: 0.7352 - val_acc: 0.5385 - val_auROC: 0.6036\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.7299 - acc: 0.5171 - auROC: 0.5575 - val_loss: 0.6924 - val_acc: 0.5769 - val_auROC: 0.5518\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6907 - acc: 0.5278 - auROC: 0.5785 - val_loss: 0.6569 - val_acc: 0.6154 - val_auROC: 0.7212\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6612 - acc: 0.5278 - auROC: 0.6985 - val_loss: 0.6381 - val_acc: 0.5962 - val_auROC: 0.7737\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6411 - acc: 0.5705 - auROC: 0.7438 - val_loss: 0.6176 - val_acc: 0.6346 - val_auROC: 0.7929\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6268 - acc: 0.6068 - auROC: 0.7694 - val_loss: 0.6011 - val_acc: 0.6346 - val_auROC: 0.8536\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.6069 - acc: 0.6774 - auROC: 0.7958 - val_loss: 0.5875 - val_acc: 0.7308 - val_auROC: 0.8698\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5912 - acc: 0.6944 - auROC: 0.8251 - val_loss: 0.5899 - val_acc: 0.7115 - val_auROC: 0.8550\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5772 - acc: 0.7158 - auROC: 0.8418 - val_loss: 0.5900 - val_acc: 0.7115 - val_auROC: 0.8432\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5639 - acc: 0.7179 - auROC: 0.8678 - val_loss: 0.5852 - val_acc: 0.7115 - val_auROC: 0.8373\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5529 - acc: 0.7265 - auROC: 0.8828 - val_loss: 0.5834 - val_acc: 0.7500 - val_auROC: 0.8447\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5420 - acc: 0.7564 - auROC: 0.8888 - val_loss: 0.5799 - val_acc: 0.7500 - val_auROC: 0.8558\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.5298 - acc: 0.7692 - auROC: 0.8988 - val_loss: 0.5752 - val_acc: 0.7115 - val_auROC: 0.8469\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5218 - acc: 0.7628 - auROC: 0.9005 - val_loss: 0.5700 - val_acc: 0.6923 - val_auROC: 0.8402\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.5139 - acc: 0.7778 - auROC: 0.9026 - val_loss: 0.5639 - val_acc: 0.6923 - val_auROC: 0.8454\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5053 - acc: 0.7799 - auROC: 0.9071 - val_loss: 0.5610 - val_acc: 0.6731 - val_auROC: 0.8499\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5019 - acc: 0.7671 - auROC: 0.9064 - val_loss: 0.5514 - val_acc: 0.6731 - val_auROC: 0.8550\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.4912 - acc: 0.7778 - auROC: 0.9122 - val_loss: 0.5473 - val_acc: 0.6538 - val_auROC: 0.8513\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4806 - acc: 0.7735 - auROC: 0.9206 - val_loss: 0.5425 - val_acc: 0.6538 - val_auROC: 0.8536\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4746 - acc: 0.7821 - auROC: 0.9204 - val_loss: 0.5373 - val_acc: 0.6538 - val_auROC: 0.8506\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4662 - acc: 0.7885 - auROC: 0.9226 - val_loss: 0.5339 - val_acc: 0.8077 - val_auROC: 0.8543\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.4591 - acc: 0.8846 - auROC: 0.9240 - val_loss: 0.5282 - val_acc: 0.8077 - val_auROC: 0.8580\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4520 - acc: 0.8932 - auROC: 0.9277 - val_loss: 0.5233 - val_acc: 0.8077 - val_auROC: 0.8595\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4455 - acc: 0.8932 - auROC: 0.9275 - val_loss: 0.5186 - val_acc: 0.8077 - val_auROC: 0.8528\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4405 - acc: 0.8932 - auROC: 0.9275 - val_loss: 0.5104 - val_acc: 0.8077 - val_auROC: 0.8580\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4325 - acc: 0.9017 - auROC: 0.9339 - val_loss: 0.5058 - val_acc: 0.8077 - val_auROC: 0.8595\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4255 - acc: 0.9038 - auROC: 0.9340 - val_loss: 0.5073 - val_acc: 0.7885 - val_auROC: 0.8484\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4159 - acc: 0.9081 - auROC: 0.9358 - val_loss: 0.5056 - val_acc: 0.8077 - val_auROC: 0.8476\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4097 - acc: 0.8974 - auROC: 0.9348 - val_loss: 0.5040 - val_acc: 0.8077 - val_auROC: 0.8550\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.4021 - acc: 0.9038 - auROC: 0.9385 - val_loss: 0.5021 - val_acc: 0.7885 - val_auROC: 0.8572\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3966 - acc: 0.9124 - auROC: 0.9387 - val_loss: 0.4908 - val_acc: 0.7885 - val_auROC: 0.8743\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3899 - acc: 0.9145 - auROC: 0.9406 - val_loss: 0.4903 - val_acc: 0.7885 - val_auROC: 0.8587\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3838 - acc: 0.9145 - auROC: 0.9402 - val_loss: 0.4972 - val_acc: 0.7885 - val_auROC: 0.8536\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3779 - acc: 0.9145 - auROC: 0.9423 - val_loss: 0.4967 - val_acc: 0.7885 - val_auROC: 0.8462\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3722 - acc: 0.9167 - auROC: 0.9436 - val_loss: 0.4958 - val_acc: 0.7885 - val_auROC: 0.8447\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3651 - acc: 0.9145 - auROC: 0.9463 - val_loss: 0.4996 - val_acc: 0.7885 - val_auROC: 0.8425\n",
      "Epoch 37/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3265 - acc: 0.9453 - auROC: 0.9784\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3597 - acc: 0.9145 - auROC: 0.9490 - val_loss: 0.5135 - val_acc: 0.7885 - val_auROC: 0.8395\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3545 - acc: 0.9209 - auROC: 0.9509 - val_loss: 0.5125 - val_acc: 0.7885 - val_auROC: 0.8402\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3535 - acc: 0.9209 - auROC: 0.9494 - val_loss: 0.5102 - val_acc: 0.7692 - val_auROC: 0.8395\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3530 - acc: 0.9188 - auROC: 0.9500 - val_loss: 0.5074 - val_acc: 0.7885 - val_auROC: 0.8425\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3523 - acc: 0.9188 - auROC: 0.9504 - val_loss: 0.5071 - val_acc: 0.7885 - val_auROC: 0.8417\n",
      "Epoch 42/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3349 - acc: 0.9297 - auROC: 0.9707\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3516 - acc: 0.9188 - auROC: 0.9505 - val_loss: 0.5090 - val_acc: 0.7692 - val_auROC: 0.8410\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3510 - acc: 0.9188 - auROC: 0.9510 - val_loss: 0.5093 - val_acc: 0.7692 - val_auROC: 0.8410\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3509 - acc: 0.9209 - auROC: 0.9510 - val_loss: 0.5096 - val_acc: 0.7692 - val_auROC: 0.8410\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3508 - acc: 0.9209 - auROC: 0.9511 - val_loss: 0.5101 - val_acc: 0.7692 - val_auROC: 0.8410\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3507 - acc: 0.9209 - auROC: 0.9506 - val_loss: 0.5108 - val_acc: 0.7692 - val_auROC: 0.8410\n",
      "Epoch 47/300\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3677 - acc: 0.8984 - auROC: 0.9390\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3506 - acc: 0.9209 - auROC: 0.9507 - val_loss: 0.5117 - val_acc: 0.7692 - val_auROC: 0.8417\n",
      "Epoch 00047: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 4)                 8380      \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 18,984,680\n",
      "Trainable params: 8,424\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 47/346\n",
      "4/4 [==============================] - 1s 198ms/step - loss: 0.3852 - acc: 0.9145 - auROC: 0.9414 - val_loss: 0.4913 - val_acc: 0.7885 - val_auROC: 0.8550\n",
      "Epoch 48/346\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.3822 - acc: 0.9145 - auROC: 0.9425 - val_loss: 0.4899 - val_acc: 0.7885 - val_auROC: 0.8609\n",
      "Epoch 49/346\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3801 - acc: 0.9145 - auROC: 0.9430 - val_loss: 0.4882 - val_acc: 0.8077 - val_auROC: 0.8595\n",
      "Epoch 50/346\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.3786 - acc: 0.9145 - auROC: 0.9433 - val_loss: 0.4861 - val_acc: 0.8269 - val_auROC: 0.8617\n",
      "Epoch 51/346\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.3773 - acc: 0.9145 - auROC: 0.9441 - val_loss: 0.4857 - val_acc: 0.8269 - val_auROC: 0.8639\n",
      "Epoch 52/346\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3760 - acc: 0.9167 - auROC: 0.9445 - val_loss: 0.4862 - val_acc: 0.8077 - val_auROC: 0.8617\n",
      "Epoch 53/346\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3748 - acc: 0.9167 - auROC: 0.9444 - val_loss: 0.4867 - val_acc: 0.8269 - val_auROC: 0.8580\n",
      "Epoch 54/346\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3735 - acc: 0.9167 - auROC: 0.9451 - val_loss: 0.4876 - val_acc: 0.8462 - val_auROC: 0.8558\n",
      "Epoch 55/346\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3724 - acc: 0.9167 - auROC: 0.9460 - val_loss: 0.4870 - val_acc: 0.8269 - val_auROC: 0.8558\n",
      "Epoch 56/346\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3714 - acc: 0.9167 - auROC: 0.9476 - val_loss: 0.4868 - val_acc: 0.8269 - val_auROC: 0.8536\n",
      "Epoch 57/346\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3704 - acc: 0.9167 - auROC: 0.9492 - val_loss: 0.4867 - val_acc: 0.8462 - val_auROC: 0.8536\n",
      "Epoch 58/346\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3693 - acc: 0.9167 - auROC: 0.9499 - val_loss: 0.4865 - val_acc: 0.8462 - val_auROC: 0.8521\n",
      "Epoch 59/346\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.3683 - acc: 0.9188 - auROC: 0.9505 - val_loss: 0.4866 - val_acc: 0.8462 - val_auROC: 0.8513\n",
      "Epoch 60/346\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3673 - acc: 0.9188 - auROC: 0.9511 - val_loss: 0.4869 - val_acc: 0.8462 - val_auROC: 0.8506\n",
      "Epoch 61/346\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3663 - acc: 0.9209 - auROC: 0.9521 - val_loss: 0.4881 - val_acc: 0.8462 - val_auROC: 0.8506\n",
      "Epoch 62/346\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3651 - acc: 0.9209 - auROC: 0.9526 - val_loss: 0.4895 - val_acc: 0.8462 - val_auROC: 0.8499\n",
      "Epoch 63/346\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3643 - acc: 0.9209 - auROC: 0.9542 - val_loss: 0.4910 - val_acc: 0.8462 - val_auROC: 0.8491\n",
      "Epoch 64/346\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3627 - acc: 0.9209 - auROC: 0.9563 - val_loss: 0.4916 - val_acc: 0.8462 - val_auROC: 0.8476\n",
      "Epoch 65/346\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.3603 - acc: 0.9295 - auROC: 0.9582 - val_loss: 0.4922 - val_acc: 0.8462 - val_auROC: 0.8469\n",
      "Epoch 66/346\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.3588 - acc: 0.9349 - auROC: 0.9612Restoring model weights from the end of the best epoch.\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.3590 - acc: 0.9295 - auROC: 0.9590 - val_loss: 0.4930 - val_acc: 0.8462 - val_auROC: 0.8476\n",
      "Epoch 00066: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.167554  ...    0.0 -0.213678 -0.072247    0.0\n",
      "1     0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "2     0.0    0.0    0.0 -0.167958  ...    0.0 -0.137187  0.022820    0.0\n",
      "3     0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "4     0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "5     0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "6     0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "7     0.0    0.0    0.0 -0.167958  ...    0.0 -0.033750 -0.040301    0.0\n",
      "8     0.0    0.0    0.0 -0.167285  ...    0.0 -0.129569 -0.072247    0.0\n",
      "9     0.0    0.0    0.0 -0.167958  ...    0.0 -0.027666 -0.072247    0.0\n",
      "10    0.0    0.0    0.0 -0.167404  ...    0.0 -0.179087 -0.029255    0.0\n",
      "11    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "12    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "13    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "14    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "15    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "16    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "17    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "18    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "19    0.0    0.0    0.0 -0.167958  ...    0.0 -0.213678 -0.072247    0.0\n",
      "\n",
      "[20 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-25 14:08:00.228438: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:08:00.236969: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:08:00.239197: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559da78ec840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:08:00.239231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:08:00.356141: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:00.367361: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:00.468721: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:00.548673: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:00.637717: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:08:20.328692: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1223.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 646.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 74.85it/s]\n",
      "2020-12-25 14:08:41.454245: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:08:41.466303: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:08:41.468327: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56261d944250 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:08:41.468359: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:08:41.572286: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:41.584011: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:41.662491: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:41.741812: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:08:41.823919: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:09:19.435087: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1230.00it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 634.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 67.15it/s]\n",
      "2020-12-25 14:09:42.372732: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:09:42.381730: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:09:42.383705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559818c99f90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:09:42.383744: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:09:42.502272: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:09:42.513348: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:09:42.599958: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:09:42.678065: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:09:42.768412: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:10:08.453608: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 909.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 482.02it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.98it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 65.14it/s]\n",
      "2020-12-25 14:10:30.671010: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:10:30.679896: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:10:30.682172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5644ec3575c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:10:30.682195: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:10:30.788025: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:10:30.799431: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:10:30.807212: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:10:30.939029: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:10:31.024930: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:11:01.216870: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1182.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 606.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.45it/s]\n",
      "2020-12-25 14:11:22.444208: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:11:22.453437: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:11:22.455486: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e2b95b17c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:11:22.455515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:11:22.721757: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:11:22.736137: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:11:22.742736: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:11:22.819979: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:11:22.921888: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:11:45.132428: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 791.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 404.74it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 63.09it/s]\n",
      "2020-12-25 14:12:08.065504: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:12:08.075210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:12:08.077336: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564796a10910 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:12:08.077376: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:12:08.198561: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:08.209592: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:08.217287: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:08.335110: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:08.423929: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:12:31.560362: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1217.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 667.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 44.06it/s]\n",
      "2020-12-25 14:12:56.649439: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:12:56.657625: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:12:56.659654: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564b2b64f980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:12:56.659685: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:12:56.761485: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:56.774028: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:56.781682: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:56.890099: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:12:56.977840: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:13:19.688981: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1216.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 582.74it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 70.10it/s]\n",
      "2020-12-25 14:13:40.728321: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:13:40.737939: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:13:40.739967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e65f98fe70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:13:40.739992: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:13:40.846894: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:13:40.858281: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:13:40.866625: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:13:40.996759: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:13:41.079650: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:14:00.586067: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1234.71it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 638.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 66.70it/s]\n",
      "2020-12-25 14:14:23.221591: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:14:23.229878: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:14:23.231974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5622cca00340 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:14:23.232004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:14:23.341150: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:14:23.355373: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:14:23.362805: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:14:23.490149: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:14:23.578895: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:14:42.006112: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 2/2 [00:00<00:00, 951.95it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 519.90it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 60.35it/s]\n",
      "2020-12-25 14:15:04.855304: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-12-25 14:15:04.864416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-12-25 14:15:04.866378: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f009e49430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-12-25 14:15:04.866403: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-12-25 14:15:04.976450: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:15:04.987189: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:15:05.079488: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:15:05.153439: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-12-25 14:15:05.243930: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-12-25 14:15:25.622792: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chonghui/envs/miniconda3/envs/expert/bin/expert\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/expert/CLI/main.py\", line 48, in main\n",
      "    evaluate(cfg, args)\n",
      "  File \"/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/expert/CLI/main_evaluate.py\", line 11, in evaluate\n",
      "    layers = [os.path.join(args.input, i) for i in sorted(os.listdir(args.input), key=lambda x: int(x.split('.')[0].split('-')[1]))]\n",
      "  File \"/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/expert/CLI/main_evaluate.py\", line 11, in <lambda>\n",
      "    layers = [os.path.join(args.input, i) for i in sorted(os.listdir(args.input), key=lambda x: int(x.split('.')[0].split('-')[1]))]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'for((i=1; i<11; i++)); do\\n    expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_DM \\\\\\n            -m ../Disease-diagnosis/experiments/exp_3/TrainModel/ --finetune --update-statistics;\\n    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult_Adapt_ft_DM;\\n    expert evaluate -i experiments/exp_$i/SearchResult_Adapt_ft_DM -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Adapt_ft_DM -S 0;\\ndone\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-734-2e8ff656d004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for((i=1; i<11; i++)); do\\n    expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_DM \\\\\\n            -m ../Disease-diagnosis/experiments/exp_3/TrainModel/ --finetune --update-statistics;\\n    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult_Adapt_ft_DM;\\n    expert evaluate -i experiments/exp_$i/SearchResult_Adapt_ft_DM -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Adapt_ft_DM -S 0;\\ndone\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/envs/miniconda3/envs/expert/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/miniconda3/envs/expert/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/envs/miniconda3/envs/expert/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/miniconda3/envs/expert/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'for((i=1; i<11; i++)); do\\n    expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_DM \\\\\\n            -m ../Disease-diagnosis/experiments/exp_3/TrainModel/ --finetune --update-statistics;\\n    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult_Adapt_ft_DM;\\n    expert evaluate -i experiments/exp_$i/SearchResult_Adapt_ft_DM -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Adapt_ft_DM -S 0;\\ndone\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for((i=1; i<11; i++)); do\n",
    "    #expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_DM \\\n",
    "    #        -m ../Disease-diagnosis/experiments/exp_3/TrainModel/ --finetune --update-statistics;\n",
    "    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult_Adapt_ft_DM;\n",
    "    expert evaluate -i experiments/exp_$i/SearchResult_Adapt_ft_DM -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Adapt_ft_DM -S 0;\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8783010913549)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Figure.pdf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "suffix = 'Adapt_ft_DM'\n",
    "contributions = pd.concat([pd.read_csv('experiments/exp_{}/SearchResult_{}/layer-2.csv'.format(i, suffix), index_col=0) for i in range(1, 11)])\n",
    "metadata = pd.read_csv('dataFiles/metadata.csv').set_index('#SampleID')\n",
    "contributions = contributions.join(metadata, how='left')\n",
    "#contributions = contributions[contributions.Phase != 'T6']\n",
    "#data = contributions.groupby(by=['People', 'Phase'], as_index=False).mean()\n",
    "contributions['GroupAll'] = '1'\n",
    "contributions = contributions.sort_values('Timepoint')\n",
    "contributions['Timepoint_str'] = contributions['Timepoint'].astype(str)\n",
    "contributions['is_MT10'] = (contributions.People == 'MT10').map({True: 'MT10', False: 'MT1-9'})\n",
    "data = contributions\n",
    "\n",
    "T_unique = contributions[['Phase', 'Timepoint']].drop_duplicates()\n",
    "contributions['Period'] = contributions.Phase.map(T_unique.Phase.value_counts(sort=False).to_dict())\n",
    "contributions['Status (MT10)'] = 'Normal'\n",
    "contributions.loc[(contributions.People == 'MT10')&(contributions.Timepoint > 15)&(contributions.Timepoint < 21), 'Status (MT10)'] = 'Early back'\n",
    "\n",
    "plot = (ggplot(data, aes(x='Timepoint_str', y='root:Trinidad and Tobago'))\n",
    "        + geom_boxplot(aes(fill='Phase', group='Timepoint_str'), outlier_shape='', show_legend=True, data=data[data.People != 'MT10'])\n",
    "        #+ geom_violin(aes(fill='Phase', group='Timepoint_str'), show_legend=True, data=data[data.People != 'MT10'], bw=0.1)\n",
    "        + scale_fill_manual([\"#E64B35FF\", \"#4DBBD5FF\", \"#00A087FF\", \"#3C5488FF\", \"#F39B7FFF\", \"#8491B4FF\", \"#91D1C2FF\", \"#DC0000FF\", \"#7E6148FF\", \"#B09C85FF\"])\n",
    "        #+ geom_boxplot(aes(group='Timepoint_str'), fill='white', alpha=0.5, show_legend=False, outlier_shape='', data=data[data.People != 'MT10'], width=0.4)\n",
    "        + geom_smooth(aes(linetype='is_MT10', group='is_MT10'), se=False, method='loess', show_legend=True)\n",
    "        + scale_linetype_manual(['solid', 'dashdot'])\n",
    "        + geom_point(aes(color='Status (MT10)'), data=data[data.People == 'MT10'])\n",
    "        + scale_color_manual(['brown', \"black\"])\n",
    "        + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(),\n",
    "             axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "        + geom_hline(yintercept=0.5, linetype=\"dotted\")\n",
    "        + geom_vline(xintercept=[2.5, 20.5], linetype=\"dashed\", size=0.6)\n",
    "        #+ geom_label(data='Early back', position=[15.5, 0.2])\n",
    "        + scale_x_discrete(limits=contributions['Timepoint_str'].unique())\n",
    "        + xlab('Time point')\n",
    "        + ylab('Contribution from \"Trinidad and Tobago\"')\n",
    ")\n",
    "\n",
    "print(plot)\n",
    "plot.save('Figure.pdf'.format(suffix), dpi=120, width=6.4*1, height=4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.5327818397656441, pvalue=0.6015066309222087)\n",
      "Ttest_indResult(statistic=1.0220101050330948, pvalue=0.31898006018972447)\n",
      "Ttest_indResult(statistic=4.06690102642737, pvalue=0.00044489357943088974)\n",
      "Ttest_indResult(statistic=2.6235403171387484, pvalue=0.025442662476214083)\n",
      "Ttest_indResult(statistic=-2.744206836828457, pvalue=0.011840271564764084)\n",
      "Ttest_indResult(statistic=7.85609573748906, pvalue=1.4765500804871871e-08)\n",
      "Ttest_indResult(statistic=2.558875837004759, pvalue=0.017229442047329617)\n",
      "Ttest_indResult(statistic=0.8379415384238925, pvalue=0.4130523563833476)\n",
      "Ttest_indResult(statistic=3.6589010009032354, pvalue=0.0017956984427735657)\n",
      "Ttest_indResult(statistic=2.8830981871475583, pvalue=0.010813580256581343)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "for i in range(1, 11):\n",
    "    print(ttest_ind(data_16_20.loc[(data_16_20.People == 'MT{}'.format(i))&(data_16_20.Group == 'MTT'), 'root:Trinidad and Tobago'], \n",
    "                    data_16_20.loc[(data_16_20.People == 'MT{}'.format(i))&(data_16_20.Group == 'MTT'), 'root:China']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>People</th>\n",
       "      <th>Group</th>\n",
       "      <th>Phase</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTC1T1</td>\n",
       "      <td>MT1</td>\n",
       "      <td>MTC</td>\n",
       "      <td>T1</td>\n",
       "      <td>root:China</td>\n",
       "      <td>0.775153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MTC5T1</td>\n",
       "      <td>MT5</td>\n",
       "      <td>MTC</td>\n",
       "      <td>T1</td>\n",
       "      <td>root:China</td>\n",
       "      <td>0.808747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MTC6T1</td>\n",
       "      <td>MT6</td>\n",
       "      <td>MTC</td>\n",
       "      <td>T1</td>\n",
       "      <td>root:China</td>\n",
       "      <td>0.780593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MTC7T1</td>\n",
       "      <td>MT7</td>\n",
       "      <td>MTC</td>\n",
       "      <td>T1</td>\n",
       "      <td>root:China</td>\n",
       "      <td>0.737699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTC3T1</td>\n",
       "      <td>MT3</td>\n",
       "      <td>MTC</td>\n",
       "      <td>T1</td>\n",
       "      <td>root:China</td>\n",
       "      <td>0.807118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>MTB6T28</td>\n",
       "      <td>MT6</td>\n",
       "      <td>MTB</td>\n",
       "      <td>T6</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.025541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>MTB8T28</td>\n",
       "      <td>MT8</td>\n",
       "      <td>MTB</td>\n",
       "      <td>T6</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>MTB6T29</td>\n",
       "      <td>MT6</td>\n",
       "      <td>MTB</td>\n",
       "      <td>T6</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.031111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>MTB8T29</td>\n",
       "      <td>MT8</td>\n",
       "      <td>MTB</td>\n",
       "      <td>T6</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.061626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>MTB8T30</td>\n",
       "      <td>MT8</td>\n",
       "      <td>MTB</td>\n",
       "      <td>T6</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index People Group Phase    variable     value\n",
       "0     MTC1T1    MT1   MTC    T1  root:China  0.775153\n",
       "1     MTC5T1    MT5   MTC    T1  root:China  0.808747\n",
       "2     MTC6T1    MT6   MTC    T1  root:China  0.780593\n",
       "3     MTC7T1    MT7   MTC    T1  root:China  0.737699\n",
       "4     MTC3T1    MT3   MTC    T1  root:China  0.807118\n",
       "..       ...    ...   ...   ...         ...       ...\n",
       "556  MTB6T28    MT6   MTB    T6     Unknown  0.025541\n",
       "557  MTB8T28    MT8   MTB    T6     Unknown  0.000000\n",
       "558  MTB6T29    MT6   MTB    T6     Unknown  0.031111\n",
       "559  MTB8T29    MT8   MTB    T6     Unknown  0.061626\n",
       "560  MTB8T30    MT8   MTB    T6     Unknown  0.000000\n",
       "\n",
       "[561 rows x 6 columns]"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = contributions.reset_index().melt(id_vars=['index', 'People', 'Group', 'Phase'], value_vars=['root:China', 'root:Trinidad and Tobago', 'Unknown'])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Location'] = p.Group.map({'MTC': 'China', 'MTT': 'Trinidad and Tobago', 'MTB': 'China'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8782910561603)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 6.4 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Figure_contrib.pdf\n"
     ]
    }
   ],
   "source": [
    "plot = (ggplot(p[p.variable=='root:Trinidad and Tobago'], aes(x='Location', y='value', fill='Location'))        \n",
    "        + geom_boxplot(show_legend=True)\n",
    "        #+ geom_violin(aes(fill='Phase', group='Timepoint_str'), show_legend=True, data=data[data.People != 'MT10'], bw=0.1)\n",
    "        #+ scale_fill_manual([\"#E64B35FF\", \"#4DBBD5FF\", \"#00A087FF\", \"#3C5488FF\", \"#F39B7FFF\", \"#8491B4FF\", \"#91D1C2FF\", \"#DC0000FF\", \"#7E6148FF\", \"#B09C85FF\"])\n",
    "        #+ geom_boxplot(aes(group='Timepoint_str'), fill='white', alpha=0.5, show_legend=False, outlier_shape='', data=data[data.People != 'MT10'], width=0.4)\n",
    "        #+ geom_smooth(aes(linetype='is_MT10', group='is_MT10'), se=False, method='loess', show_legend=True)\n",
    "        #+ scale_linetype_manual(['solid', 'dashdot'])\n",
    "        #+ geom_point(aes(color='People'))\n",
    "        #+ scale_color_manual(['brown', \"black\"])\n",
    "        + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(),\n",
    "             axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "        + xlab('Time point')\n",
    "        + ylab('Contribution from \"Trinidad and Tobago\"')\n",
    "        #+ coord_flip()\n",
    "        #+ facet_wrap('Phase')\n",
    "        + facet_wrap(['People'], nrow=5, ncol=4)\n",
    ")\n",
    "print(plot)\n",
    "plot.save('Figure_contrib.pdf'.format(suffix), dpi=120, width=6.4*1, height=4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa = pd.DataFrame(abu.index.to_series().apply(lambda x: dict(map(lambda y: y.split('__'), filter(lambda x: not x.endswith('__'), x.split(';'))))).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing the abundance data...\n",
      "Trying calculating jensenshannon beta_diversity using scikit-bio & scikit-learn package...\n",
      "This could be time-consuming.\n",
      "Failed, the metric you selected is not supported by neither scikit-bio nor scikit-learn.\n",
      "Trying using SciPy...\n",
      "Succeeded!\n",
      "Visualizing the data using plotnine package...\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Plots.JSD/PCoA.pdf\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 1 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Plots.JSD/PC1_boxplot.pdf\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 1 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Plots.JSD/PC2_boxplot.pdf\n",
      "Plots are saved in Plots.JSD. Import them into Illustrator for further improvements.\n"
     ]
    }
   ],
   "source": [
    "meta.to_csv('Metadata_PCoA.csv')\n",
    "abu.to_csv('abundance_PCoA.csv')\n",
    "!python ../../UniPCoA/UniPCoA.py -i abundance_PCoA.csv -m Metadata_PCoA.csv \\\n",
    "    --metric jensenshannon -o Plots.JSD\n",
    "\n",
    "#-t ../../UniPCoA/LTPs132_SSU_tree.newick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =[pd.read_csv('experiments/exp_{}/EvalResult_{}/overall.csv'.format(i, 'Adapt_ft_DM'), index_col=0).rename(columns=lambda x: '{}-exp_{}-{}'.format(x, i, 'Adapt_ft_DM')).dropna()\n",
    "        for i in range(5)]\n",
    "overall = pd.concat(metrics, axis=1)\n",
    "overall\n",
    "overall = overall.reset_index().melt(id_vars=['index'], value_vars=overall.columns.tolist(), var_name='metric').dropna()\n",
    "overall['Experiment'] = overall['metric'].str.split('-').apply(lambda x: '{}'.format( x[3])).map({'Adapt_ft_DM': 'Transfer (DM)', 'Adapt_ft_HM': 'Transfer (HM)', 'Train': 'Independent'})\n",
    "overall['Metric'] = overall['metric'].str.split('-').apply(lambda x: '{}-{}'.format(x[0], x[1]))\n",
    "overall = overall[overall.Metric == 'ROC-AUC'].groupby(by='index', as_index=False).mean().round(4)\n",
    "overall['index'] = overall['index'].apply(lambda x: x.split(' ')[2].rstrip(')'))\n",
    "overall['ROC-AUC'] = overall['value']\n",
    "overall['Stage'] = overall['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8737241783248)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 3.6 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: CRC_stage_performance.pdf\n"
     ]
    }
   ],
   "source": [
    "from plotnine import *\n",
    "plot = (ggplot(overall, aes(x='Stage', y='ROC-AUC', fill='Stage'))\n",
    "         + geom_bar(stat='identity', width=0.3, show_legend = False)\n",
    "         + scale_fill_manual(['#E64B35FF','#4DBBD5FF','#00A087FF','#3C5488FF','#F39B7FFF','#8491B4FF','#91D1C2FF'])\n",
    "         + geom_text(aes(label='value'), position=position_dodge(width=0.9), nudge_y=0.02)\n",
    "         + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(),\n",
    "                 axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "       )\n",
    "print(plot)\n",
    "plot.save('CRC_stage_performance.pdf', dpi=120, width=3.6, height=4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "        ERR475482   ERR475493   ERR475500  ...   ERR481063   ERR481064   ERR481065\n",
      "count  854.000000  854.000000  854.000000  ...  854.000000  854.000000  854.000000\n",
      "mean     0.188848    0.155293    0.091492  ...    0.221712    0.225143    0.222403\n",
      "std      2.274977    1.508092    1.018507  ...    4.716516    4.774864    4.758246\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     47.130700   37.035000   24.845200  ...  135.370960  137.099840  136.803670\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/854 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "         ERR475482    ERR475493  ...    ERR481064    ERR481065\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026303     0.020940  ...     0.031547     0.031352\n",
      "std       0.859542     0.568293  ...     1.800976     1.794912\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      47.130699    37.035000  ...   137.099838   136.803665\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Normalizing results...\n",
      "         ERR475482    ERR475493  ...    ERR481064    ERR481065\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.005441     0.004519  ...     0.009505     0.009532\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.298344     0.294471  ...     0.723581     0.726522\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.170845  ...    0.0  33.050780  69.748160    0.0\n",
      "1      0.0    0.0    0.0 -0.171821  ...    0.0   2.232037   4.557535    0.0\n",
      "2      0.0    0.0    0.0  5.015766  ...    0.0  15.904994  23.933508    0.0\n",
      "3      0.0    0.0    0.0 -0.170144  ...    0.0  11.578986  24.785040    0.0\n",
      "4      0.0    0.0    0.0  1.735051  ...    0.0   3.927780   8.491182    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "631    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "632    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "633    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "634    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.172956  ...    0.0  33.823331  71.783639    0.0\n",
      "1      0.0    0.0    0.0 -0.173955  ...    0.0   2.282320   4.691487    0.0\n",
      "2      0.0    0.0    0.0  5.141887  ...    0.0  16.275716  24.632632    0.0\n",
      "3      0.0    0.0    0.0 -0.172237  ...    0.0  11.848324  25.509002    0.0\n",
      "4      0.0    0.0    0.0  1.780061  ...    0.0   4.017805   8.739873    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "631    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "632    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "633    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "634    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.174914  ...    0.0  33.551714  69.381050    0.0\n",
      "1      0.0    0.0    0.0 -0.175921  ...    0.0   2.265456   4.532195    0.0\n",
      "2      0.0    0.0    0.0  5.177374  ...    0.0  16.145829  23.806587    0.0\n",
      "3      0.0    0.0    0.0 -0.174190  ...    0.0  11.754197  24.653654    0.0\n",
      "4      0.0    0.0    0.0  1.791862  ...    0.0   3.986923   8.445219    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "631    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "632    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "633    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "634    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.173595  ...    0.0  33.276793  69.338235    0.0\n",
      "1      0.0    0.0    0.0 -0.174612  ...    0.0   2.246413   4.529124    0.0\n",
      "2      0.0    0.0    0.0  5.233645  ...    0.0  16.013264  23.791703    0.0\n",
      "3      0.0    0.0    0.0 -0.172864  ...    0.0  11.657549  24.638252    0.0\n",
      "4      0.0    0.0    0.0  1.813374  ...    0.0   3.953801   8.439750    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "631    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "632    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "633    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "634    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.172859  ...    0.0  33.199631  69.466157    0.0\n",
      "1      0.0    0.0    0.0 -0.173845  ...    0.0   2.242247   4.538022    0.0\n",
      "2      0.0    0.0    0.0  5.068282  ...    0.0  15.976713  23.835978    0.0\n",
      "3      0.0    0.0    0.0 -0.172151  ...    0.0  11.631244  24.684081    0.0\n",
      "4      0.0    0.0    0.0  1.753075  ...    0.0   3.945619   8.455830    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "631    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "632    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "633    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "634    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('dataFiles/species_abundance.csv', index_col=0).to_csv('dataFiles/species_abundance.tsv', sep='\\t')\n",
    "!ls dataFiles/species_abundance.tsv > tmp\n",
    "!expert convert -i tmp -o CRC_cm.h5 --in-cm\n",
    "!for i in {0,1,2,3,4}; do expert search -i CRC_cm.h5 -o CRC_contribution_$i -m ../Disease-diagnosis/experiments/exp_$i/TrainModel; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
