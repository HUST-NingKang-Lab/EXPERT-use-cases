{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to assess the capability of EXPERT on CRC detection. The data comes from *[REF]* (below). Detailed illustration of the results is avaliable in our papar (below).\n",
    "\n",
    "[REF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Resource|Description|\n",
    "| - | - |\n",
    "|`dataFiles`|data files used in this tutorial.|\n",
    "|`experiments`|intermediate/temporary files generated by the program.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EXPERT supports reproducible optimization & inference.\n",
    "- Rerunning the entire notebook should yield **completely consistent** results (compared to those reported in our paper).\n",
    "\n",
    "[REF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed to get reproducible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(2)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from expert.src.utils import read_genus_abu, read_labels, load_otlg, zero_weight_unk, parse_otlg, get_dmax\n",
    "from expert.src.preprocessing import *\n",
    "from expert.src.model import *\n",
    "from expert.CLI.CLI_utils import find_pkg_resource as find_expert_resource\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, AlphaDropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.initializers import HeUniform, GlorotUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROC-AUC_3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics =[pd.read_csv('../Disease-diagnosis/experiments/exp_{}/EvalResult_{}/overall.csv'.format(i, 'Train'), index_col=0)[['ROC-AUC']].rename(columns=lambda x: x + '_' + str(i))\n",
    "        for i in range(5)]\n",
    "overall = pd.concat(metrics, axis=1)\n",
    "#avg = overall.T.groupby(by=overall.columns.to_series().apply(lambda x: '-'.join(x.split('-')[0:2]) + '(' + x.split('-')[3] + ')')).mean().T\n",
    "#avg = avg.reset_index()\n",
    "overall.mean().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ontology using mapper file of source samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:CRC (stage 0)\n",
      "root:CRC (stage I)\n",
      "root:CRC (stage II)\n",
      "root:CRC (stage III)\n",
      "root:CRC (stage IV)\n",
      "Reading microbiome structure...\n",
      "Generating Ontology...\n",
      "100%|██████████████████████████████████████████| 5/5 [00:00<00:00, 41282.52it/s]\n",
      "root\n",
      "├── root:CRC (stage 0)\n",
      "├── root:CRC (stage I)\n",
      "├── root:CRC (stage II)\n",
      "├── root:CRC (stage III)\n",
      "└── root:CRC (stage IV)\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!awk -F ',' '{print $5}' dataFiles/CRC_samples_stages.csv | grep -v \"Env\" | sort | uniq  > microbiomes.txt\n",
    "!expert construct -i microbiomes.txt -o ontology.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyper-parameters for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = HeUniform(seed=2)\n",
    "sig_init = GlorotUniform(seed=2)\n",
    "phylogeny_path = find_expert_resource('resources/phylogeny.csv')\n",
    "ontology = load_otlg('ontology.pkl')\n",
    "phylogeny = pd.read_csv(phylogeny_path, index_col=0)\n",
    "lrreducer = ReduceLROnPlateau(monitor='val_loss', patience=5, min_lr=1e-5, verbose=5, factor=0.1)\n",
    "stopper = EarlyStopping(monitor='val_loss', patience=15, verbose=5, restore_best_weights=True)\n",
    "callbacks = [lrreducer, stopper]\n",
    "phylogeny = pd.read_csv(find_expert_resource('resources/phylogeny.csv'), index_col=0)\n",
    "optimizer = Adam(lr=1e-4)\n",
    "metrics = [BinaryAccuracy(name='acc'), AUC(name='AUC')]\n",
    "loss = BinaryCrossentropy()\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "validation_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data, using EXPERT's command-line API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       571\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       15\n",
      "root:CRC (stage I)      177\n",
      "root:CRC (stage II)     117\n",
      "root:CRC (stage III)     84\n",
      "root:CRC (stage IV)     178\n",
      "Unknown                   0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       64\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       1\n",
      "root:CRC (stage I)      19\n",
      "root:CRC (stage II)      9\n",
      "root:CRC (stage III)     9\n",
      "root:CRC (stage IV)     26\n",
      "Unknown                  0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       571\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       13\n",
      "root:CRC (stage I)      180\n",
      "root:CRC (stage II)     114\n",
      "root:CRC (stage III)     80\n",
      "root:CRC (stage IV)     184\n",
      "Unknown                   0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       64\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       3\n",
      "root:CRC (stage I)      16\n",
      "root:CRC (stage II)     12\n",
      "root:CRC (stage III)    13\n",
      "root:CRC (stage IV)     20\n",
      "Unknown                  0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       571\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       15\n",
      "root:CRC (stage I)      175\n",
      "root:CRC (stage II)     111\n",
      "root:CRC (stage III)     86\n",
      "root:CRC (stage IV)     184\n",
      "Unknown                   0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       64\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       1\n",
      "root:CRC (stage I)      21\n",
      "root:CRC (stage II)     15\n",
      "root:CRC (stage III)     7\n",
      "root:CRC (stage IV)     20\n",
      "Unknown                  0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       571\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       14\n",
      "root:CRC (stage I)      180\n",
      "root:CRC (stage II)     117\n",
      "root:CRC (stage III)     84\n",
      "root:CRC (stage IV)     176\n",
      "Unknown                   0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       64\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       2\n",
      "root:CRC (stage I)      16\n",
      "root:CRC (stage II)      9\n",
      "root:CRC (stage III)     9\n",
      "root:CRC (stage IV)     28\n",
      "Unknown                  0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       571\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       15\n",
      "root:CRC (stage I)      174\n",
      "root:CRC (stage II)     112\n",
      "root:CRC (stage III)     86\n",
      "root:CRC (stage IV)     184\n",
      "Unknown                   0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       64\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:CRC (stage 0)       1\n",
      "root:CRC (stage I)      22\n",
      "root:CRC (stage II)     14\n",
      "root:CRC (stage III)     7\n",
      "root:CRC (stage IV)     20\n",
      "Unknown                  0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 215.05it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 26.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 271.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 300.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 36.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 287.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 36.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 279.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 309.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 39.18it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 268.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 280.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 36.29it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 293.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 270.73it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 28.92it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for((i=0; i<5; i++)); do \\\n",
    "ls experiments/exp_$i/QueryCM.tsv.gz > tmp; expert convert -i tmp --in-cm -o experiments/exp_$i/QueryCM.h5; \\\n",
    "ls experiments/exp_$i/SourceCM.tsv.gz > tmp; expert convert -i tmp --in-cm -o experiments/exp_$i/SourceCM.h5; \\\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/exp_$i/SourceMapper.csv.gz -o experiments/exp_$i/SourceLabels.h5; \\\n",
    "expert map --to-otlg -t ontology.pkl -i experiments/exp_$i/QueryMapper.csv.gz -o experiments/exp_$i/QueryLabels.h5; \\\n",
    "done\n",
    "#rm tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "N. NaN in input features: 0\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015062  0.041284\n",
      "4      0.015041  0.041268\n",
      "...         ...       ...\n",
      "18013  0.000061  0.000243\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002382  0.011947\n",
      "18016  0.002317  0.011715\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Pre-training using Adam with lr=1e-05...\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.7021 - acc: 0.4877 - val_loss: 0.7029 - val_acc: 0.5069\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6914 - acc: 0.5099 - val_loss: 0.6967 - val_acc: 0.5448\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 18,998,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training using Adam with lr=0.001...\n",
      "Epoch 3/1002\n",
      "1/1 [==============================] - 1s 765ms/step - loss: 0.6823 - acc: 0.5279 - auROC: 0.4625 - val_loss: 0.6941 - val_acc: 0.6379 - val_auROC: 0.4617\n",
      "Epoch 4/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6688 - acc: 0.6363 - auROC: 0.5201 - val_loss: 0.6735 - val_acc: 0.6345 - val_auROC: 0.5097\n",
      "Epoch 5/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6355 - acc: 0.6542 - auROC: 0.5638 - val_loss: 0.6531 - val_acc: 0.6552 - val_auROC: 0.5166\n",
      "Epoch 6/1002\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6104 - acc: 0.6854 - auROC: 0.5948 - val_loss: 0.6269 - val_acc: 0.6931 - val_auROC: 0.5285\n",
      "Epoch 7/1002\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5873 - acc: 0.6947 - auROC: 0.5983 - val_loss: 0.6222 - val_acc: 0.6655 - val_auROC: 0.5337\n",
      "Epoch 8/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5709 - acc: 0.6858 - auROC: 0.6302 - val_loss: 0.6046 - val_acc: 0.6966 - val_auROC: 0.5516\n",
      "Epoch 9/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5535 - acc: 0.7298 - auROC: 0.6523 - val_loss: 0.5951 - val_acc: 0.6621 - val_auROC: 0.5532\n",
      "Epoch 10/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5477 - acc: 0.6951 - auROC: 0.6486 - val_loss: 0.5898 - val_acc: 0.7345 - val_auROC: 0.5855\n",
      "Epoch 11/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5368 - acc: 0.7454 - auROC: 0.6779 - val_loss: 0.5720 - val_acc: 0.7034 - val_auROC: 0.5977\n",
      "Epoch 12/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5226 - acc: 0.7341 - auROC: 0.6920 - val_loss: 0.5651 - val_acc: 0.7345 - val_auROC: 0.6020\n",
      "Epoch 13/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5156 - acc: 0.7669 - auROC: 0.6999 - val_loss: 0.5588 - val_acc: 0.7345 - val_auROC: 0.6096\n",
      "Epoch 14/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5075 - acc: 0.7743 - auROC: 0.7094 - val_loss: 0.5575 - val_acc: 0.7414 - val_auROC: 0.6151\n",
      "Epoch 15/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5008 - acc: 0.7778 - auROC: 0.7237 - val_loss: 0.5433 - val_acc: 0.7345 - val_auROC: 0.6479\n",
      "Epoch 16/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4924 - acc: 0.7864 - auROC: 0.7431 - val_loss: 0.5418 - val_acc: 0.7655 - val_auROC: 0.6700\n",
      "Epoch 17/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4885 - acc: 0.7805 - auROC: 0.7566 - val_loss: 0.5363 - val_acc: 0.7448 - val_auROC: 0.6580\n",
      "Epoch 18/1002\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.4834 - acc: 0.7840 - auROC: 0.7564 - val_loss: 0.5318 - val_acc: 0.7517 - val_auROC: 0.6544\n",
      "Epoch 19/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4776 - acc: 0.7957 - auROC: 0.7520 - val_loss: 0.5237 - val_acc: 0.7448 - val_auROC: 0.6796\n",
      "Epoch 20/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4699 - acc: 0.8156 - auROC: 0.7786 - val_loss: 0.5195 - val_acc: 0.7483 - val_auROC: 0.6879\n",
      "Epoch 21/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4652 - acc: 0.8226 - auROC: 0.7955 - val_loss: 0.5148 - val_acc: 0.7379 - val_auROC: 0.6976\n",
      "Epoch 22/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4606 - acc: 0.8140 - auROC: 0.8010 - val_loss: 0.5151 - val_acc: 0.7517 - val_auROC: 0.6921\n",
      "Epoch 23/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4559 - acc: 0.8101 - auROC: 0.8072 - val_loss: 0.5050 - val_acc: 0.7517 - val_auROC: 0.7146\n",
      "Epoch 24/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.4489 - acc: 0.8234 - auROC: 0.8213 - val_loss: 0.5010 - val_acc: 0.7655 - val_auROC: 0.7158\n",
      "Epoch 25/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4443 - acc: 0.8292 - auROC: 0.8227 - val_loss: 0.5010 - val_acc: 0.7655 - val_auROC: 0.7131\n",
      "Epoch 26/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4405 - acc: 0.8347 - auROC: 0.8291 - val_loss: 0.4977 - val_acc: 0.7586 - val_auROC: 0.7142\n",
      "Epoch 27/1002\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.4368 - acc: 0.8257 - auROC: 0.8233 - val_loss: 0.4946 - val_acc: 0.7724 - val_auROC: 0.7295\n",
      "Epoch 28/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4353 - acc: 0.8448 - auROC: 0.8393 - val_loss: 0.4900 - val_acc: 0.7793 - val_auROC: 0.7326\n",
      "Epoch 29/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4303 - acc: 0.8402 - auROC: 0.8429 - val_loss: 0.4817 - val_acc: 0.7897 - val_auROC: 0.7520\n",
      "Epoch 30/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4234 - acc: 0.8441 - auROC: 0.8542 - val_loss: 0.4762 - val_acc: 0.7897 - val_auROC: 0.7551\n",
      "Epoch 31/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4222 - acc: 0.8468 - auROC: 0.8484 - val_loss: 0.4772 - val_acc: 0.7759 - val_auROC: 0.7419\n",
      "Epoch 32/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4173 - acc: 0.8448 - auROC: 0.8508 - val_loss: 0.4743 - val_acc: 0.7793 - val_auROC: 0.7575\n",
      "Epoch 33/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4137 - acc: 0.8596 - auROC: 0.8641 - val_loss: 0.4741 - val_acc: 0.7966 - val_auROC: 0.7587\n",
      "Epoch 34/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4093 - acc: 0.8647 - auROC: 0.8718 - val_loss: 0.4719 - val_acc: 0.7759 - val_auROC: 0.7465\n",
      "Epoch 35/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4089 - acc: 0.8437 - auROC: 0.8499 - val_loss: 0.4669 - val_acc: 0.8034 - val_auROC: 0.7723\n",
      "Epoch 36/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4055 - acc: 0.8741 - auROC: 0.8764 - val_loss: 0.4661 - val_acc: 0.7897 - val_auROC: 0.7718\n",
      "Epoch 37/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4023 - acc: 0.8577 - auROC: 0.8715 - val_loss: 0.4601 - val_acc: 0.8000 - val_auROC: 0.7764\n",
      "Epoch 38/1002\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3982 - acc: 0.8643 - auROC: 0.8761 - val_loss: 0.4552 - val_acc: 0.8069 - val_auROC: 0.7826\n",
      "Epoch 39/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3945 - acc: 0.8635 - auROC: 0.8785 - val_loss: 0.4562 - val_acc: 0.8034 - val_auROC: 0.7952\n",
      "Epoch 40/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3928 - acc: 0.8628 - auROC: 0.8859 - val_loss: 0.4536 - val_acc: 0.8034 - val_auROC: 0.7798\n",
      "Epoch 41/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3911 - acc: 0.8710 - auROC: 0.8823 - val_loss: 0.4513 - val_acc: 0.8034 - val_auROC: 0.7784\n",
      "Epoch 42/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3866 - acc: 0.8698 - auROC: 0.8819 - val_loss: 0.4492 - val_acc: 0.8241 - val_auROC: 0.7888\n",
      "Epoch 43/1002\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3856 - acc: 0.8671 - auROC: 0.8853 - val_loss: 0.4479 - val_acc: 0.8241 - val_auROC: 0.7915\n",
      "Epoch 44/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3836 - acc: 0.8780 - auROC: 0.8930 - val_loss: 0.4498 - val_acc: 0.8103 - val_auROC: 0.7813\n",
      "Epoch 45/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3816 - acc: 0.8702 - auROC: 0.8867 - val_loss: 0.4460 - val_acc: 0.8172 - val_auROC: 0.7921\n",
      "Epoch 46/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3786 - acc: 0.8702 - auROC: 0.8916 - val_loss: 0.4405 - val_acc: 0.8172 - val_auROC: 0.7903\n",
      "Epoch 47/1002\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3748 - acc: 0.8846 - auROC: 0.8963 - val_loss: 0.4412 - val_acc: 0.8103 - val_auROC: 0.7828\n",
      "Epoch 48/1002\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3745 - acc: 0.8760 - auROC: 0.8893 - val_loss: 0.4416 - val_acc: 0.8207 - val_auROC: 0.8011\n",
      "Epoch 49/1002\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3709 - acc: 0.8834 - auROC: 0.9044 - val_loss: 0.4380 - val_acc: 0.8000 - val_auROC: 0.7862\n",
      "Epoch 50/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3689 - acc: 0.8690 - auROC: 0.8932 - val_loss: 0.4348 - val_acc: 0.8103 - val_auROC: 0.7958\n",
      "Epoch 51/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3672 - acc: 0.8768 - auROC: 0.8990 - val_loss: 0.4311 - val_acc: 0.8138 - val_auROC: 0.8021\n",
      "Epoch 52/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3645 - acc: 0.8807 - auROC: 0.9043 - val_loss: 0.4327 - val_acc: 0.8379 - val_auROC: 0.8082\n",
      "Epoch 53/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3628 - acc: 0.8842 - auROC: 0.9071 - val_loss: 0.4311 - val_acc: 0.8276 - val_auROC: 0.7996\n",
      "Epoch 54/1002\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3606 - acc: 0.8838 - auROC: 0.9045 - val_loss: 0.4296 - val_acc: 0.8241 - val_auROC: 0.7989\n",
      "Epoch 55/1002\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3586 - acc: 0.8862 - auROC: 0.9043 - val_loss: 0.4268 - val_acc: 0.8241 - val_auROC: 0.8058\n",
      "Epoch 56/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3565 - acc: 0.8862 - auROC: 0.9096 - val_loss: 0.4281 - val_acc: 0.8379 - val_auROC: 0.8158\n",
      "Epoch 57/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3556 - acc: 0.8908 - auROC: 0.9144 - val_loss: 0.4238 - val_acc: 0.8310 - val_auROC: 0.8053\n",
      "Epoch 58/1002\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3539 - acc: 0.8823 - auROC: 0.9082 - val_loss: 0.4224 - val_acc: 0.8414 - val_auROC: 0.8092\n",
      "Epoch 59/1002\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3517 - acc: 0.8889 - auROC: 0.9118 - val_loss: 0.4233 - val_acc: 0.8483 - val_auROC: 0.8152\n",
      "Epoch 60/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3494 - acc: 0.8943 - auROC: 0.9168 - val_loss: 0.4214 - val_acc: 0.8345 - val_auROC: 0.8092\n",
      "Epoch 61/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3486 - acc: 0.8932 - auROC: 0.9137 - val_loss: 0.4203 - val_acc: 0.8517 - val_auROC: 0.8199\n",
      "Epoch 62/1002\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.3471 - acc: 0.8998 - auROC: 0.9211 - val_loss: 0.4172 - val_acc: 0.8345 - val_auROC: 0.8071\n",
      "Epoch 63/1002\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3451 - acc: 0.8963 - auROC: 0.9138 - val_loss: 0.4160 - val_acc: 0.8276 - val_auROC: 0.8094\n",
      "Epoch 64/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3434 - acc: 0.8943 - auROC: 0.9165 - val_loss: 0.4163 - val_acc: 0.8621 - val_auROC: 0.8226\n",
      "Epoch 65/1002\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3416 - acc: 0.9115 - auROC: 0.9258 - val_loss: 0.4133 - val_acc: 0.8552 - val_auROC: 0.8164\n",
      "Epoch 66/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3399 - acc: 0.9080 - auROC: 0.9210 - val_loss: 0.4117 - val_acc: 0.8448 - val_auROC: 0.8146\n",
      "Epoch 67/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3397 - acc: 0.8986 - auROC: 0.9180 - val_loss: 0.4121 - val_acc: 0.8655 - val_auROC: 0.8313\n",
      "Epoch 68/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3378 - acc: 0.9060 - auROC: 0.9280 - val_loss: 0.4075 - val_acc: 0.8448 - val_auROC: 0.8238\n",
      "Epoch 69/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3359 - acc: 0.9010 - auROC: 0.9220 - val_loss: 0.4058 - val_acc: 0.8448 - val_auROC: 0.8229\n",
      "Epoch 70/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3343 - acc: 0.9014 - auROC: 0.9229 - val_loss: 0.4050 - val_acc: 0.8552 - val_auROC: 0.8349\n",
      "Epoch 71/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3327 - acc: 0.9099 - auROC: 0.9319 - val_loss: 0.4058 - val_acc: 0.8448 - val_auROC: 0.8183\n",
      "Epoch 72/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3308 - acc: 0.9021 - auROC: 0.9234 - val_loss: 0.4059 - val_acc: 0.8448 - val_auROC: 0.8151\n",
      "Epoch 73/1002\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3296 - acc: 0.9014 - auROC: 0.9228 - val_loss: 0.4017 - val_acc: 0.8552 - val_auROC: 0.8332\n",
      "Epoch 74/1002\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3276 - acc: 0.9111 - auROC: 0.9315 - val_loss: 0.3974 - val_acc: 0.8621 - val_auROC: 0.8345\n",
      "Epoch 75/1002\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3257 - acc: 0.9096 - auROC: 0.9306 - val_loss: 0.3967 - val_acc: 0.8552 - val_auROC: 0.8272\n",
      "Epoch 76/1002\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3241 - acc: 0.9057 - auROC: 0.9295 - val_loss: 0.3971 - val_acc: 0.8621 - val_auROC: 0.8323\n",
      "Epoch 77/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3229 - acc: 0.9111 - auROC: 0.9333 - val_loss: 0.3954 - val_acc: 0.8552 - val_auROC: 0.8301\n",
      "Epoch 78/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3203 - acc: 0.9072 - auROC: 0.9324 - val_loss: 0.3918 - val_acc: 0.8621 - val_auROC: 0.8359\n",
      "Epoch 79/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.3194 - acc: 0.9080 - auROC: 0.9332 - val_loss: 0.3906 - val_acc: 0.8655 - val_auROC: 0.8429\n",
      "Epoch 80/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3184 - acc: 0.9099 - auROC: 0.9358 - val_loss: 0.3892 - val_acc: 0.8586 - val_auROC: 0.8346\n",
      "Epoch 81/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3165 - acc: 0.9072 - auROC: 0.9329 - val_loss: 0.3893 - val_acc: 0.8552 - val_auROC: 0.8352\n",
      "Epoch 82/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3151 - acc: 0.9088 - auROC: 0.9334 - val_loss: 0.3896 - val_acc: 0.8724 - val_auROC: 0.8394\n",
      "Epoch 83/1002\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3150 - acc: 0.9127 - auROC: 0.9360 - val_loss: 0.3893 - val_acc: 0.8621 - val_auROC: 0.8371\n",
      "Epoch 84/1002\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3139 - acc: 0.9099 - auROC: 0.9346 - val_loss: 0.3878 - val_acc: 0.8621 - val_auROC: 0.8393\n",
      "Epoch 85/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3119 - acc: 0.9111 - auROC: 0.9364 - val_loss: 0.3887 - val_acc: 0.8759 - val_auROC: 0.8381\n",
      "Epoch 86/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3113 - acc: 0.9131 - auROC: 0.9368 - val_loss: 0.3882 - val_acc: 0.8586 - val_auROC: 0.8334\n",
      "Epoch 87/1002\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3096 - acc: 0.9103 - auROC: 0.9358 - val_loss: 0.3869 - val_acc: 0.8655 - val_auROC: 0.8346\n",
      "Epoch 88/1002\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.3080 - acc: 0.9142 - auROC: 0.9376 - val_loss: 0.3861 - val_acc: 0.8793 - val_auROC: 0.8370\n",
      "Epoch 89/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3071 - acc: 0.9216 - auROC: 0.9397 - val_loss: 0.3867 - val_acc: 0.8724 - val_auROC: 0.8315\n",
      "Epoch 90/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3050 - acc: 0.9158 - auROC: 0.9378 - val_loss: 0.3837 - val_acc: 0.8828 - val_auROC: 0.8381\n",
      "Epoch 91/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3040 - acc: 0.9216 - auROC: 0.9372 - val_loss: 0.3812 - val_acc: 0.8897 - val_auROC: 0.8495\n",
      "Epoch 92/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3020 - acc: 0.9279 - auROC: 0.9401 - val_loss: 0.3816 - val_acc: 0.8897 - val_auROC: 0.8434\n",
      "Epoch 93/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3011 - acc: 0.9279 - auROC: 0.9387 - val_loss: 0.3792 - val_acc: 0.8897 - val_auROC: 0.8440\n",
      "Epoch 94/1002\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2994 - acc: 0.9302 - auROC: 0.9388 - val_loss: 0.3776 - val_acc: 0.8931 - val_auROC: 0.8488\n",
      "Epoch 95/1002\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2984 - acc: 0.9294 - auROC: 0.9408 - val_loss: 0.3779 - val_acc: 0.8793 - val_auROC: 0.8439\n",
      "Epoch 96/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2979 - acc: 0.9279 - auROC: 0.9383 - val_loss: 0.3819 - val_acc: 0.8793 - val_auROC: 0.8417\n",
      "Epoch 97/1002\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2962 - acc: 0.9310 - auROC: 0.9399 - val_loss: 0.3839 - val_acc: 0.8793 - val_auROC: 0.8383\n",
      "Epoch 98/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2951 - acc: 0.9314 - auROC: 0.9410 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8350\n",
      "Epoch 99/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2959 - acc: 0.9318 - auROC: 0.9394\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2959 - acc: 0.9318 - auROC: 0.9394 - val_loss: 0.3854 - val_acc: 0.8828 - val_auROC: 0.8364\n",
      "Epoch 100/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2943 - acc: 0.9329 - auROC: 0.9415 - val_loss: 0.3854 - val_acc: 0.8828 - val_auROC: 0.8356\n",
      "Epoch 101/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2937 - acc: 0.9333 - auROC: 0.9418 - val_loss: 0.3848 - val_acc: 0.8828 - val_auROC: 0.8356\n",
      "Epoch 102/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2930 - acc: 0.9329 - auROC: 0.9419 - val_loss: 0.3840 - val_acc: 0.8828 - val_auROC: 0.8364\n",
      "Epoch 103/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2925 - acc: 0.9326 - auROC: 0.9419 - val_loss: 0.3848 - val_acc: 0.8828 - val_auROC: 0.8338\n",
      "Epoch 104/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2923 - acc: 0.9329 - auROC: 0.9413\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2923 - acc: 0.9329 - auROC: 0.9413 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8319\n",
      "Epoch 105/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2922 - acc: 0.9329 - auROC: 0.9409 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8319\n",
      "Epoch 106/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2922 - acc: 0.9329 - auROC: 0.9409 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8320\n",
      "Epoch 107/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2922 - acc: 0.9329 - auROC: 0.9409 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8320\n",
      "Epoch 108/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2922 - acc: 0.9326 - auROC: 0.9409 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8320\n",
      "Epoch 109/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2922 - acc: 0.9326 - auROC: 0.9409\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2922 - acc: 0.9326 - auROC: 0.9409 - val_loss: 0.3852 - val_acc: 0.8862 - val_auROC: 0.8318\n",
      "Epoch 00109: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.364832  ...    0.0 -0.199372 -0.197762    0.0\n",
      "1     0.0    0.0    0.0 -0.207833  ...    0.0 -0.199372 -0.197762    0.0\n",
      "2     0.0    0.0    0.0 -0.330822  ...    0.0 -0.199372 -0.197762    0.0\n",
      "3     0.0    0.0    0.0 -0.364268  ...    0.0  2.631308  2.667556    0.0\n",
      "4     0.0    0.0    0.0  1.188823  ...    0.0 -0.199372 -0.197762    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0 -0.265638  ...    0.0 -0.199372 -0.197762    0.0\n",
      "60    0.0    0.0    0.0 -0.364832  ...    0.0 -0.199372 -0.197762    0.0\n",
      "61    0.0    0.0    0.0 -0.364832  ...    0.0 -0.199372 -0.197762    0.0\n",
      "62    0.0    0.0    0.0 -0.364832  ...    0.0 -0.199372 -0.197762    0.0\n",
      "63    0.0    0.0    0.0 -0.364832  ...    0.0 -0.199372 -0.197762    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878    0.932  0.8085\n",
      "0.01   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878    0.932  0.8085\n",
      "0.02   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878    0.932  0.8085\n",
      "0.03   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878    0.932  0.8085\n",
      "0.04   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878    0.932  0.8085\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN    0.932  0.8085\n",
      "0.98  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN    0.932  0.8085\n",
      "0.99  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN    0.932  0.8085\n",
      "1.00  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN    0.932  0.8085\n",
      "1.01  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN    0.932  0.8085\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  48   0  14  0.2258  1.0  ...  1.0  1.0  0.2258  0.3684    0.982  0.963\n",
      "0.01   0  48   0  14  0.2258  1.0  ...  1.0  1.0  0.2258  0.3684    0.982  0.963\n",
      "0.02   0  48   0  14  0.2258  1.0  ...  1.0  1.0  0.2258  0.3684    0.982  0.963\n",
      "0.03   0  48   0  14  0.2258  1.0  ...  1.0  1.0  0.2258  0.3684    0.982  0.963\n",
      "0.04   0  48   0  14  0.2258  1.0  ...  1.0  1.0  0.2258  0.3684    0.982  0.963\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  48   0  14   0  0.7742  0.0  ...  0.0  0.0  0.0000     NaN    0.982  0.963\n",
      "0.98  48   0  14   0  0.7742  0.0  ...  0.0  0.0  0.0000     NaN    0.982  0.963\n",
      "0.99  48   0  14   0  0.7742  0.0  ...  0.0  0.0  0.0000     NaN    0.982  0.963\n",
      "1.00  48   0  14   0  0.7742  0.0  ...  0.0  0.0  0.0000     NaN    0.982  0.963\n",
      "1.01  48   0  14   0  0.7742  0.0  ...  0.0  0.0  0.0000     NaN    0.982  0.963\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765     0.88  0.8889\n",
      "0.01   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765     0.88  0.8889\n",
      "0.02   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765     0.88  0.8889\n",
      "0.03   7  48   0   6  0.2131  1.0  ...  0.8727  1.0  0.1111  0.2000     0.88  0.8889\n",
      "0.04  14  41   0   6  0.3279  1.0  ...  0.7455  1.0  0.1277  0.2264     0.88  0.8889\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN     0.88  0.8889\n",
      "0.98  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN     0.88  0.8889\n",
      "0.99  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN     0.88  0.8889\n",
      "1.00  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN     0.88  0.8889\n",
      "1.01  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN     0.88  0.8889\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9888  0.9444\n",
      "0.01   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9888  0.9444\n",
      "0.02   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9888  0.9444\n",
      "0.03   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9888  0.9444\n",
      "0.04   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9888  0.9444\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9888  0.9444\n",
      "0.98  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9888  0.9444\n",
      "0.99  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9888  0.9444\n",
      "1.00  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9888  0.9444\n",
      "1.01  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9888  0.9444\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "Total correct samples: 571?571\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015062  0.041284\n",
      "4      0.015041  0.041268\n",
      "...         ...       ...\n",
      "18013  0.000061  0.000243\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002382  0.011947\n",
      "18016  0.002317  0.011715\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.6981 - acc: 0.5797 - auROC: 0.4507 - val_loss: 0.6006 - val_acc: 0.6586 - val_auROC: 0.5480\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.5981 - acc: 0.6803 - auROC: 0.5276 - val_loss: 0.5534 - val_acc: 0.7241 - val_auROC: 0.5964\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.5471 - acc: 0.7474 - auROC: 0.6079 - val_loss: 0.5179 - val_acc: 0.7793 - val_auROC: 0.6923\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.5269 - acc: 0.7708 - auROC: 0.6520 - val_loss: 0.4984 - val_acc: 0.7966 - val_auROC: 0.7096\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5182 - acc: 0.7801 - auROC: 0.6441 - val_loss: 0.5027 - val_acc: 0.7759 - val_auROC: 0.6680\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.4993 - acc: 0.7903 - auROC: 0.6873 - val_loss: 0.4908 - val_acc: 0.7724 - val_auROC: 0.7085\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4781 - acc: 0.7899 - auROC: 0.7397 - val_loss: 0.4576 - val_acc: 0.8069 - val_auROC: 0.7842\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4574 - acc: 0.8066 - auROC: 0.7805 - val_loss: 0.4377 - val_acc: 0.8103 - val_auROC: 0.8158\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4400 - acc: 0.8105 - auROC: 0.8045 - val_loss: 0.4257 - val_acc: 0.8241 - val_auROC: 0.8241\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4271 - acc: 0.8265 - auROC: 0.8150 - val_loss: 0.4185 - val_acc: 0.8207 - val_auROC: 0.8190\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4187 - acc: 0.8304 - auROC: 0.8332 - val_loss: 0.4048 - val_acc: 0.8379 - val_auROC: 0.8475\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4190 - acc: 0.8253 - auROC: 0.8287 - val_loss: 0.3923 - val_acc: 0.8310 - val_auROC: 0.8631\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4053 - acc: 0.8288 - auROC: 0.8421 - val_loss: 0.3894 - val_acc: 0.8414 - val_auROC: 0.8682\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3990 - acc: 0.8394 - auROC: 0.8493 - val_loss: 0.3810 - val_acc: 0.8483 - val_auROC: 0.8732\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3890 - acc: 0.8491 - auROC: 0.8618 - val_loss: 0.3744 - val_acc: 0.8552 - val_auROC: 0.8758\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3783 - acc: 0.8515 - auROC: 0.8751 - val_loss: 0.3623 - val_acc: 0.8517 - val_auROC: 0.8902\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3715 - acc: 0.8577 - auROC: 0.8794 - val_loss: 0.3597 - val_acc: 0.8517 - val_auROC: 0.8892\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3721 - acc: 0.8577 - auROC: 0.8750 - val_loss: 0.3470 - val_acc: 0.8586 - val_auROC: 0.9196\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3636 - acc: 0.8565 - auROC: 0.8856 - val_loss: 0.3465 - val_acc: 0.8655 - val_auROC: 0.9080\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3621 - acc: 0.8569 - auROC: 0.8868 - val_loss: 0.3668 - val_acc: 0.8552 - val_auROC: 0.8730\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3642 - acc: 0.8616 - auROC: 0.8834 - val_loss: 0.3503 - val_acc: 0.8552 - val_auROC: 0.8968\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3524 - acc: 0.8624 - auROC: 0.8932 - val_loss: 0.3397 - val_acc: 0.8690 - val_auROC: 0.9095\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3475 - acc: 0.8620 - auROC: 0.8963 - val_loss: 0.3396 - val_acc: 0.8655 - val_auROC: 0.9114\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3430 - acc: 0.8678 - auROC: 0.9029 - val_loss: 0.3259 - val_acc: 0.8759 - val_auROC: 0.9246\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3371 - acc: 0.8710 - auROC: 0.9085 - val_loss: 0.3230 - val_acc: 0.8759 - val_auROC: 0.9376\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3345 - acc: 0.8698 - auROC: 0.9117 - val_loss: 0.3287 - val_acc: 0.8724 - val_auROC: 0.9138\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3379 - acc: 0.8686 - auROC: 0.9005 - val_loss: 0.3371 - val_acc: 0.8724 - val_auROC: 0.8986\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3416 - acc: 0.8577 - auROC: 0.8936 - val_loss: 0.3436 - val_acc: 0.8759 - val_auROC: 0.8751\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3317 - acc: 0.8710 - auROC: 0.9021 - val_loss: 0.3278 - val_acc: 0.8828 - val_auROC: 0.8983\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3234 - acc: 0.8795 - auROC: 0.9111 - val_loss: 0.3123 - val_acc: 0.8966 - val_auROC: 0.9158\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3133 - acc: 0.8920 - auROC: 0.9237 - val_loss: 0.3030 - val_acc: 0.8966 - val_auROC: 0.9327\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3060 - acc: 0.8920 - auROC: 0.9311 - val_loss: 0.2975 - val_acc: 0.9000 - val_auROC: 0.9352\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3018 - acc: 0.9002 - auROC: 0.9340 - val_loss: 0.2936 - val_acc: 0.9000 - val_auROC: 0.9408\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2963 - acc: 0.9018 - auROC: 0.9398 - val_loss: 0.2786 - val_acc: 0.9103 - val_auROC: 0.9567\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2895 - acc: 0.9096 - auROC: 0.9452 - val_loss: 0.2764 - val_acc: 0.9138 - val_auROC: 0.9559\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2845 - acc: 0.9103 - auROC: 0.9467 - val_loss: 0.2683 - val_acc: 0.9207 - val_auROC: 0.9601\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2783 - acc: 0.9142 - auROC: 0.9526 - val_loss: 0.2642 - val_acc: 0.9241 - val_auROC: 0.9634\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2766 - acc: 0.9162 - auROC: 0.9534 - val_loss: 0.2618 - val_acc: 0.9345 - val_auROC: 0.9679\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2740 - acc: 0.9119 - auROC: 0.9536 - val_loss: 0.2591 - val_acc: 0.9241 - val_auROC: 0.9689\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2722 - acc: 0.9142 - auROC: 0.9526 - val_loss: 0.2635 - val_acc: 0.9138 - val_auROC: 0.9598\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2817 - acc: 0.9049 - auROC: 0.9419 - val_loss: 0.2652 - val_acc: 0.9138 - val_auROC: 0.9628\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2691 - acc: 0.9181 - auROC: 0.9530 - val_loss: 0.2830 - val_acc: 0.8966 - val_auROC: 0.9464\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3079 - acc: 0.8955 - auROC: 0.9097 - val_loss: 0.2889 - val_acc: 0.8966 - val_auROC: 0.9358\n",
      "Epoch 44/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2863 - acc: 0.9012 - auROC: 0.9363\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2861 - acc: 0.9014 - auROC: 0.9365 - val_loss: 0.2616 - val_acc: 0.9138 - val_auROC: 0.9581\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2696 - acc: 0.9127 - auROC: 0.9493 - val_loss: 0.2533 - val_acc: 0.9207 - val_auROC: 0.9657\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2643 - acc: 0.9158 - auROC: 0.9537 - val_loss: 0.2500 - val_acc: 0.9207 - val_auROC: 0.9682\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2617 - acc: 0.9177 - auROC: 0.9568 - val_loss: 0.2504 - val_acc: 0.9172 - val_auROC: 0.9702\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2586 - acc: 0.9197 - auROC: 0.9595 - val_loss: 0.2388 - val_acc: 0.9241 - val_auROC: 0.9781\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2550 - acc: 0.9240 - auROC: 0.9604 - val_loss: 0.2330 - val_acc: 0.9310 - val_auROC: 0.9807\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2560 - acc: 0.9209 - auROC: 0.9609 - val_loss: 0.2435 - val_acc: 0.9241 - val_auROC: 0.9722\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2589 - acc: 0.9189 - auROC: 0.9583 - val_loss: 0.2362 - val_acc: 0.9345 - val_auROC: 0.9761\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2536 - acc: 0.9244 - auROC: 0.9632 - val_loss: 0.2268 - val_acc: 0.9414 - val_auROC: 0.9821\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2481 - acc: 0.9290 - auROC: 0.9670 - val_loss: 0.2240 - val_acc: 0.9414 - val_auROC: 0.9841\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2466 - acc: 0.9318 - auROC: 0.9674 - val_loss: 0.2236 - val_acc: 0.9483 - val_auROC: 0.9839\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2449 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2231 - val_acc: 0.9483 - val_auROC: 0.9840\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2437 - acc: 0.9329 - auROC: 0.9689 - val_loss: 0.2227 - val_acc: 0.9483 - val_auROC: 0.9844\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2427 - acc: 0.9318 - auROC: 0.9694 - val_loss: 0.2218 - val_acc: 0.9483 - val_auROC: 0.9847\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2418 - acc: 0.9337 - auROC: 0.9699 - val_loss: 0.2215 - val_acc: 0.9483 - val_auROC: 0.9843\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2411 - acc: 0.9337 - auROC: 0.9704 - val_loss: 0.2200 - val_acc: 0.9483 - val_auROC: 0.9851\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2404 - acc: 0.9333 - auROC: 0.9708 - val_loss: 0.2190 - val_acc: 0.9448 - val_auROC: 0.9858\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2397 - acc: 0.9337 - auROC: 0.9708 - val_loss: 0.2197 - val_acc: 0.9448 - val_auROC: 0.9852\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2391 - acc: 0.9345 - auROC: 0.9709 - val_loss: 0.2204 - val_acc: 0.9483 - val_auROC: 0.9841\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2383 - acc: 0.9349 - auROC: 0.9715 - val_loss: 0.2179 - val_acc: 0.9483 - val_auROC: 0.9855\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2378 - acc: 0.9345 - auROC: 0.9718 - val_loss: 0.2163 - val_acc: 0.9483 - val_auROC: 0.9865\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2373 - acc: 0.9337 - auROC: 0.9723 - val_loss: 0.2160 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2363 - acc: 0.9345 - auROC: 0.9730 - val_loss: 0.2155 - val_acc: 0.9448 - val_auROC: 0.9867\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2353 - acc: 0.9349 - auROC: 0.9741 - val_loss: 0.2156 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2344 - acc: 0.9353 - auROC: 0.9747 - val_loss: 0.2167 - val_acc: 0.9448 - val_auROC: 0.9852\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2336 - acc: 0.9357 - auROC: 0.9747 - val_loss: 0.2162 - val_acc: 0.9448 - val_auROC: 0.9854\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2329 - acc: 0.9372 - auROC: 0.9749 - val_loss: 0.2162 - val_acc: 0.9448 - val_auROC: 0.9854\n",
      "Epoch 71/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2312 - acc: 0.9406 - auROC: 0.9750\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2326 - acc: 0.9388 - auROC: 0.9747 - val_loss: 0.2157 - val_acc: 0.9448 - val_auROC: 0.9860\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2320 - acc: 0.9388 - auROC: 0.9749 - val_loss: 0.2158 - val_acc: 0.9448 - val_auROC: 0.9855\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2319 - acc: 0.9392 - auROC: 0.9750 - val_loss: 0.2160 - val_acc: 0.9448 - val_auROC: 0.9855\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2318 - acc: 0.9384 - auROC: 0.9752 - val_loss: 0.2160 - val_acc: 0.9448 - val_auROC: 0.9854\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2316 - acc: 0.9388 - auROC: 0.9753 - val_loss: 0.2158 - val_acc: 0.9448 - val_auROC: 0.9855\n",
      "Epoch 76/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2315 - acc: 0.9387 - auROC: 0.9753\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2315 - acc: 0.9388 - auROC: 0.9753 - val_loss: 0.2157 - val_acc: 0.9448 - val_auROC: 0.9855\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2313 - acc: 0.9388 - auROC: 0.9754 - val_loss: 0.2156 - val_acc: 0.9448 - val_auROC: 0.9856\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2312 - acc: 0.9388 - auROC: 0.9754 - val_loss: 0.2154 - val_acc: 0.9448 - val_auROC: 0.9856\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2311 - acc: 0.9384 - auROC: 0.9754 - val_loss: 0.2153 - val_acc: 0.9448 - val_auROC: 0.9856\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2310 - acc: 0.9384 - auROC: 0.9755 - val_loss: 0.2151 - val_acc: 0.9448 - val_auROC: 0.9856\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2310 - acc: 0.9384 - auROC: 0.9756 - val_loss: 0.2149 - val_acc: 0.9448 - val_auROC: 0.9858\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2309 - acc: 0.9384 - auROC: 0.9756 - val_loss: 0.2148 - val_acc: 0.9448 - val_auROC: 0.9860\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2308 - acc: 0.9384 - auROC: 0.9756 - val_loss: 0.2147 - val_acc: 0.9448 - val_auROC: 0.9860\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2308 - acc: 0.9384 - auROC: 0.9756 - val_loss: 0.2148 - val_acc: 0.9448 - val_auROC: 0.9857\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2307 - acc: 0.9384 - auROC: 0.9757 - val_loss: 0.2147 - val_acc: 0.9448 - val_auROC: 0.9857\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2307 - acc: 0.9384 - auROC: 0.9757 - val_loss: 0.2145 - val_acc: 0.9448 - val_auROC: 0.9861\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2306 - acc: 0.9384 - auROC: 0.9757 - val_loss: 0.2144 - val_acc: 0.9448 - val_auROC: 0.9862\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2305 - acc: 0.9384 - auROC: 0.9758 - val_loss: 0.2141 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2304 - acc: 0.9384 - auROC: 0.9758 - val_loss: 0.2140 - val_acc: 0.9448 - val_auROC: 0.9864\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2304 - acc: 0.9384 - auROC: 0.9758 - val_loss: 0.2139 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2303 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2137 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2302 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2135 - val_acc: 0.9448 - val_auROC: 0.9866\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2302 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2135 - val_acc: 0.9448 - val_auROC: 0.9864\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2301 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9866\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2301 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9866\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2300 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9866\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2300 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9864\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2299 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9864\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2299 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9863\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2298 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2133 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2298 - acc: 0.9384 - auROC: 0.9759 - val_loss: 0.2132 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2297 - acc: 0.9384 - auROC: 0.9760 - val_loss: 0.2132 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2296 - acc: 0.9384 - auROC: 0.9760 - val_loss: 0.2131 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2296 - acc: 0.9384 - auROC: 0.9760 - val_loss: 0.2132 - val_acc: 0.9448 - val_auROC: 0.9865\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2296 - acc: 0.9384 - auROC: 0.9760 - val_loss: 0.2131 - val_acc: 0.9448 - val_auROC: 0.9866\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2296 - acc: 0.9388 - auROC: 0.9760 - val_loss: 0.2130 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2295 - acc: 0.9388 - auROC: 0.9760 - val_loss: 0.2128 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2294 - acc: 0.9388 - auROC: 0.9760 - val_loss: 0.2128 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2294 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2127 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2293 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2126 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2293 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2127 - val_acc: 0.9448 - val_auROC: 0.9868\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2292 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2133 - val_acc: 0.9448 - val_auROC: 0.9860\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2292 - acc: 0.9384 - auROC: 0.9761 - val_loss: 0.2134 - val_acc: 0.9448 - val_auROC: 0.9857\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2292 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2133 - val_acc: 0.9448 - val_auROC: 0.9859\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2291 - acc: 0.9392 - auROC: 0.9761 - val_loss: 0.2130 - val_acc: 0.9448 - val_auROC: 0.9863\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2290 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2127 - val_acc: 0.9448 - val_auROC: 0.9864\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2291 - acc: 0.9388 - auROC: 0.9761 - val_loss: 0.2123 - val_acc: 0.9483 - val_auROC: 0.9861\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2290 - acc: 0.9388 - auROC: 0.9762 - val_loss: 0.2122 - val_acc: 0.9483 - val_auROC: 0.9863\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2290 - acc: 0.9388 - auROC: 0.9762 - val_loss: 0.2121 - val_acc: 0.9483 - val_auROC: 0.9863\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2290 - acc: 0.9388 - auROC: 0.9763 - val_loss: 0.2121 - val_acc: 0.9483 - val_auROC: 0.9861\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2289 - acc: 0.9388 - auROC: 0.9763 - val_loss: 0.2119 - val_acc: 0.9483 - val_auROC: 0.9864\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2288 - acc: 0.9384 - auROC: 0.9762 - val_loss: 0.2114 - val_acc: 0.9517 - val_auROC: 0.9865\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2288 - acc: 0.9384 - auROC: 0.9762 - val_loss: 0.2113 - val_acc: 0.9517 - val_auROC: 0.9867\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2288 - acc: 0.9384 - auROC: 0.9762 - val_loss: 0.2114 - val_acc: 0.9517 - val_auROC: 0.9867\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2287 - acc: 0.9384 - auROC: 0.9762 - val_loss: 0.2114 - val_acc: 0.9517 - val_auROC: 0.9866\n",
      "Epoch 126/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2287 - acc: 0.9384 - auROC: 0.9763 - val_loss: 0.2112 - val_acc: 0.9517 - val_auROC: 0.9866\n",
      "Epoch 127/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2286 - acc: 0.9384 - auROC: 0.9763 - val_loss: 0.2113 - val_acc: 0.9517 - val_auROC: 0.9866\n",
      "Epoch 128/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2285 - acc: 0.9384 - auROC: 0.9763 - val_loss: 0.2113 - val_acc: 0.9517 - val_auROC: 0.9866\n",
      "Epoch 129/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2285 - acc: 0.9384 - auROC: 0.9763 - val_loss: 0.2113 - val_acc: 0.9517 - val_auROC: 0.9864\n",
      "Epoch 130/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2284 - acc: 0.9384 - auROC: 0.9764 - val_loss: 0.2112 - val_acc: 0.9517 - val_auROC: 0.9865\n",
      "Epoch 131/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2284 - acc: 0.9384 - auROC: 0.9764 - val_loss: 0.2111 - val_acc: 0.9517 - val_auROC: 0.9866\n",
      "Epoch 132/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2283 - acc: 0.9384 - auROC: 0.9765 - val_loss: 0.2112 - val_acc: 0.9517 - val_auROC: 0.9868\n",
      "Epoch 133/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2283 - acc: 0.9384 - auROC: 0.9765 - val_loss: 0.2122 - val_acc: 0.9483 - val_auROC: 0.9860\n",
      "Epoch 134/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2287 - acc: 0.9380 - auROC: 0.9764 - val_loss: 0.2124 - val_acc: 0.9483 - val_auROC: 0.9858\n",
      "Epoch 135/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2286 - acc: 0.9380 - auROC: 0.9762 - val_loss: 0.2120 - val_acc: 0.9517 - val_auROC: 0.9860\n",
      "Epoch 136/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2284 - acc: 0.9380 - auROC: 0.9762 - val_loss: 0.2116 - val_acc: 0.9517 - val_auROC: 0.9862\n",
      "Epoch 137/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2282 - acc: 0.9380 - auROC: 0.9763 - val_loss: 0.2112 - val_acc: 0.9517 - val_auROC: 0.9869\n",
      "Epoch 138/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2281 - acc: 0.9380 - auROC: 0.9764 - val_loss: 0.2110 - val_acc: 0.9517 - val_auROC: 0.9868\n",
      "Epoch 139/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2280 - acc: 0.9380 - auROC: 0.9764 - val_loss: 0.2109 - val_acc: 0.9517 - val_auROC: 0.9869\n",
      "Epoch 140/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2279 - acc: 0.9380 - auROC: 0.9764 - val_loss: 0.2108 - val_acc: 0.9517 - val_auROC: 0.9869\n",
      "Epoch 141/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2279 - acc: 0.9384 - auROC: 0.9763 - val_loss: 0.2106 - val_acc: 0.9483 - val_auROC: 0.9871\n",
      "Epoch 142/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2278 - acc: 0.9384 - auROC: 0.9763 - val_loss: 0.2105 - val_acc: 0.9483 - val_auROC: 0.9871\n",
      "Epoch 143/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2278 - acc: 0.9388 - auROC: 0.9763 - val_loss: 0.2103 - val_acc: 0.9483 - val_auROC: 0.9873\n",
      "Epoch 144/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2277 - acc: 0.9388 - auROC: 0.9764 - val_loss: 0.2102 - val_acc: 0.9483 - val_auROC: 0.9873\n",
      "Epoch 145/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2276 - acc: 0.9388 - auROC: 0.9764 - val_loss: 0.2100 - val_acc: 0.9517 - val_auROC: 0.9873\n",
      "Epoch 146/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2276 - acc: 0.9388 - auROC: 0.9764 - val_loss: 0.2098 - val_acc: 0.9517 - val_auROC: 0.9874\n",
      "Epoch 147/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2275 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2100 - val_acc: 0.9517 - val_auROC: 0.9875\n",
      "Epoch 148/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2274 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2101 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 149/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2274 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2102 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 150/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2274 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2102 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 151/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2273 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2101 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 152/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2273 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2101 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 153/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2272 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2101 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 154/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2272 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2100 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 155/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2271 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2098 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 156/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2270 - acc: 0.9392 - auROC: 0.9764 - val_loss: 0.2097 - val_acc: 0.9483 - val_auROC: 0.9876\n",
      "Epoch 157/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2270 - acc: 0.9396 - auROC: 0.9764 - val_loss: 0.2097 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 158/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2270 - acc: 0.9396 - auROC: 0.9765 - val_loss: 0.2096 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 159/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2269 - acc: 0.9392 - auROC: 0.9765 - val_loss: 0.2096 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 160/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2268 - acc: 0.9392 - auROC: 0.9765 - val_loss: 0.2096 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 161/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2268 - acc: 0.9392 - auROC: 0.9766 - val_loss: 0.2096 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 162/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2267 - acc: 0.9392 - auROC: 0.9766 - val_loss: 0.2092 - val_acc: 0.9483 - val_auROC: 0.9877\n",
      "Epoch 163/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2269 - acc: 0.9392 - auROC: 0.9765 - val_loss: 0.2080 - val_acc: 0.9483 - val_auROC: 0.9884\n",
      "Epoch 164/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2271 - acc: 0.9392 - auROC: 0.9766 - val_loss: 0.2079 - val_acc: 0.9517 - val_auROC: 0.9884\n",
      "Epoch 165/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2270 - acc: 0.9392 - auROC: 0.9766 - val_loss: 0.2079 - val_acc: 0.9448 - val_auROC: 0.9883\n",
      "Epoch 166/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2268 - acc: 0.9392 - auROC: 0.9768 - val_loss: 0.2080 - val_acc: 0.9448 - val_auROC: 0.9883\n",
      "Epoch 167/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2267 - acc: 0.9392 - auROC: 0.9768 - val_loss: 0.2081 - val_acc: 0.9448 - val_auROC: 0.9883\n",
      "Epoch 168/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2266 - acc: 0.9396 - auROC: 0.9768 - val_loss: 0.2083 - val_acc: 0.9448 - val_auROC: 0.9882\n",
      "Epoch 169/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2265 - acc: 0.9396 - auROC: 0.9768 - val_loss: 0.2084 - val_acc: 0.9448 - val_auROC: 0.9881\n",
      "Epoch 170/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2264 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9448 - val_auROC: 0.9881\n",
      "Epoch 171/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2263 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9448 - val_auROC: 0.9881\n",
      "Epoch 172/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2262 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2087 - val_acc: 0.9448 - val_auROC: 0.9880\n",
      "Epoch 173/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2262 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9448 - val_auROC: 0.9880\n",
      "Epoch 174/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2261 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9448 - val_auROC: 0.9880\n",
      "Epoch 175/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2261 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9483 - val_auROC: 0.9880\n",
      "Epoch 176/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2260 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9483 - val_auROC: 0.9878\n",
      "Epoch 177/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2260 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2086 - val_acc: 0.9483 - val_auROC: 0.9879\n",
      "Epoch 178/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2260 - acc: 0.9396 - auROC: 0.9769 - val_loss: 0.2085 - val_acc: 0.9483 - val_auROC: 0.9880\n",
      "Epoch 179/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2260 - acc: 0.9395 - auROC: 0.9769Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2259 - acc: 0.9396 - auROC: 0.9770 - val_loss: 0.2084 - val_acc: 0.9448 - val_auROC: 0.9880\n",
      "Epoch 00179: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 21,795\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 179/478\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 0.2269 - acc: 0.9384 - auROC: 0.9766 - val_loss: 0.2092 - val_acc: 0.9517 - val_auROC: 0.9873\n",
      "Epoch 180/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2255 - acc: 0.9388 - auROC: 0.9770 - val_loss: 0.2086 - val_acc: 0.9448 - val_auROC: 0.9876\n",
      "Epoch 181/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2243 - acc: 0.9392 - auROC: 0.9775 - val_loss: 0.2061 - val_acc: 0.9517 - val_auROC: 0.9886\n",
      "Epoch 182/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2234 - acc: 0.9392 - auROC: 0.9778 - val_loss: 0.2061 - val_acc: 0.9483 - val_auROC: 0.9881\n",
      "Epoch 183/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2226 - acc: 0.9392 - auROC: 0.9782 - val_loss: 0.2053 - val_acc: 0.9517 - val_auROC: 0.9887\n",
      "Epoch 184/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2217 - acc: 0.9400 - auROC: 0.9784 - val_loss: 0.2047 - val_acc: 0.9517 - val_auROC: 0.9889\n",
      "Epoch 185/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2210 - acc: 0.9404 - auROC: 0.9788 - val_loss: 0.2039 - val_acc: 0.9517 - val_auROC: 0.9890\n",
      "Epoch 186/478\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2204 - acc: 0.9411 - auROC: 0.9790 - val_loss: 0.2033 - val_acc: 0.9552 - val_auROC: 0.9892\n",
      "Epoch 187/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2199 - acc: 0.9407 - auROC: 0.9791 - val_loss: 0.2032 - val_acc: 0.9552 - val_auROC: 0.9890\n",
      "Epoch 188/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2191 - acc: 0.9407 - auROC: 0.9794 - val_loss: 0.2026 - val_acc: 0.9517 - val_auROC: 0.9890\n",
      "Epoch 189/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2186 - acc: 0.9415 - auROC: 0.9795 - val_loss: 0.2026 - val_acc: 0.9517 - val_auROC: 0.9894\n",
      "Epoch 190/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2180 - acc: 0.9427 - auROC: 0.9797 - val_loss: 0.2020 - val_acc: 0.9552 - val_auROC: 0.9895\n",
      "Epoch 191/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2175 - acc: 0.9427 - auROC: 0.9799 - val_loss: 0.2015 - val_acc: 0.9552 - val_auROC: 0.9895\n",
      "Epoch 192/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2170 - acc: 0.9427 - auROC: 0.9800 - val_loss: 0.2012 - val_acc: 0.9517 - val_auROC: 0.9895\n",
      "Epoch 193/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2165 - acc: 0.9431 - auROC: 0.9802 - val_loss: 0.2013 - val_acc: 0.9517 - val_auROC: 0.9896\n",
      "Epoch 194/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2166 - acc: 0.9423 - auROC: 0.9802 - val_loss: 0.2053 - val_acc: 0.9517 - val_auROC: 0.9871\n",
      "Epoch 195/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2167 - acc: 0.9419 - auROC: 0.9801 - val_loss: 0.2033 - val_acc: 0.9517 - val_auROC: 0.9881\n",
      "Epoch 196/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2154 - acc: 0.9423 - auROC: 0.9808 - val_loss: 0.2010 - val_acc: 0.9517 - val_auROC: 0.9891\n",
      "Epoch 197/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2146 - acc: 0.9435 - auROC: 0.9809 - val_loss: 0.1996 - val_acc: 0.9552 - val_auROC: 0.9897\n",
      "Epoch 198/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2142 - acc: 0.9439 - auROC: 0.9811 - val_loss: 0.1986 - val_acc: 0.9586 - val_auROC: 0.9899\n",
      "Epoch 199/478\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2138 - acc: 0.9446 - auROC: 0.9812 - val_loss: 0.1981 - val_acc: 0.9586 - val_auROC: 0.9900\n",
      "Epoch 200/478\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2134 - acc: 0.9450 - auROC: 0.9814 - val_loss: 0.1975 - val_acc: 0.9586 - val_auROC: 0.9902\n",
      "Epoch 201/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2131 - acc: 0.9454 - auROC: 0.9814 - val_loss: 0.1971 - val_acc: 0.9586 - val_auROC: 0.9903\n",
      "Epoch 202/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2128 - acc: 0.9454 - auROC: 0.9814 - val_loss: 0.1967 - val_acc: 0.9621 - val_auROC: 0.9903\n",
      "Epoch 203/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2126 - acc: 0.9458 - auROC: 0.9815 - val_loss: 0.1962 - val_acc: 0.9621 - val_auROC: 0.9905\n",
      "Epoch 204/478\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2123 - acc: 0.9458 - auROC: 0.9816 - val_loss: 0.1959 - val_acc: 0.9621 - val_auROC: 0.9905\n",
      "Epoch 205/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2121 - acc: 0.9458 - auROC: 0.9816 - val_loss: 0.1957 - val_acc: 0.9621 - val_auROC: 0.9906\n",
      "Epoch 206/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2119 - acc: 0.9462 - auROC: 0.9816 - val_loss: 0.1954 - val_acc: 0.9621 - val_auROC: 0.9906\n",
      "Epoch 207/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2117 - acc: 0.9462 - auROC: 0.9817 - val_loss: 0.1951 - val_acc: 0.9621 - val_auROC: 0.9906\n",
      "Epoch 208/478\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2115 - acc: 0.9462 - auROC: 0.9818 - val_loss: 0.1949 - val_acc: 0.9621 - val_auROC: 0.9906\n",
      "Epoch 209/478\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2114 - acc: 0.9462 - auROC: 0.9818 - val_loss: 0.1951 - val_acc: 0.9621 - val_auROC: 0.9905\n",
      "Epoch 210/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2112 - acc: 0.9462 - auROC: 0.9818 - val_loss: 0.1944 - val_acc: 0.9621 - val_auROC: 0.9905\n",
      "Epoch 211/478\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2109 - acc: 0.9462 - auROC: 0.9819 - val_loss: 0.1940 - val_acc: 0.9621 - val_auROC: 0.9907\n",
      "Epoch 212/478\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2106 - acc: 0.9466 - auROC: 0.9818 - val_loss: 0.1939 - val_acc: 0.9621 - val_auROC: 0.9910\n",
      "Epoch 213/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2104 - acc: 0.9466 - auROC: 0.9819 - val_loss: 0.1937 - val_acc: 0.9621 - val_auROC: 0.9909\n",
      "Epoch 214/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2101 - acc: 0.9466 - auROC: 0.9819 - val_loss: 0.1935 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 215/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2099 - acc: 0.9466 - auROC: 0.9821 - val_loss: 0.1934 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 216/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2095 - acc: 0.9466 - auROC: 0.9826 - val_loss: 0.1932 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 217/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2093 - acc: 0.9462 - auROC: 0.9829 - val_loss: 0.1931 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 218/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2089 - acc: 0.9466 - auROC: 0.9831 - val_loss: 0.1929 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 219/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2086 - acc: 0.9466 - auROC: 0.9833 - val_loss: 0.1929 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 220/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2082 - acc: 0.9466 - auROC: 0.9836 - val_loss: 0.1927 - val_acc: 0.9552 - val_auROC: 0.9910\n",
      "Epoch 221/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2081 - acc: 0.9462 - auROC: 0.9836 - val_loss: 0.1929 - val_acc: 0.9552 - val_auROC: 0.9908\n",
      "Epoch 222/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2079 - acc: 0.9466 - auROC: 0.9838 - val_loss: 0.1924 - val_acc: 0.9552 - val_auROC: 0.9910\n",
      "Epoch 223/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2076 - acc: 0.9466 - auROC: 0.9838 - val_loss: 0.1922 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 224/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2074 - acc: 0.9470 - auROC: 0.9840 - val_loss: 0.1919 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 225/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2074 - acc: 0.9470 - auROC: 0.9841 - val_loss: 0.1916 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 226/478\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2071 - acc: 0.9470 - auROC: 0.9841 - val_loss: 0.1916 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 227/478\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2068 - acc: 0.9474 - auROC: 0.9842 - val_loss: 0.1916 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 228/478\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2066 - acc: 0.9474 - auROC: 0.9842 - val_loss: 0.1913 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 229/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2064 - acc: 0.9478 - auROC: 0.9843 - val_loss: 0.1912 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 230/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2062 - acc: 0.9478 - auROC: 0.9843 - val_loss: 0.1910 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 231/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2060 - acc: 0.9478 - auROC: 0.9844 - val_loss: 0.1907 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 232/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2058 - acc: 0.9478 - auROC: 0.9844 - val_loss: 0.1905 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 233/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2056 - acc: 0.9478 - auROC: 0.9844 - val_loss: 0.1903 - val_acc: 0.9586 - val_auROC: 0.9913\n",
      "Epoch 234/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2054 - acc: 0.9478 - auROC: 0.9843 - val_loss: 0.1901 - val_acc: 0.9586 - val_auROC: 0.9913\n",
      "Epoch 235/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2053 - acc: 0.9478 - auROC: 0.9844 - val_loss: 0.1900 - val_acc: 0.9586 - val_auROC: 0.9912\n",
      "Epoch 236/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2051 - acc: 0.9478 - auROC: 0.9844 - val_loss: 0.1900 - val_acc: 0.9552 - val_auROC: 0.9911\n",
      "Epoch 237/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2052 - acc: 0.9478 - auROC: 0.9843 - val_loss: 0.1907 - val_acc: 0.9552 - val_auROC: 0.9909\n",
      "Epoch 238/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2053 - acc: 0.9481 - auROC: 0.9842 - val_loss: 0.1900 - val_acc: 0.9552 - val_auROC: 0.9912\n",
      "Epoch 239/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2052 - acc: 0.9478 - auROC: 0.9845 - val_loss: 0.1943 - val_acc: 0.9552 - val_auROC: 0.9901\n",
      "Epoch 240/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2057 - acc: 0.9481 - auROC: 0.9844 - val_loss: 0.1952 - val_acc: 0.9552 - val_auROC: 0.9896\n",
      "Epoch 241/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2055 - acc: 0.9481 - auROC: 0.9844 - val_loss: 0.1949 - val_acc: 0.9552 - val_auROC: 0.9897\n",
      "Epoch 242/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2050 - acc: 0.9481 - auROC: 0.9845 - val_loss: 0.1942 - val_acc: 0.9552 - val_auROC: 0.9897\n",
      "Epoch 243/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2047 - acc: 0.9481 - auROC: 0.9843 - val_loss: 0.1933 - val_acc: 0.9552 - val_auROC: 0.9903\n",
      "Epoch 244/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2044 - acc: 0.9481 - auROC: 0.9843 - val_loss: 0.1924 - val_acc: 0.9552 - val_auROC: 0.9906\n",
      "Epoch 245/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2042 - acc: 0.9481 - auROC: 0.9843 - val_loss: 0.1915 - val_acc: 0.9552 - val_auROC: 0.9908\n",
      "Epoch 246/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2040 - acc: 0.9481 - auROC: 0.9844 - val_loss: 0.1904 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 247/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2038 - acc: 0.9481 - auROC: 0.9845 - val_loss: 0.1896 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 248/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2036 - acc: 0.9481 - auROC: 0.9845 - val_loss: 0.1891 - val_acc: 0.9586 - val_auROC: 0.9912\n",
      "Epoch 249/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2034 - acc: 0.9481 - auROC: 0.9846 - val_loss: 0.1887 - val_acc: 0.9586 - val_auROC: 0.9913\n",
      "Epoch 250/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2034 - acc: 0.9485 - auROC: 0.9846 - val_loss: 0.1901 - val_acc: 0.9586 - val_auROC: 0.9912\n",
      "Epoch 251/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2038 - acc: 0.9485 - auROC: 0.9844 - val_loss: 0.1901 - val_acc: 0.9586 - val_auROC: 0.9912\n",
      "Epoch 252/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2035 - acc: 0.9485 - auROC: 0.9846 - val_loss: 0.1895 - val_acc: 0.9586 - val_auROC: 0.9914\n",
      "Epoch 253/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2032 - acc: 0.9485 - auROC: 0.9846 - val_loss: 0.1892 - val_acc: 0.9586 - val_auROC: 0.9914\n",
      "Epoch 254/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2030 - acc: 0.9485 - auROC: 0.9846 - val_loss: 0.1889 - val_acc: 0.9586 - val_auROC: 0.9915\n",
      "Epoch 255/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2028 - acc: 0.9485 - auROC: 0.9847 - val_loss: 0.1886 - val_acc: 0.9586 - val_auROC: 0.9915\n",
      "Epoch 256/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2027 - acc: 0.9485 - auROC: 0.9847 - val_loss: 0.1884 - val_acc: 0.9586 - val_auROC: 0.9915\n",
      "Epoch 257/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2027 - acc: 0.9485 - auROC: 0.9848 - val_loss: 0.1882 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 258/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2026 - acc: 0.9485 - auROC: 0.9847 - val_loss: 0.1882 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 259/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2024 - acc: 0.9485 - auROC: 0.9848 - val_loss: 0.1881 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 260/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2023 - acc: 0.9485 - auROC: 0.9848 - val_loss: 0.1879 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 261/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2022 - acc: 0.9485 - auROC: 0.9848 - val_loss: 0.1878 - val_acc: 0.9586 - val_auROC: 0.9915\n",
      "Epoch 262/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2022 - acc: 0.9485 - auROC: 0.9848 - val_loss: 0.1880 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 263/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2018 - acc: 0.9485 - auROC: 0.9848 - val_loss: 0.1886 - val_acc: 0.9586 - val_auROC: 0.9904\n",
      "Epoch 264/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2014 - acc: 0.9489 - auROC: 0.9850 - val_loss: 0.1889 - val_acc: 0.9586 - val_auROC: 0.9902\n",
      "Epoch 265/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2012 - acc: 0.9489 - auROC: 0.9850 - val_loss: 0.1891 - val_acc: 0.9586 - val_auROC: 0.9902\n",
      "Epoch 266/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2009 - acc: 0.9493 - auROC: 0.9852 - val_loss: 0.1894 - val_acc: 0.9586 - val_auROC: 0.9897\n",
      "Epoch 267/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2008 - acc: 0.9493 - auROC: 0.9852 - val_loss: 0.1892 - val_acc: 0.9586 - val_auROC: 0.9897\n",
      "Epoch 268/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2006 - acc: 0.9493 - auROC: 0.9852 - val_loss: 0.1886 - val_acc: 0.9586 - val_auROC: 0.9902\n",
      "Epoch 269/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2004 - acc: 0.9493 - auROC: 0.9853 - val_loss: 0.1882 - val_acc: 0.9586 - val_auROC: 0.9904\n",
      "Epoch 270/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2001 - acc: 0.9497 - auROC: 0.9853 - val_loss: 0.1879 - val_acc: 0.9586 - val_auROC: 0.9906\n",
      "Epoch 271/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1999 - acc: 0.9505 - auROC: 0.9854 - val_loss: 0.1878 - val_acc: 0.9586 - val_auROC: 0.9906\n",
      "Epoch 272/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1995 - acc: 0.9509 - auROC: 0.9854 - val_loss: 0.1879 - val_acc: 0.9586 - val_auROC: 0.9906\n",
      "Epoch 273/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1992 - acc: 0.9509 - auROC: 0.9856 - val_loss: 0.1879 - val_acc: 0.9552 - val_auROC: 0.9906\n",
      "Epoch 274/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1989 - acc: 0.9509 - auROC: 0.9857 - val_loss: 0.1877 - val_acc: 0.9552 - val_auROC: 0.9906\n",
      "Epoch 275/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1988 - acc: 0.9509 - auROC: 0.9857 - val_loss: 0.1876 - val_acc: 0.9586 - val_auROC: 0.9907\n",
      "Epoch 276/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1987 - acc: 0.9509 - auROC: 0.9857 - val_loss: 0.1874 - val_acc: 0.9586 - val_auROC: 0.9907\n",
      "Epoch 277/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1986 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1873 - val_acc: 0.9586 - val_auROC: 0.9907\n",
      "Epoch 278/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1985 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1873 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 279/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1984 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1874 - val_acc: 0.9586 - val_auROC: 0.9906\n",
      "Epoch 280/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1983 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1871 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 281/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1983 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1870 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 282/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1984 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1866 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 283/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1983 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1864 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 284/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1981 - acc: 0.9509 - auROC: 0.9859 - val_loss: 0.1863 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 285/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1980 - acc: 0.9509 - auROC: 0.9859 - val_loss: 0.1864 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 286/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1979 - acc: 0.9509 - auROC: 0.9859 - val_loss: 0.1863 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 287/478\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1979 - acc: 0.9509 - auROC: 0.9859 - val_loss: 0.1863 - val_acc: 0.9586 - val_auROC: 0.9913\n",
      "Epoch 288/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1978 - acc: 0.9509 - auROC: 0.9858 - val_loss: 0.1861 - val_acc: 0.9586 - val_auROC: 0.9912\n",
      "Epoch 289/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1977 - acc: 0.9509 - auROC: 0.9859 - val_loss: 0.1861 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 290/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1976 - acc: 0.9509 - auROC: 0.9860 - val_loss: 0.1862 - val_acc: 0.9586 - val_auROC: 0.9908\n",
      "Epoch 291/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1975 - acc: 0.9509 - auROC: 0.9860 - val_loss: 0.1863 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 292/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1974 - acc: 0.9509 - auROC: 0.9859 - val_loss: 0.1862 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 293/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1973 - acc: 0.9509 - auROC: 0.9860 - val_loss: 0.1860 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 294/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1972 - acc: 0.9509 - auROC: 0.9860 - val_loss: 0.1859 - val_acc: 0.9586 - val_auROC: 0.9906\n",
      "Epoch 295/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1973 - acc: 0.9509 - auROC: 0.9860 - val_loss: 0.1860 - val_acc: 0.9586 - val_auROC: 0.9907\n",
      "Epoch 296/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1974 - acc: 0.9509 - auROC: 0.9860 - val_loss: 0.1856 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 297/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1971 - acc: 0.9513 - auROC: 0.9861 - val_loss: 0.1855 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 298/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1970 - acc: 0.9513 - auROC: 0.9862 - val_loss: 0.1857 - val_acc: 0.9586 - val_auROC: 0.9910\n",
      "Epoch 299/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1968 - acc: 0.9513 - auROC: 0.9861 - val_loss: 0.1856 - val_acc: 0.9586 - val_auROC: 0.9909\n",
      "Epoch 300/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1967 - acc: 0.9513 - auROC: 0.9862 - val_loss: 0.1854 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 301/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1985 - acc: 0.9497 - auROC: 0.9857 - val_loss: 0.1900 - val_acc: 0.9586 - val_auROC: 0.9887\n",
      "Epoch 302/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1974 - acc: 0.9513 - auROC: 0.9859 - val_loss: 0.1869 - val_acc: 0.9586 - val_auROC: 0.9902\n",
      "Epoch 303/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1959 - acc: 0.9524 - auROC: 0.9863 - val_loss: 0.1861 - val_acc: 0.9621 - val_auROC: 0.9905\n",
      "Epoch 304/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1956 - acc: 0.9536 - auROC: 0.9864 - val_loss: 0.1853 - val_acc: 0.9621 - val_auROC: 0.9910\n",
      "Epoch 305/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1952 - acc: 0.9536 - auROC: 0.9865 - val_loss: 0.1849 - val_acc: 0.9586 - val_auROC: 0.9913\n",
      "Epoch 306/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1949 - acc: 0.9540 - auROC: 0.9865 - val_loss: 0.1849 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 307/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1948 - acc: 0.9540 - auROC: 0.9866 - val_loss: 0.1849 - val_acc: 0.9586 - val_auROC: 0.9915\n",
      "Epoch 308/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1947 - acc: 0.9540 - auROC: 0.9866 - val_loss: 0.1847 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 309/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1946 - acc: 0.9540 - auROC: 0.9866 - val_loss: 0.1846 - val_acc: 0.9586 - val_auROC: 0.9919\n",
      "Epoch 310/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1945 - acc: 0.9540 - auROC: 0.9866 - val_loss: 0.1845 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 311/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1944 - acc: 0.9540 - auROC: 0.9866 - val_loss: 0.1844 - val_acc: 0.9586 - val_auROC: 0.9918\n",
      "Epoch 312/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1943 - acc: 0.9540 - auROC: 0.9867 - val_loss: 0.1843 - val_acc: 0.9586 - val_auROC: 0.9919\n",
      "Epoch 313/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1943 - acc: 0.9540 - auROC: 0.9867 - val_loss: 0.1841 - val_acc: 0.9586 - val_auROC: 0.9919\n",
      "Epoch 314/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1942 - acc: 0.9540 - auROC: 0.9867 - val_loss: 0.1839 - val_acc: 0.9586 - val_auROC: 0.9921\n",
      "Epoch 315/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1940 - acc: 0.9540 - auROC: 0.9867 - val_loss: 0.1836 - val_acc: 0.9586 - val_auROC: 0.9920\n",
      "Epoch 316/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1939 - acc: 0.9540 - auROC: 0.9868 - val_loss: 0.1833 - val_acc: 0.9586 - val_auROC: 0.9918\n",
      "Epoch 317/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1938 - acc: 0.9540 - auROC: 0.9868 - val_loss: 0.1829 - val_acc: 0.9586 - val_auROC: 0.9917\n",
      "Epoch 318/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1936 - acc: 0.9544 - auROC: 0.9868 - val_loss: 0.1826 - val_acc: 0.9586 - val_auROC: 0.9918\n",
      "Epoch 319/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1934 - acc: 0.9548 - auROC: 0.9868 - val_loss: 0.1823 - val_acc: 0.9586 - val_auROC: 0.9919\n",
      "Epoch 320/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1932 - acc: 0.9548 - auROC: 0.9869 - val_loss: 0.1820 - val_acc: 0.9621 - val_auROC: 0.9921\n",
      "Epoch 321/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1930 - acc: 0.9556 - auROC: 0.9869 - val_loss: 0.1819 - val_acc: 0.9621 - val_auROC: 0.9920\n",
      "Epoch 322/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1929 - acc: 0.9556 - auROC: 0.9869 - val_loss: 0.1819 - val_acc: 0.9621 - val_auROC: 0.9919\n",
      "Epoch 323/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1928 - acc: 0.9556 - auROC: 0.9869 - val_loss: 0.1817 - val_acc: 0.9621 - val_auROC: 0.9920\n",
      "Epoch 324/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1927 - acc: 0.9556 - auROC: 0.9869 - val_loss: 0.1817 - val_acc: 0.9621 - val_auROC: 0.9921\n",
      "Epoch 325/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1926 - acc: 0.9552 - auROC: 0.9869 - val_loss: 0.1841 - val_acc: 0.9586 - val_auROC: 0.9911\n",
      "Epoch 326/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1926 - acc: 0.9548 - auROC: 0.9868 - val_loss: 0.1833 - val_acc: 0.9586 - val_auROC: 0.9916\n",
      "Epoch 327/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1923 - acc: 0.9552 - auROC: 0.9869 - val_loss: 0.1824 - val_acc: 0.9586 - val_auROC: 0.9922\n",
      "Epoch 328/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1921 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1817 - val_acc: 0.9621 - val_auROC: 0.9923\n",
      "Epoch 329/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1920 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1816 - val_acc: 0.9621 - val_auROC: 0.9925\n",
      "Epoch 330/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1920 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1819 - val_acc: 0.9621 - val_auROC: 0.9923\n",
      "Epoch 331/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1919 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1817 - val_acc: 0.9621 - val_auROC: 0.9923\n",
      "Epoch 332/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1918 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1812 - val_acc: 0.9621 - val_auROC: 0.9925\n",
      "Epoch 333/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1917 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1810 - val_acc: 0.9621 - val_auROC: 0.9926\n",
      "Epoch 334/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1916 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1807 - val_acc: 0.9621 - val_auROC: 0.9926\n",
      "Epoch 335/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1915 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1806 - val_acc: 0.9621 - val_auROC: 0.9925\n",
      "Epoch 336/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1915 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1811 - val_acc: 0.9586 - val_auROC: 0.9924\n",
      "Epoch 337/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1915 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1808 - val_acc: 0.9586 - val_auROC: 0.9925\n",
      "Epoch 338/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1913 - acc: 0.9556 - auROC: 0.9870 - val_loss: 0.1805 - val_acc: 0.9621 - val_auROC: 0.9926\n",
      "Epoch 339/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1912 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1804 - val_acc: 0.9621 - val_auROC: 0.9928\n",
      "Epoch 340/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1912 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1803 - val_acc: 0.9621 - val_auROC: 0.9926\n",
      "Epoch 341/478\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1911 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1802 - val_acc: 0.9621 - val_auROC: 0.9926\n",
      "Epoch 342/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1910 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1801 - val_acc: 0.9621 - val_auROC: 0.9928\n",
      "Epoch 343/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1910 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1800 - val_acc: 0.9621 - val_auROC: 0.9928\n",
      "Epoch 344/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1909 - acc: 0.9556 - auROC: 0.9871 - val_loss: 0.1798 - val_acc: 0.9621 - val_auROC: 0.9928\n",
      "Epoch 345/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1908 - acc: 0.9556 - auROC: 0.9872 - val_loss: 0.1798 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 346/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1908 - acc: 0.9556 - auROC: 0.9872 - val_loss: 0.1797 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 347/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1907 - acc: 0.9556 - auROC: 0.9872 - val_loss: 0.1796 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 348/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1906 - acc: 0.9563 - auROC: 0.9871 - val_loss: 0.1795 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 349/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1906 - acc: 0.9563 - auROC: 0.9871 - val_loss: 0.1793 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 350/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1905 - acc: 0.9563 - auROC: 0.9870 - val_loss: 0.1794 - val_acc: 0.9655 - val_auROC: 0.9925\n",
      "Epoch 351/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1905 - acc: 0.9563 - auROC: 0.9870 - val_loss: 0.1793 - val_acc: 0.9655 - val_auROC: 0.9925\n",
      "Epoch 352/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1904 - acc: 0.9563 - auROC: 0.9870 - val_loss: 0.1793 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 353/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1901 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1793 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 354/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1900 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1794 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 355/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1899 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1794 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 356/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1899 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1793 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 357/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1898 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1792 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 358/478\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1898 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1792 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 359/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1898 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1798 - val_acc: 0.9621 - val_auROC: 0.9928\n",
      "Epoch 360/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1898 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1795 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 361/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1897 - acc: 0.9567 - auROC: 0.9872 - val_loss: 0.1791 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 362/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1896 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1789 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 363/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1895 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1789 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 364/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1895 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1789 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 365/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1894 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1789 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 366/478\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.1894 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1790 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 367/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1893 - acc: 0.9567 - auROC: 0.9873 - val_loss: 0.1789 - val_acc: 0.9655 - val_auROC: 0.9927\n",
      "Epoch 368/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1893 - acc: 0.9567 - auROC: 0.9874 - val_loss: 0.1788 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 369/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1892 - acc: 0.9571 - auROC: 0.9874 - val_loss: 0.1787 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 370/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1891 - acc: 0.9571 - auROC: 0.9873 - val_loss: 0.1788 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 371/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1890 - acc: 0.9571 - auROC: 0.9873 - val_loss: 0.1787 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 372/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1889 - acc: 0.9571 - auROC: 0.9874 - val_loss: 0.1787 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 373/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1889 - acc: 0.9571 - auROC: 0.9874 - val_loss: 0.1787 - val_acc: 0.9655 - val_auROC: 0.9927\n",
      "Epoch 374/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1889 - acc: 0.9571 - auROC: 0.9875 - val_loss: 0.1793 - val_acc: 0.9621 - val_auROC: 0.9927\n",
      "Epoch 375/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1889 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1794 - val_acc: 0.9621 - val_auROC: 0.9930\n",
      "Epoch 376/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1890 - acc: 0.9575 - auROC: 0.9874 - val_loss: 0.1797 - val_acc: 0.9621 - val_auROC: 0.9923\n",
      "Epoch 377/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1888 - acc: 0.9575 - auROC: 0.9874 - val_loss: 0.1786 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 378/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1886 - acc: 0.9575 - auROC: 0.9874 - val_loss: 0.1782 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 379/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1886 - acc: 0.9575 - auROC: 0.9874 - val_loss: 0.1783 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 380/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1885 - acc: 0.9575 - auROC: 0.9874 - val_loss: 0.1783 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 381/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1883 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1783 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 382/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1882 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1782 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 383/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1881 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1782 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 384/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1881 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1782 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 385/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1880 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1781 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 386/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1879 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1780 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 387/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1879 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1779 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 388/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1878 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1778 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 389/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1878 - acc: 0.9575 - auROC: 0.9875 - val_loss: 0.1777 - val_acc: 0.9655 - val_auROC: 0.9928\n",
      "Epoch 390/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1877 - acc: 0.9579 - auROC: 0.9875 - val_loss: 0.1776 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 391/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1877 - acc: 0.9579 - auROC: 0.9875 - val_loss: 0.1776 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 392/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1876 - acc: 0.9579 - auROC: 0.9875 - val_loss: 0.1775 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 393/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1876 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1775 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 394/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1875 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1777 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 395/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1875 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1776 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 396/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1874 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1775 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 397/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1873 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1774 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 398/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1873 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1773 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 399/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1872 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1772 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 400/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1872 - acc: 0.9579 - auROC: 0.9877 - val_loss: 0.1772 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 401/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1872 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1771 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 402/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1871 - acc: 0.9579 - auROC: 0.9877 - val_loss: 0.1770 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 403/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1870 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1769 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 404/478\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1871 - acc: 0.9579 - auROC: 0.9875 - val_loss: 0.1772 - val_acc: 0.9655 - val_auROC: 0.9925\n",
      "Epoch 405/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1871 - acc: 0.9579 - auROC: 0.9874 - val_loss: 0.1771 - val_acc: 0.9655 - val_auROC: 0.9927\n",
      "Epoch 406/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1870 - acc: 0.9579 - auROC: 0.9876 - val_loss: 0.1771 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 407/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1869 - acc: 0.9579 - auROC: 0.9877 - val_loss: 0.1767 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 408/478\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1871 - acc: 0.9575 - auROC: 0.9876 - val_loss: 0.1775 - val_acc: 0.9621 - val_auROC: 0.9928\n",
      "Epoch 409/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1876 - acc: 0.9575 - auROC: 0.9874 - val_loss: 0.1776 - val_acc: 0.9655 - val_auROC: 0.9926\n",
      "Epoch 410/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1872 - acc: 0.9579 - auROC: 0.9875 - val_loss: 0.1771 - val_acc: 0.9690 - val_auROC: 0.9929\n",
      "Epoch 411/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1867 - acc: 0.9583 - auROC: 0.9876 - val_loss: 0.1770 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 412/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1865 - acc: 0.9583 - auROC: 0.9876 - val_loss: 0.1769 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 413/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1864 - acc: 0.9583 - auROC: 0.9877 - val_loss: 0.1769 - val_acc: 0.9690 - val_auROC: 0.9929\n",
      "Epoch 414/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1862 - acc: 0.9587 - auROC: 0.9876 - val_loss: 0.1768 - val_acc: 0.9690 - val_auROC: 0.9929\n",
      "Epoch 415/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1860 - acc: 0.9587 - auROC: 0.9877 - val_loss: 0.1767 - val_acc: 0.9690 - val_auROC: 0.9931\n",
      "Epoch 416/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1859 - acc: 0.9595 - auROC: 0.9877 - val_loss: 0.1767 - val_acc: 0.9690 - val_auROC: 0.9931\n",
      "Epoch 417/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1858 - acc: 0.9595 - auROC: 0.9877 - val_loss: 0.1767 - val_acc: 0.9690 - val_auROC: 0.9931\n",
      "Epoch 418/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1857 - acc: 0.9595 - auROC: 0.9877 - val_loss: 0.1767 - val_acc: 0.9690 - val_auROC: 0.9931\n",
      "Epoch 419/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1856 - acc: 0.9595 - auROC: 0.9877 - val_loss: 0.1768 - val_acc: 0.9690 - val_auROC: 0.9931\n",
      "Epoch 420/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1855 - acc: 0.9595 - auROC: 0.9878 - val_loss: 0.1771 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 421/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1853 - acc: 0.9598 - auROC: 0.9878 - val_loss: 0.1771 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 422/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1852 - acc: 0.9598 - auROC: 0.9878 - val_loss: 0.1769 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 423/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1852 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1768 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 424/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1851 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1767 - val_acc: 0.9655 - val_auROC: 0.9929\n",
      "Epoch 425/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1850 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1766 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 426/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1850 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1764 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 427/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1850 - acc: 0.9598 - auROC: 0.9878 - val_loss: 0.1761 - val_acc: 0.9690 - val_auROC: 0.9930\n",
      "Epoch 428/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1851 - acc: 0.9598 - auROC: 0.9878 - val_loss: 0.1762 - val_acc: 0.9690 - val_auROC: 0.9930\n",
      "Epoch 429/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1849 - acc: 0.9598 - auROC: 0.9878 - val_loss: 0.1763 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 430/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1848 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1766 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 431/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1847 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1766 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 432/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1847 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1765 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 433/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1846 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1762 - val_acc: 0.9655 - val_auROC: 0.9930\n",
      "Epoch 434/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1846 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1759 - val_acc: 0.9655 - val_auROC: 0.9932\n",
      "Epoch 435/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1845 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1758 - val_acc: 0.9655 - val_auROC: 0.9932\n",
      "Epoch 436/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1845 - acc: 0.9598 - auROC: 0.9879 - val_loss: 0.1758 - val_acc: 0.9655 - val_auROC: 0.9932\n",
      "Epoch 437/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1844 - acc: 0.9598 - auROC: 0.9880 - val_loss: 0.1756 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 438/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1843 - acc: 0.9602 - auROC: 0.9879 - val_loss: 0.1750 - val_acc: 0.9690 - val_auROC: 0.9932\n",
      "Epoch 439/478\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1843 - acc: 0.9602 - auROC: 0.9880 - val_loss: 0.1751 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 440/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1842 - acc: 0.9602 - auROC: 0.9880 - val_loss: 0.1755 - val_acc: 0.9655 - val_auROC: 0.9932\n",
      "Epoch 441/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1841 - acc: 0.9602 - auROC: 0.9881 - val_loss: 0.1757 - val_acc: 0.9655 - val_auROC: 0.9931\n",
      "Epoch 442/478\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1839 - acc: 0.9602 - auROC: 0.9881 - val_loss: 0.1755 - val_acc: 0.9655 - val_auROC: 0.9932\n",
      "Epoch 443/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1838 - acc: 0.9606 - auROC: 0.9881 - val_loss: 0.1753 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 444/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1837 - acc: 0.9606 - auROC: 0.9881 - val_loss: 0.1753 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 445/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1837 - acc: 0.9606 - auROC: 0.9881 - val_loss: 0.1753 - val_acc: 0.9655 - val_auROC: 0.9932\n",
      "Epoch 446/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1835 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1752 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 447/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1834 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1751 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 448/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1834 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1749 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 449/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1833 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1748 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 450/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1833 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1748 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 451/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1833 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1747 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 452/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1832 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1748 - val_acc: 0.9655 - val_auROC: 0.9934\n",
      "Epoch 453/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1832 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1748 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 454/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1832 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1747 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 455/478\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1831 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1745 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 456/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1831 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1745 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 457/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1830 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1744 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 458/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1830 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1743 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 459/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1830 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1743 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 460/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1829 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1743 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 461/478\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1829 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1742 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 462/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1829 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1742 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 463/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1828 - acc: 0.9610 - auROC: 0.9882 - val_loss: 0.1743 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 464/478\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1828 - acc: 0.9610 - auROC: 0.9883 - val_loss: 0.1744 - val_acc: 0.9655 - val_auROC: 0.9933\n",
      "Epoch 465/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1827 - acc: 0.9610 - auROC: 0.9883 - val_loss: 0.1742 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 466/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1827 - acc: 0.9610 - auROC: 0.9883 - val_loss: 0.1740 - val_acc: 0.9690 - val_auROC: 0.9934\n",
      "Epoch 467/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1827 - acc: 0.9610 - auROC: 0.9883 - val_loss: 0.1739 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 468/478\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1826 - acc: 0.9610 - auROC: 0.9884 - val_loss: 0.1738 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 469/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1823 - acc: 0.9610 - auROC: 0.9885 - val_loss: 0.1738 - val_acc: 0.9690 - val_auROC: 0.9933\n",
      "Epoch 470/478\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1821 - acc: 0.9610 - auROC: 0.9886 - val_loss: 0.1738 - val_acc: 0.9690 - val_auROC: 0.9934\n",
      "Epoch 471/478\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1819 - acc: 0.9610 - auROC: 0.9887 - val_loss: 0.1738 - val_acc: 0.9690 - val_auROC: 0.9934\n",
      "Epoch 472/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1818 - acc: 0.9610 - auROC: 0.9887 - val_loss: 0.1738 - val_acc: 0.9690 - val_auROC: 0.9934\n",
      "Epoch 473/478\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1817 - acc: 0.9610 - auROC: 0.9887 - val_loss: 0.1737 - val_acc: 0.9690 - val_auROC: 0.9934\n",
      "Epoch 474/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1816 - acc: 0.9610 - auROC: 0.9888 - val_loss: 0.1737 - val_acc: 0.9690 - val_auROC: 0.9935\n",
      "Epoch 475/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1815 - acc: 0.9610 - auROC: 0.9888 - val_loss: 0.1736 - val_acc: 0.9690 - val_auROC: 0.9935\n",
      "Epoch 476/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1814 - acc: 0.9610 - auROC: 0.9889 - val_loss: 0.1735 - val_acc: 0.9690 - val_auROC: 0.9936\n",
      "Epoch 477/478\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1812 - acc: 0.9610 - auROC: 0.9889 - val_loss: 0.1734 - val_acc: 0.9690 - val_auROC: 0.9935\n",
      "Epoch 478/478\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.1811 - acc: 0.9610 - auROC: 0.9889 - val_loss: 0.1733 - val_acc: 0.9690 - val_auROC: 0.9935\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "1     0.0    0.0    0.0 -0.207833  ...    0.0 -0.199372 -0.197763    0.0\n",
      "2     0.0    0.0    0.0 -0.330822  ...    0.0 -0.199372 -0.197763    0.0\n",
      "3     0.0    0.0    0.0 -0.364268  ...    0.0  2.631312  2.667564    0.0\n",
      "4     0.0    0.0    0.0  1.188823  ...    0.0 -0.199372 -0.197763    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0 -0.265638  ...    0.0 -0.199372 -0.197763    0.0\n",
      "60    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "61    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "62    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "63    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                  ...                                      \n",
      "0.00   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   1  60   0   0  0.0164  0.0  ...  0.9836  0.0  0.0 NaN      0.0    NaN\n",
      "0.02  13  48   0   0  0.2131  0.0  ...  0.7869  0.0  0.0 NaN      0.0    NaN\n",
      "0.03  26  35   0   0  0.4262  0.0  ...  0.5738  0.0  0.0 NaN      0.0    NaN\n",
      "0.04  42  19   0   0  0.6885  0.0  ...  0.3115  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...  ...  ..      ...    ...\n",
      "0.97  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878   0.9929  0.9744\n",
      "0.01   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878   0.9929  0.9744\n",
      "0.02   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878   0.9929  0.9744\n",
      "0.03   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878   0.9929  0.9744\n",
      "0.04   0  42   0  20  0.3226  1.0  ...  1.0  1.0  0.3226  0.4878   0.9929  0.9744\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN   0.9929  0.9744\n",
      "0.98  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN   0.9929  0.9744\n",
      "0.99  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN   0.9929  0.9744\n",
      "1.00  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN   0.9929  0.9744\n",
      "1.01  42   0  20   0  0.6774  0.0  ...  0.0  0.0  0.0000     NaN   0.9929  0.9744\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  48   0  14  0.2258  1.0  ...  1.0000  1.0  0.2258  0.3684   0.9755   0.88\n",
      "0.01   0  48   0  14  0.2258  1.0  ...  1.0000  1.0  0.2258  0.3684   0.9755   0.88\n",
      "0.02   0  48   0  14  0.2258  1.0  ...  1.0000  1.0  0.2258  0.3684   0.9755   0.88\n",
      "0.03   2  45   0  14  0.2623  1.0  ...  0.9574  1.0  0.2373  0.3836   0.9755   0.88\n",
      "0.04   2  45   0  13  0.2500  1.0  ...  0.9574  1.0  0.2241  0.3662   0.9755   0.88\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN   0.9755   0.88\n",
      "0.98  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN   0.9755   0.88\n",
      "0.99  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN   0.9755   0.88\n",
      "1.00  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN   0.9755   0.88\n",
      "1.01  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN   0.9755   0.88\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.01   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.02   0  55   0   6  0.0984  1.0  ...  1.0000  1.0  0.0984  0.1791      1.0    1.0\n",
      "0.03   7  48   0   6  0.2131  1.0  ...  0.8727  1.0  0.1111  0.2000      1.0    1.0\n",
      "0.04  11  44   0   6  0.2787  1.0  ...  0.8000  1.0  0.1200  0.2143      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  43   0  19  0.3065  1.0  ...  1.0000  1.0  0.3065  0.4691   0.9815  0.9091\n",
      "0.01   0  42   0  19  0.3115  1.0  ...  1.0000  1.0  0.3115  0.4750   0.9815  0.9091\n",
      "0.02   0  42   0  19  0.3115  1.0  ...  1.0000  1.0  0.3115  0.4750   0.9815  0.9091\n",
      "0.03   1  41   0  19  0.3279  1.0  ...  0.9762  1.0  0.3167  0.4810   0.9815  0.9091\n",
      "0.04   6  36   0  19  0.4098  1.0  ...  0.8571  1.0  0.3455  0.5135   0.9815  0.9091\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9815  0.9091\n",
      "0.98  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9815  0.9091\n",
      "0.99  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9815  0.9091\n",
      "1.00  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9815  0.9091\n",
      "1.01  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9815  0.9091\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "Total correct samples: 571?571\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015062  0.041284\n",
      "4      0.015041  0.041268\n",
      "...         ...       ...\n",
      "18013  0.000061  0.000243\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002382  0.011947\n",
      "18016  0.002317  0.011715\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.6633 - acc: 0.6175 - auROC: 0.5070 - val_loss: 0.5981 - val_acc: 0.7000 - val_auROC: 0.5849\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.5835 - acc: 0.6947 - auROC: 0.5788 - val_loss: 0.5659 - val_acc: 0.7000 - val_auROC: 0.5898\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.5467 - acc: 0.7458 - auROC: 0.6251 - val_loss: 0.5473 - val_acc: 0.7552 - val_auROC: 0.6221\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.5213 - acc: 0.7786 - auROC: 0.6666 - val_loss: 0.5373 - val_acc: 0.7552 - val_auROC: 0.5929\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4904 - acc: 0.7891 - auROC: 0.6953 - val_loss: 0.5213 - val_acc: 0.7828 - val_auROC: 0.6076\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.4719 - acc: 0.8218 - auROC: 0.7359 - val_loss: 0.4984 - val_acc: 0.7897 - val_auROC: 0.6615\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4554 - acc: 0.8152 - auROC: 0.7563 - val_loss: 0.4660 - val_acc: 0.8069 - val_auROC: 0.7433\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4450 - acc: 0.8168 - auROC: 0.7817 - val_loss: 0.4571 - val_acc: 0.8103 - val_auROC: 0.7286\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.4384 - acc: 0.8183 - auROC: 0.7808 - val_loss: 0.4486 - val_acc: 0.8207 - val_auROC: 0.7615\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4259 - acc: 0.8316 - auROC: 0.8152 - val_loss: 0.4427 - val_acc: 0.8241 - val_auROC: 0.7656\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4128 - acc: 0.8366 - auROC: 0.8341 - val_loss: 0.4371 - val_acc: 0.8103 - val_auROC: 0.7818\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4056 - acc: 0.8413 - auROC: 0.8423 - val_loss: 0.4377 - val_acc: 0.8103 - val_auROC: 0.7757\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3980 - acc: 0.8483 - auROC: 0.8525 - val_loss: 0.4342 - val_acc: 0.8103 - val_auROC: 0.7686\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3949 - acc: 0.8491 - auROC: 0.8437 - val_loss: 0.4356 - val_acc: 0.8034 - val_auROC: 0.7616\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4095 - acc: 0.8292 - auROC: 0.8228 - val_loss: 0.4376 - val_acc: 0.8345 - val_auROC: 0.7436\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.3955 - acc: 0.8339 - auROC: 0.8466 - val_loss: 0.4269 - val_acc: 0.8034 - val_auROC: 0.7794\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3892 - acc: 0.8483 - auROC: 0.8478 - val_loss: 0.4177 - val_acc: 0.8000 - val_auROC: 0.7967\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3729 - acc: 0.8503 - auROC: 0.8768 - val_loss: 0.4143 - val_acc: 0.8241 - val_auROC: 0.7850\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3786 - acc: 0.8394 - auROC: 0.8600 - val_loss: 0.4044 - val_acc: 0.8276 - val_auROC: 0.8032\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3670 - acc: 0.8534 - auROC: 0.8767 - val_loss: 0.3998 - val_acc: 0.8207 - val_auROC: 0.8147\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3604 - acc: 0.8585 - auROC: 0.8840 - val_loss: 0.3937 - val_acc: 0.8207 - val_auROC: 0.8352\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3545 - acc: 0.8710 - auROC: 0.8880 - val_loss: 0.3913 - val_acc: 0.8345 - val_auROC: 0.8235\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3461 - acc: 0.8784 - auROC: 0.9024 - val_loss: 0.3857 - val_acc: 0.8310 - val_auROC: 0.8442\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3397 - acc: 0.8834 - auROC: 0.9080 - val_loss: 0.3836 - val_acc: 0.8448 - val_auROC: 0.8369\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3410 - acc: 0.8721 - auROC: 0.9050 - val_loss: 0.3800 - val_acc: 0.8517 - val_auROC: 0.8451\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3363 - acc: 0.8819 - auROC: 0.9055 - val_loss: 0.3805 - val_acc: 0.8448 - val_auROC: 0.8296\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3244 - acc: 0.8904 - auROC: 0.9163 - val_loss: 0.3742 - val_acc: 0.8517 - val_auROC: 0.8505\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3201 - acc: 0.8932 - auROC: 0.9253 - val_loss: 0.3648 - val_acc: 0.8655 - val_auROC: 0.8560\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3127 - acc: 0.8982 - auROC: 0.9294 - val_loss: 0.3643 - val_acc: 0.8483 - val_auROC: 0.8615\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3104 - acc: 0.9025 - auROC: 0.9309 - val_loss: 0.3628 - val_acc: 0.8483 - val_auROC: 0.8619\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3028 - acc: 0.9053 - auROC: 0.9358 - val_loss: 0.3470 - val_acc: 0.8690 - val_auROC: 0.8796\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2976 - acc: 0.9084 - auROC: 0.9382 - val_loss: 0.3416 - val_acc: 0.8586 - val_auROC: 0.8844\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2913 - acc: 0.9103 - auROC: 0.9437 - val_loss: 0.3329 - val_acc: 0.8793 - val_auROC: 0.8924\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2885 - acc: 0.9115 - auROC: 0.9435 - val_loss: 0.3450 - val_acc: 0.8724 - val_auROC: 0.8758\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2908 - acc: 0.9162 - auROC: 0.9381 - val_loss: 0.3342 - val_acc: 0.8724 - val_auROC: 0.8849\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2768 - acc: 0.9244 - auROC: 0.9509 - val_loss: 0.3423 - val_acc: 0.8655 - val_auROC: 0.8785\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2774 - acc: 0.9279 - auROC: 0.9471 - val_loss: 0.3223 - val_acc: 0.8897 - val_auROC: 0.8900\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2778 - acc: 0.9263 - auROC: 0.9428 - val_loss: 0.3242 - val_acc: 0.8793 - val_auROC: 0.8955\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2772 - acc: 0.9279 - auROC: 0.9416 - val_loss: 0.3484 - val_acc: 0.8931 - val_auROC: 0.8541\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2731 - acc: 0.9193 - auROC: 0.9435 - val_loss: 0.3367 - val_acc: 0.8724 - val_auROC: 0.8730\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2894 - acc: 0.9103 - auROC: 0.9198 - val_loss: 0.3337 - val_acc: 0.8828 - val_auROC: 0.8801\n",
      "Epoch 42/300\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.2740 - acc: 0.9198 - auROC: 0.9357\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2727 - acc: 0.9170 - auROC: 0.9399 - val_loss: 0.3387 - val_acc: 0.8759 - val_auROC: 0.8600\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2682 - acc: 0.9173 - auROC: 0.9404 - val_loss: 0.3311 - val_acc: 0.8759 - val_auROC: 0.8707\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.2552 - acc: 0.9306 - auROC: 0.9539 - val_loss: 0.3220 - val_acc: 0.8862 - val_auROC: 0.8809\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.2509 - acc: 0.9415 - auROC: 0.9565 - val_loss: 0.3262 - val_acc: 0.8724 - val_auROC: 0.8812\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2527 - acc: 0.9384 - auROC: 0.9553 - val_loss: 0.3162 - val_acc: 0.8793 - val_auROC: 0.8924\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2501 - acc: 0.9431 - auROC: 0.9571 - val_loss: 0.3092 - val_acc: 0.9034 - val_auROC: 0.8992\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2460 - acc: 0.9466 - auROC: 0.9595 - val_loss: 0.3070 - val_acc: 0.8966 - val_auROC: 0.9024\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2430 - acc: 0.9478 - auROC: 0.9614 - val_loss: 0.3073 - val_acc: 0.8862 - val_auROC: 0.9015\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2410 - acc: 0.9481 - auROC: 0.9631 - val_loss: 0.3079 - val_acc: 0.8897 - val_auROC: 0.9009\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2399 - acc: 0.9505 - auROC: 0.9637 - val_loss: 0.3078 - val_acc: 0.8862 - val_auROC: 0.9009\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2389 - acc: 0.9497 - auROC: 0.9640 - val_loss: 0.3078 - val_acc: 0.8862 - val_auROC: 0.9008\n",
      "Epoch 53/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2375 - acc: 0.9512 - auROC: 0.9648\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2379 - acc: 0.9509 - auROC: 0.9645 - val_loss: 0.3080 - val_acc: 0.8897 - val_auROC: 0.8999\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2372 - acc: 0.9509 - auROC: 0.9651 - val_loss: 0.3084 - val_acc: 0.8897 - val_auROC: 0.8996\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2372 - acc: 0.9513 - auROC: 0.9651 - val_loss: 0.3085 - val_acc: 0.8897 - val_auROC: 0.8994\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2372 - acc: 0.9509 - auROC: 0.9652 - val_loss: 0.3091 - val_acc: 0.8931 - val_auROC: 0.9002\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2371 - acc: 0.9517 - auROC: 0.9651 - val_loss: 0.3090 - val_acc: 0.8931 - val_auROC: 0.9003\n",
      "Epoch 58/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2370 - acc: 0.9516 - auROC: 0.9652\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2370 - acc: 0.9517 - auROC: 0.9653 - val_loss: 0.3081 - val_acc: 0.8897 - val_auROC: 0.9009\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2368 - acc: 0.9509 - auROC: 0.9653 - val_loss: 0.3070 - val_acc: 0.8931 - val_auROC: 0.9023\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2366 - acc: 0.9509 - auROC: 0.9656 - val_loss: 0.3064 - val_acc: 0.8931 - val_auROC: 0.9040\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2365 - acc: 0.9505 - auROC: 0.9654 - val_loss: 0.3060 - val_acc: 0.8931 - val_auROC: 0.9045\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2365 - acc: 0.9509 - auROC: 0.9656 - val_loss: 0.3057 - val_acc: 0.8931 - val_auROC: 0.9046\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2364 - acc: 0.9517 - auROC: 0.9654 - val_loss: 0.3055 - val_acc: 0.8931 - val_auROC: 0.9045\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2363 - acc: 0.9517 - auROC: 0.9655 - val_loss: 0.3053 - val_acc: 0.8931 - val_auROC: 0.9044\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2362 - acc: 0.9513 - auROC: 0.9655 - val_loss: 0.3048 - val_acc: 0.8931 - val_auROC: 0.9041\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.2361 - acc: 0.9524 - auROC: 0.9656 - val_loss: 0.3050 - val_acc: 0.8931 - val_auROC: 0.9038\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2360 - acc: 0.9517 - auROC: 0.9657 - val_loss: 0.3048 - val_acc: 0.8931 - val_auROC: 0.9038\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2359 - acc: 0.9517 - auROC: 0.9656 - val_loss: 0.3047 - val_acc: 0.8931 - val_auROC: 0.9040\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2358 - acc: 0.9517 - auROC: 0.9655 - val_loss: 0.3045 - val_acc: 0.8931 - val_auROC: 0.9044\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2357 - acc: 0.9509 - auROC: 0.9656 - val_loss: 0.3037 - val_acc: 0.8931 - val_auROC: 0.9054\n",
      "Epoch 71/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2356 - acc: 0.9513 - auROC: 0.9657 - val_loss: 0.3036 - val_acc: 0.8931 - val_auROC: 0.9056\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2355 - acc: 0.9517 - auROC: 0.9658 - val_loss: 0.3035 - val_acc: 0.8931 - val_auROC: 0.9057\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2354 - acc: 0.9517 - auROC: 0.9657 - val_loss: 0.3035 - val_acc: 0.8931 - val_auROC: 0.9057\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2354 - acc: 0.9517 - auROC: 0.9658 - val_loss: 0.3038 - val_acc: 0.8931 - val_auROC: 0.9066\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2353 - acc: 0.9513 - auROC: 0.9658 - val_loss: 0.3041 - val_acc: 0.8931 - val_auROC: 0.9047\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2352 - acc: 0.9517 - auROC: 0.9659 - val_loss: 0.3040 - val_acc: 0.8931 - val_auROC: 0.9048\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2352 - acc: 0.9517 - auROC: 0.9659 - val_loss: 0.3039 - val_acc: 0.8931 - val_auROC: 0.9048\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2351 - acc: 0.9513 - auROC: 0.9659 - val_loss: 0.3031 - val_acc: 0.8897 - val_auROC: 0.9058\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2350 - acc: 0.9513 - auROC: 0.9659 - val_loss: 0.3029 - val_acc: 0.8931 - val_auROC: 0.9071\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2349 - acc: 0.9517 - auROC: 0.9660 - val_loss: 0.3030 - val_acc: 0.8931 - val_auROC: 0.9069\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2348 - acc: 0.9520 - auROC: 0.9662 - val_loss: 0.3030 - val_acc: 0.8931 - val_auROC: 0.9069\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2348 - acc: 0.9517 - auROC: 0.9661 - val_loss: 0.3029 - val_acc: 0.8966 - val_auROC: 0.9056\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2347 - acc: 0.9513 - auROC: 0.9660 - val_loss: 0.3030 - val_acc: 0.8966 - val_auROC: 0.9053\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2347 - acc: 0.9524 - auROC: 0.9662 - val_loss: 0.3031 - val_acc: 0.8931 - val_auROC: 0.9052\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2346 - acc: 0.9517 - auROC: 0.9662 - val_loss: 0.3030 - val_acc: 0.8931 - val_auROC: 0.9054\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2345 - acc: 0.9524 - auROC: 0.9662 - val_loss: 0.3029 - val_acc: 0.8966 - val_auROC: 0.9058\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2344 - acc: 0.9528 - auROC: 0.9663 - val_loss: 0.3028 - val_acc: 0.8966 - val_auROC: 0.9060\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2343 - acc: 0.9524 - auROC: 0.9663 - val_loss: 0.3028 - val_acc: 0.8966 - val_auROC: 0.9058\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2342 - acc: 0.9520 - auROC: 0.9664 - val_loss: 0.3028 - val_acc: 0.8966 - val_auROC: 0.9057\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2340 - acc: 0.9520 - auROC: 0.9664 - val_loss: 0.3025 - val_acc: 0.8966 - val_auROC: 0.9059\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2339 - acc: 0.9528 - auROC: 0.9665 - val_loss: 0.3017 - val_acc: 0.8966 - val_auROC: 0.9064\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2339 - acc: 0.9524 - auROC: 0.9665 - val_loss: 0.3013 - val_acc: 0.8966 - val_auROC: 0.9066\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.2338 - acc: 0.9528 - auROC: 0.9666 - val_loss: 0.3012 - val_acc: 0.8966 - val_auROC: 0.9067\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2337 - acc: 0.9528 - auROC: 0.9667 - val_loss: 0.3012 - val_acc: 0.8966 - val_auROC: 0.9068\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2336 - acc: 0.9532 - auROC: 0.9667 - val_loss: 0.3009 - val_acc: 0.9000 - val_auROC: 0.9073\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2335 - acc: 0.9536 - auROC: 0.9668 - val_loss: 0.3008 - val_acc: 0.9000 - val_auROC: 0.9074\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2333 - acc: 0.9536 - auROC: 0.9668 - val_loss: 0.3009 - val_acc: 0.9000 - val_auROC: 0.9074\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2332 - acc: 0.9536 - auROC: 0.9668 - val_loss: 0.3013 - val_acc: 0.8966 - val_auROC: 0.9071\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2331 - acc: 0.9528 - auROC: 0.9669 - val_loss: 0.3015 - val_acc: 0.8966 - val_auROC: 0.9073\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2330 - acc: 0.9532 - auROC: 0.9669 - val_loss: 0.3018 - val_acc: 0.8966 - val_auROC: 0.9068\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2329 - acc: 0.9532 - auROC: 0.9670 - val_loss: 0.3022 - val_acc: 0.8966 - val_auROC: 0.9057\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2328 - acc: 0.9536 - auROC: 0.9671 - val_loss: 0.3025 - val_acc: 0.8931 - val_auROC: 0.9071\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2327 - acc: 0.9548 - auROC: 0.9675 - val_loss: 0.3021 - val_acc: 0.8966 - val_auROC: 0.9073\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2326 - acc: 0.9544 - auROC: 0.9676 - val_loss: 0.3018 - val_acc: 0.8966 - val_auROC: 0.9074\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2326 - acc: 0.9544 - auROC: 0.9674 - val_loss: 0.3017 - val_acc: 0.8966 - val_auROC: 0.9074\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2325 - acc: 0.9544 - auROC: 0.9675 - val_loss: 0.3017 - val_acc: 0.8966 - val_auROC: 0.9078\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2324 - acc: 0.9544 - auROC: 0.9674 - val_loss: 0.3014 - val_acc: 0.9000 - val_auROC: 0.9077\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2323 - acc: 0.9552 - auROC: 0.9674 - val_loss: 0.3011 - val_acc: 0.9000 - val_auROC: 0.9080\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2323 - acc: 0.9548 - auROC: 0.9675 - val_loss: 0.3010 - val_acc: 0.9000 - val_auROC: 0.9080\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2322 - acc: 0.9548 - auROC: 0.9675 - val_loss: 0.3008 - val_acc: 0.9000 - val_auROC: 0.9084\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2321 - acc: 0.9544 - auROC: 0.9675 - val_loss: 0.3001 - val_acc: 0.9000 - val_auROC: 0.9091\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2321 - acc: 0.9548 - auROC: 0.9674 - val_loss: 0.3000 - val_acc: 0.9000 - val_auROC: 0.9091\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2320 - acc: 0.9544 - auROC: 0.9674 - val_loss: 0.3001 - val_acc: 0.9000 - val_auROC: 0.9090\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2320 - acc: 0.9548 - auROC: 0.9673 - val_loss: 0.3000 - val_acc: 0.9000 - val_auROC: 0.9091\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2321 - acc: 0.9552 - auROC: 0.9673 - val_loss: 0.3004 - val_acc: 0.9000 - val_auROC: 0.9090\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2321 - acc: 0.9552 - auROC: 0.9672 - val_loss: 0.3012 - val_acc: 0.8897 - val_auROC: 0.9084\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2320 - acc: 0.9559 - auROC: 0.9673 - val_loss: 0.3015 - val_acc: 0.8897 - val_auROC: 0.9084\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2319 - acc: 0.9559 - auROC: 0.9675 - val_loss: 0.3014 - val_acc: 0.8931 - val_auROC: 0.9093\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2317 - acc: 0.9556 - auROC: 0.9676 - val_loss: 0.3013 - val_acc: 0.8931 - val_auROC: 0.9080\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2315 - acc: 0.9556 - auROC: 0.9676 - val_loss: 0.3012 - val_acc: 0.8931 - val_auROC: 0.9080\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2314 - acc: 0.9556 - auROC: 0.9677 - val_loss: 0.3012 - val_acc: 0.8931 - val_auROC: 0.9078\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2313 - acc: 0.9556 - auROC: 0.9678 - val_loss: 0.3012 - val_acc: 0.8931 - val_auROC: 0.9082\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2313 - acc: 0.9556 - auROC: 0.9678 - val_loss: 0.3012 - val_acc: 0.8931 - val_auROC: 0.9082\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2312 - acc: 0.9559 - auROC: 0.9678 - val_loss: 0.3006 - val_acc: 0.9000 - val_auROC: 0.9089\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2311 - acc: 0.9548 - auROC: 0.9678 - val_loss: 0.3005 - val_acc: 0.9000 - val_auROC: 0.9091\n",
      "Epoch 126/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2310 - acc: 0.9552 - auROC: 0.9678 - val_loss: 0.3006 - val_acc: 0.9000 - val_auROC: 0.9091\n",
      "Epoch 127/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2315 - acc: 0.9554 - auROC: 0.9672Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.2309 - acc: 0.9552 - auROC: 0.9679 - val_loss: 0.3007 - val_acc: 0.9034 - val_auROC: 0.9087\n",
      "Epoch 00127: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 21,795\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 127/426\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 0.2323 - acc: 0.9544 - auROC: 0.9670 - val_loss: 0.3003 - val_acc: 0.9034 - val_auROC: 0.9083\n",
      "Epoch 128/426\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2274 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2994 - val_acc: 0.9103 - val_auROC: 0.9080\n",
      "Epoch 129/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2244 - acc: 0.9634 - auROC: 0.9712 - val_loss: 0.2982 - val_acc: 0.9103 - val_auROC: 0.9083\n",
      "Epoch 130/426\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2227 - acc: 0.9653 - auROC: 0.9714 - val_loss: 0.2986 - val_acc: 0.9103 - val_auROC: 0.9085\n",
      "Epoch 131/426\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2205 - acc: 0.9657 - auROC: 0.9728 - val_loss: 0.2957 - val_acc: 0.9138 - val_auROC: 0.9084\n",
      "Epoch 132/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2190 - acc: 0.9676 - auROC: 0.9733 - val_loss: 0.2935 - val_acc: 0.9172 - val_auROC: 0.9103\n",
      "Epoch 133/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2176 - acc: 0.9680 - auROC: 0.9739 - val_loss: 0.2940 - val_acc: 0.9103 - val_auROC: 0.9106\n",
      "Epoch 134/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2161 - acc: 0.9680 - auROC: 0.9746 - val_loss: 0.2930 - val_acc: 0.9138 - val_auROC: 0.9121\n",
      "Epoch 135/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2147 - acc: 0.9696 - auROC: 0.9752 - val_loss: 0.2899 - val_acc: 0.9207 - val_auROC: 0.9144\n",
      "Epoch 136/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2134 - acc: 0.9696 - auROC: 0.9755 - val_loss: 0.2890 - val_acc: 0.9138 - val_auROC: 0.9152\n",
      "Epoch 137/426\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2124 - acc: 0.9712 - auROC: 0.9757 - val_loss: 0.2885 - val_acc: 0.9103 - val_auROC: 0.9150\n",
      "Epoch 138/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2114 - acc: 0.9723 - auROC: 0.9760 - val_loss: 0.2870 - val_acc: 0.9172 - val_auROC: 0.9162\n",
      "Epoch 139/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2105 - acc: 0.9727 - auROC: 0.9760 - val_loss: 0.2867 - val_acc: 0.9138 - val_auROC: 0.9174\n",
      "Epoch 140/426\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2095 - acc: 0.9727 - auROC: 0.9762 - val_loss: 0.2870 - val_acc: 0.9172 - val_auROC: 0.9161\n",
      "Epoch 141/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2087 - acc: 0.9743 - auROC: 0.9766 - val_loss: 0.2861 - val_acc: 0.9207 - val_auROC: 0.9168\n",
      "Epoch 142/426\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.2081 - acc: 0.9754 - auROC: 0.9770 - val_loss: 0.2847 - val_acc: 0.9241 - val_auROC: 0.9162\n",
      "Epoch 143/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.2070 - acc: 0.9750 - auROC: 0.9769 - val_loss: 0.2833 - val_acc: 0.9276 - val_auROC: 0.9172\n",
      "Epoch 144/426\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2061 - acc: 0.9762 - auROC: 0.9768 - val_loss: 0.2834 - val_acc: 0.9276 - val_auROC: 0.9178\n",
      "Epoch 145/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2054 - acc: 0.9754 - auROC: 0.9771 - val_loss: 0.2819 - val_acc: 0.9276 - val_auROC: 0.9196\n",
      "Epoch 146/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2046 - acc: 0.9754 - auROC: 0.9773 - val_loss: 0.2820 - val_acc: 0.9276 - val_auROC: 0.9192\n",
      "Epoch 147/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2039 - acc: 0.9758 - auROC: 0.9772 - val_loss: 0.2815 - val_acc: 0.9276 - val_auROC: 0.9198\n",
      "Epoch 148/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2034 - acc: 0.9766 - auROC: 0.9776 - val_loss: 0.2798 - val_acc: 0.9276 - val_auROC: 0.9209\n",
      "Epoch 149/426\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2029 - acc: 0.9762 - auROC: 0.9777 - val_loss: 0.2801 - val_acc: 0.9241 - val_auROC: 0.9207\n",
      "Epoch 150/426\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2023 - acc: 0.9774 - auROC: 0.9776 - val_loss: 0.2800 - val_acc: 0.9241 - val_auROC: 0.9207\n",
      "Epoch 151/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2015 - acc: 0.9778 - auROC: 0.9779 - val_loss: 0.2793 - val_acc: 0.9241 - val_auROC: 0.9210\n",
      "Epoch 152/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2010 - acc: 0.9778 - auROC: 0.9779 - val_loss: 0.2793 - val_acc: 0.9241 - val_auROC: 0.9210\n",
      "Epoch 153/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2005 - acc: 0.9774 - auROC: 0.9780 - val_loss: 0.2779 - val_acc: 0.9276 - val_auROC: 0.9219\n",
      "Epoch 154/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1999 - acc: 0.9782 - auROC: 0.9782 - val_loss: 0.2769 - val_acc: 0.9276 - val_auROC: 0.9229\n",
      "Epoch 155/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1994 - acc: 0.9786 - auROC: 0.9784 - val_loss: 0.2761 - val_acc: 0.9310 - val_auROC: 0.9232\n",
      "Epoch 156/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1990 - acc: 0.9801 - auROC: 0.9785 - val_loss: 0.2753 - val_acc: 0.9310 - val_auROC: 0.9233\n",
      "Epoch 157/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1989 - acc: 0.9789 - auROC: 0.9784 - val_loss: 0.2751 - val_acc: 0.9276 - val_auROC: 0.9249\n",
      "Epoch 158/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1986 - acc: 0.9805 - auROC: 0.9787 - val_loss: 0.2741 - val_acc: 0.9310 - val_auROC: 0.9235\n",
      "Epoch 159/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1980 - acc: 0.9809 - auROC: 0.9788 - val_loss: 0.2736 - val_acc: 0.9310 - val_auROC: 0.9242\n",
      "Epoch 160/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1976 - acc: 0.9813 - auROC: 0.9790 - val_loss: 0.2731 - val_acc: 0.9310 - val_auROC: 0.9264\n",
      "Epoch 161/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1971 - acc: 0.9821 - auROC: 0.9790 - val_loss: 0.2733 - val_acc: 0.9276 - val_auROC: 0.9264\n",
      "Epoch 162/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1973 - acc: 0.9821 - auROC: 0.9790 - val_loss: 0.2730 - val_acc: 0.9310 - val_auROC: 0.9246\n",
      "Epoch 163/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1974 - acc: 0.9832 - auROC: 0.9789 - val_loss: 0.2724 - val_acc: 0.9310 - val_auROC: 0.9249\n",
      "Epoch 164/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1967 - acc: 0.9821 - auROC: 0.9791 - val_loss: 0.2733 - val_acc: 0.9310 - val_auROC: 0.9252\n",
      "Epoch 165/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1960 - acc: 0.9821 - auROC: 0.9793 - val_loss: 0.2733 - val_acc: 0.9310 - val_auROC: 0.9248\n",
      "Epoch 166/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1957 - acc: 0.9825 - auROC: 0.9797 - val_loss: 0.2713 - val_acc: 0.9310 - val_auROC: 0.9272\n",
      "Epoch 167/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1953 - acc: 0.9825 - auROC: 0.9793 - val_loss: 0.2711 - val_acc: 0.9310 - val_auROC: 0.9256\n",
      "Epoch 168/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1947 - acc: 0.9832 - auROC: 0.9795 - val_loss: 0.2719 - val_acc: 0.9310 - val_auROC: 0.9253\n",
      "Epoch 169/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1942 - acc: 0.9836 - auROC: 0.9802 - val_loss: 0.2713 - val_acc: 0.9310 - val_auROC: 0.9256\n",
      "Epoch 170/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1938 - acc: 0.9836 - auROC: 0.9805 - val_loss: 0.2706 - val_acc: 0.9310 - val_auROC: 0.9259\n",
      "Epoch 171/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1935 - acc: 0.9836 - auROC: 0.9806 - val_loss: 0.2697 - val_acc: 0.9345 - val_auROC: 0.9258\n",
      "Epoch 172/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1930 - acc: 0.9840 - auROC: 0.9807 - val_loss: 0.2707 - val_acc: 0.9310 - val_auROC: 0.9250\n",
      "Epoch 173/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1930 - acc: 0.9840 - auROC: 0.9804 - val_loss: 0.2714 - val_acc: 0.9310 - val_auROC: 0.9238\n",
      "Epoch 174/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1925 - acc: 0.9832 - auROC: 0.9811 - val_loss: 0.2711 - val_acc: 0.9310 - val_auROC: 0.9250\n",
      "Epoch 175/426\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1922 - acc: 0.9832 - auROC: 0.9812 - val_loss: 0.2714 - val_acc: 0.9310 - val_auROC: 0.9252\n",
      "Epoch 176/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1919 - acc: 0.9840 - auROC: 0.9813 - val_loss: 0.2708 - val_acc: 0.9310 - val_auROC: 0.9250\n",
      "Epoch 177/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1914 - acc: 0.9844 - auROC: 0.9813 - val_loss: 0.2703 - val_acc: 0.9310 - val_auROC: 0.9246\n",
      "Epoch 178/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1911 - acc: 0.9844 - auROC: 0.9814 - val_loss: 0.2694 - val_acc: 0.9310 - val_auROC: 0.9242\n",
      "Epoch 179/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1910 - acc: 0.9844 - auROC: 0.9813 - val_loss: 0.2681 - val_acc: 0.9310 - val_auROC: 0.9252\n",
      "Epoch 180/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1905 - acc: 0.9844 - auROC: 0.9813 - val_loss: 0.2665 - val_acc: 0.9310 - val_auROC: 0.9262\n",
      "Epoch 181/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1900 - acc: 0.9844 - auROC: 0.9813 - val_loss: 0.2656 - val_acc: 0.9345 - val_auROC: 0.9268\n",
      "Epoch 182/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1896 - acc: 0.9840 - auROC: 0.9814 - val_loss: 0.2655 - val_acc: 0.9345 - val_auROC: 0.9268\n",
      "Epoch 183/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1891 - acc: 0.9848 - auROC: 0.9818 - val_loss: 0.2648 - val_acc: 0.9310 - val_auROC: 0.9270\n",
      "Epoch 184/426\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1888 - acc: 0.9848 - auROC: 0.9822 - val_loss: 0.2650 - val_acc: 0.9310 - val_auROC: 0.9268\n",
      "Epoch 185/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1884 - acc: 0.9848 - auROC: 0.9823 - val_loss: 0.2646 - val_acc: 0.9310 - val_auROC: 0.9273\n",
      "Epoch 186/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1881 - acc: 0.9852 - auROC: 0.9826 - val_loss: 0.2644 - val_acc: 0.9345 - val_auROC: 0.9286\n",
      "Epoch 187/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1877 - acc: 0.9852 - auROC: 0.9829 - val_loss: 0.2645 - val_acc: 0.9345 - val_auROC: 0.9281\n",
      "Epoch 188/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1873 - acc: 0.9856 - auROC: 0.9836 - val_loss: 0.2642 - val_acc: 0.9345 - val_auROC: 0.9296\n",
      "Epoch 189/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1870 - acc: 0.9856 - auROC: 0.9836 - val_loss: 0.2632 - val_acc: 0.9345 - val_auROC: 0.9304\n",
      "Epoch 190/426\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1865 - acc: 0.9864 - auROC: 0.9841 - val_loss: 0.2628 - val_acc: 0.9345 - val_auROC: 0.9303\n",
      "Epoch 191/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1862 - acc: 0.9864 - auROC: 0.9842 - val_loss: 0.2621 - val_acc: 0.9310 - val_auROC: 0.9303\n",
      "Epoch 192/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1864 - acc: 0.9864 - auROC: 0.9840 - val_loss: 0.2644 - val_acc: 0.9345 - val_auROC: 0.9266\n",
      "Epoch 193/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1867 - acc: 0.9856 - auROC: 0.9832 - val_loss: 0.2642 - val_acc: 0.9345 - val_auROC: 0.9255\n",
      "Epoch 194/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1864 - acc: 0.9856 - auROC: 0.9835 - val_loss: 0.2633 - val_acc: 0.9345 - val_auROC: 0.9285\n",
      "Epoch 195/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1859 - acc: 0.9856 - auROC: 0.9840 - val_loss: 0.2613 - val_acc: 0.9379 - val_auROC: 0.9301\n",
      "Epoch 196/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1852 - acc: 0.9860 - auROC: 0.9843 - val_loss: 0.2581 - val_acc: 0.9379 - val_auROC: 0.9326\n",
      "Epoch 197/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1850 - acc: 0.9856 - auROC: 0.9844 - val_loss: 0.2562 - val_acc: 0.9414 - val_auROC: 0.9346\n",
      "Epoch 198/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1854 - acc: 0.9864 - auROC: 0.9844 - val_loss: 0.2562 - val_acc: 0.9379 - val_auROC: 0.9347\n",
      "Epoch 199/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1872 - acc: 0.9832 - auROC: 0.9838 - val_loss: 0.2558 - val_acc: 0.9379 - val_auROC: 0.9340\n",
      "Epoch 200/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1862 - acc: 0.9832 - auROC: 0.9845 - val_loss: 0.2554 - val_acc: 0.9379 - val_auROC: 0.9346\n",
      "Epoch 201/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1846 - acc: 0.9852 - auROC: 0.9852 - val_loss: 0.2568 - val_acc: 0.9276 - val_auROC: 0.9343\n",
      "Epoch 202/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1836 - acc: 0.9864 - auROC: 0.9854 - val_loss: 0.2558 - val_acc: 0.9379 - val_auROC: 0.9342\n",
      "Epoch 203/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1833 - acc: 0.9864 - auROC: 0.9854 - val_loss: 0.2546 - val_acc: 0.9448 - val_auROC: 0.9350\n",
      "Epoch 204/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1839 - acc: 0.9860 - auROC: 0.9853 - val_loss: 0.2534 - val_acc: 0.9414 - val_auROC: 0.9369\n",
      "Epoch 205/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1832 - acc: 0.9867 - auROC: 0.9850 - val_loss: 0.2511 - val_acc: 0.9448 - val_auROC: 0.9375\n",
      "Epoch 206/426\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.1826 - acc: 0.9871 - auROC: 0.9848 - val_loss: 0.2505 - val_acc: 0.9483 - val_auROC: 0.9377\n",
      "Epoch 207/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1820 - acc: 0.9871 - auROC: 0.9855 - val_loss: 0.2500 - val_acc: 0.9448 - val_auROC: 0.9371\n",
      "Epoch 208/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1817 - acc: 0.9867 - auROC: 0.9857 - val_loss: 0.2499 - val_acc: 0.9414 - val_auROC: 0.9378\n",
      "Epoch 209/426\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1813 - acc: 0.9871 - auROC: 0.9858 - val_loss: 0.2492 - val_acc: 0.9448 - val_auROC: 0.9384\n",
      "Epoch 210/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1811 - acc: 0.9871 - auROC: 0.9856 - val_loss: 0.2488 - val_acc: 0.9448 - val_auROC: 0.9386\n",
      "Epoch 211/426\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.1809 - acc: 0.9871 - auROC: 0.9861 - val_loss: 0.2495 - val_acc: 0.9448 - val_auROC: 0.9381\n",
      "Epoch 212/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1806 - acc: 0.9871 - auROC: 0.9864 - val_loss: 0.2496 - val_acc: 0.9448 - val_auROC: 0.9387\n",
      "Epoch 213/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1803 - acc: 0.9875 - auROC: 0.9864 - val_loss: 0.2504 - val_acc: 0.9483 - val_auROC: 0.9387\n",
      "Epoch 214/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1801 - acc: 0.9875 - auROC: 0.9865 - val_loss: 0.2491 - val_acc: 0.9483 - val_auROC: 0.9400\n",
      "Epoch 215/426\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1798 - acc: 0.9875 - auROC: 0.9866 - val_loss: 0.2480 - val_acc: 0.9483 - val_auROC: 0.9405\n",
      "Epoch 216/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1796 - acc: 0.9875 - auROC: 0.9867 - val_loss: 0.2470 - val_acc: 0.9483 - val_auROC: 0.9415\n",
      "Epoch 217/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1794 - acc: 0.9879 - auROC: 0.9869 - val_loss: 0.2466 - val_acc: 0.9483 - val_auROC: 0.9418\n",
      "Epoch 218/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1791 - acc: 0.9879 - auROC: 0.9870 - val_loss: 0.2460 - val_acc: 0.9483 - val_auROC: 0.9422\n",
      "Epoch 219/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1789 - acc: 0.9879 - auROC: 0.9871 - val_loss: 0.2454 - val_acc: 0.9483 - val_auROC: 0.9427\n",
      "Epoch 220/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1788 - acc: 0.9879 - auROC: 0.9873 - val_loss: 0.2453 - val_acc: 0.9483 - val_auROC: 0.9426\n",
      "Epoch 221/426\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1788 - acc: 0.9879 - auROC: 0.9875 - val_loss: 0.2450 - val_acc: 0.9483 - val_auROC: 0.9443\n",
      "Epoch 222/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1786 - acc: 0.9879 - auROC: 0.9878 - val_loss: 0.2450 - val_acc: 0.9483 - val_auROC: 0.9435\n",
      "Epoch 223/426\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1783 - acc: 0.9879 - auROC: 0.9880 - val_loss: 0.2439 - val_acc: 0.9483 - val_auROC: 0.9440\n",
      "Epoch 224/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1779 - acc: 0.9879 - auROC: 0.9882 - val_loss: 0.2425 - val_acc: 0.9483 - val_auROC: 0.9445\n",
      "Epoch 225/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1776 - acc: 0.9879 - auROC: 0.9885 - val_loss: 0.2421 - val_acc: 0.9483 - val_auROC: 0.9449\n",
      "Epoch 226/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1774 - acc: 0.9883 - auROC: 0.9887 - val_loss: 0.2410 - val_acc: 0.9483 - val_auROC: 0.9459\n",
      "Epoch 227/426\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1772 - acc: 0.9883 - auROC: 0.9889 - val_loss: 0.2403 - val_acc: 0.9483 - val_auROC: 0.9468\n",
      "Epoch 228/426\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1770 - acc: 0.9883 - auROC: 0.9892 - val_loss: 0.2389 - val_acc: 0.9483 - val_auROC: 0.9501\n",
      "Epoch 229/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1788 - acc: 0.9871 - auROC: 0.9892 - val_loss: 0.2425 - val_acc: 0.9448 - val_auROC: 0.9519\n",
      "Epoch 230/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1796 - acc: 0.9867 - auROC: 0.9897 - val_loss: 0.2359 - val_acc: 0.9483 - val_auROC: 0.9547\n",
      "Epoch 231/426\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1776 - acc: 0.9883 - auROC: 0.9902 - val_loss: 0.2330 - val_acc: 0.9552 - val_auROC: 0.9573\n",
      "Epoch 232/426\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1769 - acc: 0.9883 - auROC: 0.9903 - val_loss: 0.2330 - val_acc: 0.9552 - val_auROC: 0.9569\n",
      "Epoch 233/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1765 - acc: 0.9883 - auROC: 0.9902 - val_loss: 0.2367 - val_acc: 0.9552 - val_auROC: 0.9546\n",
      "Epoch 234/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1764 - acc: 0.9883 - auROC: 0.9899 - val_loss: 0.2365 - val_acc: 0.9517 - val_auROC: 0.9546\n",
      "Epoch 235/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1758 - acc: 0.9883 - auROC: 0.9903 - val_loss: 0.2351 - val_acc: 0.9517 - val_auROC: 0.9555\n",
      "Epoch 236/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1755 - acc: 0.9883 - auROC: 0.9905 - val_loss: 0.2349 - val_acc: 0.9483 - val_auROC: 0.9554\n",
      "Epoch 237/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1751 - acc: 0.9883 - auROC: 0.9905 - val_loss: 0.2356 - val_acc: 0.9517 - val_auROC: 0.9545\n",
      "Epoch 238/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1749 - acc: 0.9883 - auROC: 0.9905 - val_loss: 0.2356 - val_acc: 0.9517 - val_auROC: 0.9543\n",
      "Epoch 239/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1747 - acc: 0.9883 - auROC: 0.9906 - val_loss: 0.2351 - val_acc: 0.9517 - val_auROC: 0.9552\n",
      "Epoch 240/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1745 - acc: 0.9883 - auROC: 0.9906 - val_loss: 0.2351 - val_acc: 0.9517 - val_auROC: 0.9550\n",
      "Epoch 241/426\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1743 - acc: 0.9883 - auROC: 0.9906 - val_loss: 0.2350 - val_acc: 0.9517 - val_auROC: 0.9548\n",
      "Epoch 242/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1740 - acc: 0.9883 - auROC: 0.9906 - val_loss: 0.2347 - val_acc: 0.9517 - val_auROC: 0.9551\n",
      "Epoch 243/426\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1739 - acc: 0.9879 - auROC: 0.9907 - val_loss: 0.2353 - val_acc: 0.9483 - val_auROC: 0.9545\n",
      "Epoch 244/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1744 - acc: 0.9883 - auROC: 0.9898 - val_loss: 0.2364 - val_acc: 0.9517 - val_auROC: 0.9524\n",
      "Epoch 245/426\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1748 - acc: 0.9879 - auROC: 0.9891 - val_loss: 0.2356 - val_acc: 0.9448 - val_auROC: 0.9543\n",
      "Epoch 246/426\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1739 - acc: 0.9887 - auROC: 0.9901 - val_loss: 0.2343 - val_acc: 0.9483 - val_auROC: 0.9554\n",
      "Epoch 247/426\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.1735 - acc: 0.9887 - auROC: 0.9903Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.1735 - acc: 0.9887 - auROC: 0.9903 - val_loss: 0.2343 - val_acc: 0.9517 - val_auROC: 0.9552\n",
      "Epoch 00247: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "1     0.0    0.0    0.0 -0.207833  ...    0.0 -0.199372 -0.197763    0.0\n",
      "2     0.0    0.0    0.0 -0.330822  ...    0.0 -0.199372 -0.197763    0.0\n",
      "3     0.0    0.0    0.0 -0.364268  ...    0.0  2.631315  2.667567    0.0\n",
      "4     0.0    0.0    0.0  1.188823  ...    0.0 -0.199372 -0.197763    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0 -0.265638  ...    0.0 -0.199372 -0.197763    0.0\n",
      "60    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "61    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "62    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "63    0.0    0.0    0.0 -0.364831  ...    0.0 -0.199372 -0.197763    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                  ...                                      \n",
      "0.00   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  61   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   2  59   0   0  0.0328  0.0  ...  0.9672  0.0  0.0 NaN      0.0    NaN\n",
      "0.04  18  43   0   0  0.2951  0.0  ...  0.7049  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...  ...  ..      ...    ...\n",
      "0.97  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  42   0  20  0.3226  1.0  ...  1.0000  1.0  0.3226  0.4878      1.0    1.0\n",
      "0.01   0  42   0  20  0.3226  1.0  ...  1.0000  1.0  0.3226  0.4878      1.0    1.0\n",
      "0.02   0  42   0  20  0.3226  1.0  ...  1.0000  1.0  0.3226  0.4878      1.0    1.0\n",
      "0.03   0  42   0  20  0.3226  1.0  ...  1.0000  1.0  0.3226  0.4878      1.0    1.0\n",
      "0.04   1  40   0  20  0.3443  1.0  ...  0.9756  1.0  0.3333  0.5000      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  42   0  20   0  0.6774  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  42   0  20   0  0.6774  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  42   0  20   0  0.6774  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  42   0  20   0  0.6774  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  42   0  20   0  0.6774  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  48   0  14  0.2258  1.0  ...  1.0000  1.0  0.2258  0.3684      1.0    1.0\n",
      "0.01   0  48   0  14  0.2258  1.0  ...  1.0000  1.0  0.2258  0.3684      1.0    1.0\n",
      "0.02   0  48   0  14  0.2258  1.0  ...  1.0000  1.0  0.2258  0.3684      1.0    1.0\n",
      "0.03   1  46   0  14  0.2459  1.0  ...  0.9787  1.0  0.2333  0.3784      1.0    1.0\n",
      "0.04  13  34   0  14  0.4426  1.0  ...  0.7234  1.0  0.2917  0.4516      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  48   0  14   0  0.7742  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.01   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.02   6  49   0   6  0.1967  1.0  ...  0.8909  1.0  0.1091  0.1967      1.0    1.0\n",
      "0.03  14  41   0   6  0.3279  1.0  ...  0.7455  1.0  0.1277  0.2264      1.0    1.0\n",
      "0.04  17  38   0   6  0.3770  1.0  ...  0.6909  1.0  0.1364  0.2400      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9206  0.8824\n",
      "0.01   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9206  0.8824\n",
      "0.02   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9206  0.8824\n",
      "0.03   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9206  0.8824\n",
      "0.04   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9206  0.8824\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9206  0.8824\n",
      "0.98  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9206  0.8824\n",
      "0.99  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9206  0.8824\n",
      "1.00  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9206  0.8824\n",
      "1.01  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9206  0.8824\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "N. NaN in input features: 0\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015589  0.044423\n",
      "4      0.015568  0.044409\n",
      "...         ...       ...\n",
      "18013  0.000055  0.000231\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002367  0.011945\n",
      "18016  0.002302  0.011713\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Pre-training using Adam with lr=1e-05...\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 0.7036 - acc: 0.4901 - val_loss: 0.6880 - val_acc: 0.5207\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6929 - acc: 0.5037 - val_loss: 0.6801 - val_acc: 0.5379\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 18,998,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training using Adam with lr=0.001...\n",
      "Epoch 3/1002\n",
      "1/1 [==============================] - 1s 801ms/step - loss: 0.6839 - acc: 0.5228 - auROC: 0.4530 - val_loss: 0.6317 - val_acc: 0.6690 - val_auROC: 0.5731\n",
      "Epoch 4/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.6662 - acc: 0.6624 - auROC: 0.5130 - val_loss: 0.5806 - val_acc: 0.6793 - val_auROC: 0.6494\n",
      "Epoch 5/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.6185 - acc: 0.6374 - auROC: 0.5740 - val_loss: 0.5572 - val_acc: 0.7172 - val_auROC: 0.6537\n",
      "Epoch 6/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.6136 - acc: 0.6600 - auROC: 0.5690 - val_loss: 0.5369 - val_acc: 0.7379 - val_auROC: 0.7010\n",
      "Epoch 7/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.5915 - acc: 0.6733 - auROC: 0.6038 - val_loss: 0.5145 - val_acc: 0.7655 - val_auROC: 0.7380\n",
      "Epoch 8/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5623 - acc: 0.7138 - auROC: 0.6515 - val_loss: 0.5187 - val_acc: 0.7586 - val_auROC: 0.7201\n",
      "Epoch 9/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5644 - acc: 0.6971 - auROC: 0.6434 - val_loss: 0.5409 - val_acc: 0.7414 - val_auROC: 0.7538\n",
      "Epoch 10/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5685 - acc: 0.7427 - auROC: 0.6777 - val_loss: 0.5319 - val_acc: 0.7379 - val_auROC: 0.7617\n",
      "Epoch 11/1002\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5597 - acc: 0.7404 - auROC: 0.6896 - val_loss: 0.5140 - val_acc: 0.7655 - val_auROC: 0.7743\n",
      "Epoch 12/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5379 - acc: 0.7442 - auROC: 0.7170 - val_loss: 0.5212 - val_acc: 0.7552 - val_auROC: 0.7350\n",
      "Epoch 13/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5373 - acc: 0.7400 - auROC: 0.7069 - val_loss: 0.5082 - val_acc: 0.7793 - val_auROC: 0.7364\n",
      "Epoch 14/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5260 - acc: 0.7579 - auROC: 0.7102 - val_loss: 0.4933 - val_acc: 0.8000 - val_auROC: 0.7315\n",
      "Epoch 15/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.5216 - acc: 0.7618 - auROC: 0.6844 - val_loss: 0.4809 - val_acc: 0.8000 - val_auROC: 0.7746\n",
      "Epoch 16/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5033 - acc: 0.7673 - auROC: 0.7393 - val_loss: 0.4796 - val_acc: 0.7931 - val_auROC: 0.7896\n",
      "Epoch 17/1002\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4939 - acc: 0.7786 - auROC: 0.7744 - val_loss: 0.4745 - val_acc: 0.8138 - val_auROC: 0.7825\n",
      "Epoch 18/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4868 - acc: 0.7887 - auROC: 0.7741 - val_loss: 0.4634 - val_acc: 0.8172 - val_auROC: 0.7704\n",
      "Epoch 19/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4856 - acc: 0.8000 - auROC: 0.7552 - val_loss: 0.4587 - val_acc: 0.8345 - val_auROC: 0.7829\n",
      "Epoch 20/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4781 - acc: 0.8078 - auROC: 0.7694 - val_loss: 0.4516 - val_acc: 0.8276 - val_auROC: 0.7991\n",
      "Epoch 21/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4716 - acc: 0.8058 - auROC: 0.7833 - val_loss: 0.4496 - val_acc: 0.8345 - val_auROC: 0.8006\n",
      "Epoch 22/1002\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4659 - acc: 0.8086 - auROC: 0.7896 - val_loss: 0.4451 - val_acc: 0.8483 - val_auROC: 0.8102\n",
      "Epoch 23/1002\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.4600 - acc: 0.8140 - auROC: 0.8033 - val_loss: 0.4429 - val_acc: 0.8448 - val_auROC: 0.8155\n",
      "Epoch 24/1002\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4570 - acc: 0.8168 - auROC: 0.8108 - val_loss: 0.4375 - val_acc: 0.8345 - val_auROC: 0.8234\n",
      "Epoch 25/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.4541 - acc: 0.8133 - auROC: 0.8098 - val_loss: 0.4346 - val_acc: 0.8172 - val_auROC: 0.8283\n",
      "Epoch 26/1002\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.4507 - acc: 0.8152 - auROC: 0.8158 - val_loss: 0.4330 - val_acc: 0.8172 - val_auROC: 0.8250\n",
      "Epoch 27/1002\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.4449 - acc: 0.8214 - auROC: 0.8123 - val_loss: 0.4312 - val_acc: 0.8241 - val_auROC: 0.8257\n",
      "Epoch 28/1002\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4438 - acc: 0.8273 - auROC: 0.8110 - val_loss: 0.4331 - val_acc: 0.8414 - val_auROC: 0.8443\n",
      "Epoch 29/1002\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4413 - acc: 0.8429 - auROC: 0.8366 - val_loss: 0.4230 - val_acc: 0.8586 - val_auROC: 0.8585\n",
      "Epoch 30/1002\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.4376 - acc: 0.8405 - auROC: 0.8451 - val_loss: 0.4175 - val_acc: 0.8448 - val_auROC: 0.8473\n",
      "Epoch 31/1002\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4305 - acc: 0.8312 - auROC: 0.8448 - val_loss: 0.4165 - val_acc: 0.8241 - val_auROC: 0.8456\n",
      "Epoch 32/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4267 - acc: 0.8327 - auROC: 0.8430 - val_loss: 0.4196 - val_acc: 0.8517 - val_auROC: 0.8474\n",
      "Epoch 33/1002\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4235 - acc: 0.8511 - auROC: 0.8558 - val_loss: 0.4170 - val_acc: 0.8414 - val_auROC: 0.8492\n",
      "Epoch 34/1002\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.4191 - acc: 0.8534 - auROC: 0.8635 - val_loss: 0.4098 - val_acc: 0.8345 - val_auROC: 0.8491\n",
      "Epoch 35/1002\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.4134 - acc: 0.8472 - auROC: 0.8645 - val_loss: 0.4064 - val_acc: 0.8414 - val_auROC: 0.8477\n",
      "Epoch 36/1002\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.4130 - acc: 0.8480 - auROC: 0.8595 - val_loss: 0.3996 - val_acc: 0.8586 - val_auROC: 0.8729\n",
      "Epoch 37/1002\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4082 - acc: 0.8546 - auROC: 0.8741 - val_loss: 0.4018 - val_acc: 0.8448 - val_auROC: 0.8694\n",
      "Epoch 38/1002\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.4063 - acc: 0.8515 - auROC: 0.8751 - val_loss: 0.3976 - val_acc: 0.8448 - val_auROC: 0.8725\n",
      "Epoch 39/1002\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4019 - acc: 0.8515 - auROC: 0.8776 - val_loss: 0.3972 - val_acc: 0.8483 - val_auROC: 0.8582\n",
      "Epoch 40/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4001 - acc: 0.8530 - auROC: 0.8740 - val_loss: 0.4002 - val_acc: 0.8483 - val_auROC: 0.8620\n",
      "Epoch 41/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3972 - acc: 0.8612 - auROC: 0.8880 - val_loss: 0.4021 - val_acc: 0.8552 - val_auROC: 0.8545\n",
      "Epoch 42/1002\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3947 - acc: 0.8674 - auROC: 0.8888 - val_loss: 0.3950 - val_acc: 0.8517 - val_auROC: 0.8593\n",
      "Epoch 43/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3904 - acc: 0.8608 - auROC: 0.8851 - val_loss: 0.3908 - val_acc: 0.8345 - val_auROC: 0.8625\n",
      "Epoch 44/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3915 - acc: 0.8483 - auROC: 0.8749 - val_loss: 0.3905 - val_acc: 0.8517 - val_auROC: 0.8727\n",
      "Epoch 45/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3880 - acc: 0.8639 - auROC: 0.8920 - val_loss: 0.3873 - val_acc: 0.8586 - val_auROC: 0.8823\n",
      "Epoch 46/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3882 - acc: 0.8690 - auROC: 0.8968 - val_loss: 0.3882 - val_acc: 0.8586 - val_auROC: 0.8654\n",
      "Epoch 47/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3839 - acc: 0.8663 - auROC: 0.8969 - val_loss: 0.3857 - val_acc: 0.8414 - val_auROC: 0.8646\n",
      "Epoch 48/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3788 - acc: 0.8569 - auROC: 0.8925 - val_loss: 0.3875 - val_acc: 0.8517 - val_auROC: 0.8635\n",
      "Epoch 49/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3796 - acc: 0.8698 - auROC: 0.8865 - val_loss: 0.3894 - val_acc: 0.8586 - val_auROC: 0.8667\n",
      "Epoch 50/1002\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3764 - acc: 0.8811 - auROC: 0.8962 - val_loss: 0.3826 - val_acc: 0.8552 - val_auROC: 0.8747\n",
      "Epoch 51/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3730 - acc: 0.8768 - auROC: 0.8992 - val_loss: 0.3706 - val_acc: 0.8621 - val_auROC: 0.8866\n",
      "Epoch 52/1002\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3707 - acc: 0.8717 - auROC: 0.8988 - val_loss: 0.3661 - val_acc: 0.8690 - val_auROC: 0.8936\n",
      "Epoch 53/1002\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3683 - acc: 0.8784 - auROC: 0.9028 - val_loss: 0.3693 - val_acc: 0.8759 - val_auROC: 0.8962\n",
      "Epoch 54/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3670 - acc: 0.8834 - auROC: 0.9064 - val_loss: 0.3693 - val_acc: 0.8552 - val_auROC: 0.8934\n",
      "Epoch 55/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3619 - acc: 0.8815 - auROC: 0.9113 - val_loss: 0.3689 - val_acc: 0.8448 - val_auROC: 0.8888\n",
      "Epoch 56/1002\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.3613 - acc: 0.8682 - auROC: 0.9060 - val_loss: 0.3625 - val_acc: 0.8586 - val_auROC: 0.8958\n",
      "Epoch 57/1002\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3571 - acc: 0.8869 - auROC: 0.9131 - val_loss: 0.3602 - val_acc: 0.8724 - val_auROC: 0.8992\n",
      "Epoch 58/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3548 - acc: 0.8955 - auROC: 0.9159 - val_loss: 0.3562 - val_acc: 0.8759 - val_auROC: 0.9054\n",
      "Epoch 59/1002\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3532 - acc: 0.8959 - auROC: 0.9170 - val_loss: 0.3517 - val_acc: 0.8724 - val_auROC: 0.9106\n",
      "Epoch 60/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3496 - acc: 0.8912 - auROC: 0.9183 - val_loss: 0.3525 - val_acc: 0.8621 - val_auROC: 0.9067\n",
      "Epoch 61/1002\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3479 - acc: 0.8865 - auROC: 0.9181 - val_loss: 0.3505 - val_acc: 0.8690 - val_auROC: 0.9070\n",
      "Epoch 62/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3437 - acc: 0.8889 - auROC: 0.9249 - val_loss: 0.3480 - val_acc: 0.8759 - val_auROC: 0.9079\n",
      "Epoch 63/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3420 - acc: 0.8912 - auROC: 0.9259 - val_loss: 0.3442 - val_acc: 0.8759 - val_auROC: 0.9108\n",
      "Epoch 64/1002\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3385 - acc: 0.8943 - auROC: 0.9274 - val_loss: 0.3426 - val_acc: 0.8759 - val_auROC: 0.9152\n",
      "Epoch 65/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3357 - acc: 0.8998 - auROC: 0.9294 - val_loss: 0.3425 - val_acc: 0.8793 - val_auROC: 0.9136\n",
      "Epoch 66/1002\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3337 - acc: 0.9033 - auROC: 0.9293 - val_loss: 0.3406 - val_acc: 0.8828 - val_auROC: 0.9137\n",
      "Epoch 67/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.3309 - acc: 0.9053 - auROC: 0.9325 - val_loss: 0.3395 - val_acc: 0.8828 - val_auROC: 0.9154\n",
      "Epoch 68/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3282 - acc: 0.9057 - auROC: 0.9352 - val_loss: 0.3377 - val_acc: 0.8862 - val_auROC: 0.9157\n",
      "Epoch 69/1002\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3260 - acc: 0.9072 - auROC: 0.9347 - val_loss: 0.3357 - val_acc: 0.8897 - val_auROC: 0.9180\n",
      "Epoch 70/1002\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3230 - acc: 0.9092 - auROC: 0.9356 - val_loss: 0.3326 - val_acc: 0.8828 - val_auROC: 0.9211\n",
      "Epoch 71/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3217 - acc: 0.9084 - auROC: 0.9357 - val_loss: 0.3318 - val_acc: 0.8862 - val_auROC: 0.9208\n",
      "Epoch 72/1002\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.3181 - acc: 0.9119 - auROC: 0.9394 - val_loss: 0.3295 - val_acc: 0.8897 - val_auROC: 0.9222\n",
      "Epoch 73/1002\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3164 - acc: 0.9123 - auROC: 0.9409 - val_loss: 0.3290 - val_acc: 0.8897 - val_auROC: 0.9213\n",
      "Epoch 74/1002\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3148 - acc: 0.9135 - auROC: 0.9417 - val_loss: 0.3211 - val_acc: 0.8966 - val_auROC: 0.9306\n",
      "Epoch 75/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3121 - acc: 0.9154 - auROC: 0.9430 - val_loss: 0.3174 - val_acc: 0.8966 - val_auROC: 0.9318\n",
      "Epoch 76/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3105 - acc: 0.9170 - auROC: 0.9418 - val_loss: 0.3165 - val_acc: 0.8966 - val_auROC: 0.9330\n",
      "Epoch 77/1002\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3083 - acc: 0.9166 - auROC: 0.9420 - val_loss: 0.3140 - val_acc: 0.9000 - val_auROC: 0.9349\n",
      "Epoch 78/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3058 - acc: 0.9170 - auROC: 0.9448 - val_loss: 0.3125 - val_acc: 0.9000 - val_auROC: 0.9350\n",
      "Epoch 79/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3038 - acc: 0.9170 - auROC: 0.9469 - val_loss: 0.3129 - val_acc: 0.9000 - val_auROC: 0.9348\n",
      "Epoch 80/1002\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.3017 - acc: 0.9166 - auROC: 0.9476 - val_loss: 0.3104 - val_acc: 0.9000 - val_auROC: 0.9349\n",
      "Epoch 81/1002\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2997 - acc: 0.9170 - auROC: 0.9473 - val_loss: 0.3101 - val_acc: 0.9000 - val_auROC: 0.9343\n",
      "Epoch 82/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2976 - acc: 0.9177 - auROC: 0.9487 - val_loss: 0.3097 - val_acc: 0.9000 - val_auROC: 0.9355\n",
      "Epoch 83/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2959 - acc: 0.9220 - auROC: 0.9497 - val_loss: 0.3074 - val_acc: 0.9000 - val_auROC: 0.9364\n",
      "Epoch 84/1002\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2938 - acc: 0.9209 - auROC: 0.9505 - val_loss: 0.3084 - val_acc: 0.9000 - val_auROC: 0.9361\n",
      "Epoch 85/1002\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2930 - acc: 0.9197 - auROC: 0.9499 - val_loss: 0.3070 - val_acc: 0.9000 - val_auROC: 0.9350\n",
      "Epoch 86/1002\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2904 - acc: 0.9228 - auROC: 0.9518 - val_loss: 0.3051 - val_acc: 0.9000 - val_auROC: 0.9362\n",
      "Epoch 87/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2884 - acc: 0.9244 - auROC: 0.9521 - val_loss: 0.3053 - val_acc: 0.9000 - val_auROC: 0.9348\n",
      "Epoch 88/1002\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2878 - acc: 0.9251 - auROC: 0.9519 - val_loss: 0.3024 - val_acc: 0.9000 - val_auROC: 0.9367\n",
      "Epoch 89/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2851 - acc: 0.9267 - auROC: 0.9532 - val_loss: 0.3025 - val_acc: 0.9034 - val_auROC: 0.9356\n",
      "Epoch 90/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2835 - acc: 0.9263 - auROC: 0.9523 - val_loss: 0.3004 - val_acc: 0.9034 - val_auROC: 0.9360\n",
      "Epoch 91/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2833 - acc: 0.9259 - auROC: 0.9489 - val_loss: 0.2965 - val_acc: 0.9034 - val_auROC: 0.9398\n",
      "Epoch 92/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2785 - acc: 0.9294 - auROC: 0.9547 - val_loss: 0.3028 - val_acc: 0.9034 - val_auROC: 0.9360\n",
      "Epoch 93/1002\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2766 - acc: 0.9341 - auROC: 0.9574 - val_loss: 0.2944 - val_acc: 0.9069 - val_auROC: 0.9411\n",
      "Epoch 94/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2768 - acc: 0.9368 - auROC: 0.9548 - val_loss: 0.3046 - val_acc: 0.9000 - val_auROC: 0.9337\n",
      "Epoch 95/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2783 - acc: 0.9341 - auROC: 0.9564 - val_loss: 0.2935 - val_acc: 0.9103 - val_auROC: 0.9408\n",
      "Epoch 96/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2713 - acc: 0.9365 - auROC: 0.9568 - val_loss: 0.2918 - val_acc: 0.9138 - val_auROC: 0.9379\n",
      "Epoch 97/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2772 - acc: 0.9326 - auROC: 0.9476 - val_loss: 0.3138 - val_acc: 0.8966 - val_auROC: 0.9254\n",
      "Epoch 98/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2829 - acc: 0.9333 - auROC: 0.9509 - val_loss: 0.3007 - val_acc: 0.9138 - val_auROC: 0.9300\n",
      "Epoch 99/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2752 - acc: 0.9400 - auROC: 0.9494 - val_loss: 0.2921 - val_acc: 0.9103 - val_auROC: 0.9278\n",
      "Epoch 100/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2708 - acc: 0.9396 - auROC: 0.9518 - val_loss: 0.2893 - val_acc: 0.9138 - val_auROC: 0.9376\n",
      "Epoch 101/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2706 - acc: 0.9368 - auROC: 0.9532 - val_loss: 0.2995 - val_acc: 0.9000 - val_auROC: 0.9337\n",
      "Epoch 102/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2685 - acc: 0.9431 - auROC: 0.9571 - val_loss: 0.3113 - val_acc: 0.8931 - val_auROC: 0.9181\n",
      "Epoch 103/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2749 - acc: 0.9357 - auROC: 0.9497 - val_loss: 0.2964 - val_acc: 0.9069 - val_auROC: 0.9331\n",
      "Epoch 104/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2632 - acc: 0.9439 - auROC: 0.9591 - val_loss: 0.2909 - val_acc: 0.9103 - val_auROC: 0.9300\n",
      "Epoch 105/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2652 - acc: 0.9404 - auROC: 0.9545 - val_loss: 0.2867 - val_acc: 0.9138 - val_auROC: 0.9303\n",
      "Epoch 106/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2592 - acc: 0.9454 - auROC: 0.9567 - val_loss: 0.2913 - val_acc: 0.9172 - val_auROC: 0.9281\n",
      "Epoch 107/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2584 - acc: 0.9454 - auROC: 0.9581 - val_loss: 0.2964 - val_acc: 0.9034 - val_auROC: 0.9258\n",
      "Epoch 108/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2582 - acc: 0.9478 - auROC: 0.9597 - val_loss: 0.2956 - val_acc: 0.9034 - val_auROC: 0.9275\n",
      "Epoch 109/1002\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2576 - acc: 0.9478 - auROC: 0.9593 - val_loss: 0.2829 - val_acc: 0.9172 - val_auROC: 0.9368\n",
      "Epoch 110/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2528 - acc: 0.9489 - auROC: 0.9612 - val_loss: 0.2810 - val_acc: 0.9138 - val_auROC: 0.9329\n",
      "Epoch 111/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2538 - acc: 0.9431 - auROC: 0.9593 - val_loss: 0.2779 - val_acc: 0.9138 - val_auROC: 0.9392\n",
      "Epoch 112/1002\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.2496 - acc: 0.9466 - auROC: 0.9614 - val_loss: 0.2772 - val_acc: 0.9207 - val_auROC: 0.9396\n",
      "Epoch 113/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2491 - acc: 0.9520 - auROC: 0.9623 - val_loss: 0.2803 - val_acc: 0.9172 - val_auROC: 0.9377\n",
      "Epoch 114/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2489 - acc: 0.9528 - auROC: 0.9621 - val_loss: 0.2764 - val_acc: 0.9138 - val_auROC: 0.9408\n",
      "Epoch 115/1002\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2452 - acc: 0.9536 - auROC: 0.9624 - val_loss: 0.2737 - val_acc: 0.9172 - val_auROC: 0.9368\n",
      "Epoch 116/1002\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2437 - acc: 0.9524 - auROC: 0.9615 - val_loss: 0.2715 - val_acc: 0.9207 - val_auROC: 0.9379\n",
      "Epoch 117/1002\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2423 - acc: 0.9528 - auROC: 0.9614 - val_loss: 0.2745 - val_acc: 0.9138 - val_auROC: 0.9411\n",
      "Epoch 118/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2412 - acc: 0.9513 - auROC: 0.9633 - val_loss: 0.2791 - val_acc: 0.9138 - val_auROC: 0.9322\n",
      "Epoch 119/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2400 - acc: 0.9532 - auROC: 0.9637 - val_loss: 0.2781 - val_acc: 0.9172 - val_auROC: 0.9294\n",
      "Epoch 120/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2383 - acc: 0.9540 - auROC: 0.9638 - val_loss: 0.2725 - val_acc: 0.9207 - val_auROC: 0.9326\n",
      "Epoch 121/1002\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2369 - acc: 0.9536 - auROC: 0.9637 - val_loss: 0.2684 - val_acc: 0.9241 - val_auROC: 0.9368\n",
      "Epoch 122/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2354 - acc: 0.9548 - auROC: 0.9644 - val_loss: 0.2686 - val_acc: 0.9241 - val_auROC: 0.9364\n",
      "Epoch 123/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2341 - acc: 0.9548 - auROC: 0.9650 - val_loss: 0.2714 - val_acc: 0.9241 - val_auROC: 0.9276\n",
      "Epoch 124/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2328 - acc: 0.9544 - auROC: 0.9652 - val_loss: 0.2729 - val_acc: 0.9241 - val_auROC: 0.9276\n",
      "Epoch 125/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2316 - acc: 0.9544 - auROC: 0.9648 - val_loss: 0.2707 - val_acc: 0.9276 - val_auROC: 0.9286\n",
      "Epoch 126/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2305 - acc: 0.9563 - auROC: 0.9653 - val_loss: 0.2673 - val_acc: 0.9241 - val_auROC: 0.9308\n",
      "Epoch 127/1002\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2289 - acc: 0.9559 - auROC: 0.9666 - val_loss: 0.2637 - val_acc: 0.9276 - val_auROC: 0.9334\n",
      "Epoch 128/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2290 - acc: 0.9552 - auROC: 0.9665 - val_loss: 0.2677 - val_acc: 0.9241 - val_auROC: 0.9306\n",
      "Epoch 129/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2273 - acc: 0.9556 - auROC: 0.9656 - val_loss: 0.2694 - val_acc: 0.9276 - val_auROC: 0.9337\n",
      "Epoch 130/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2262 - acc: 0.9595 - auROC: 0.9669 - val_loss: 0.2647 - val_acc: 0.9241 - val_auROC: 0.9370\n",
      "Epoch 131/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2246 - acc: 0.9567 - auROC: 0.9676 - val_loss: 0.2568 - val_acc: 0.9310 - val_auROC: 0.9415\n",
      "Epoch 132/1002\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2236 - acc: 0.9583 - auROC: 0.9669 - val_loss: 0.2554 - val_acc: 0.9310 - val_auROC: 0.9403\n",
      "Epoch 133/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2229 - acc: 0.9583 - auROC: 0.9663 - val_loss: 0.2567 - val_acc: 0.9276 - val_auROC: 0.9392\n",
      "Epoch 134/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2217 - acc: 0.9583 - auROC: 0.9669 - val_loss: 0.2590 - val_acc: 0.9276 - val_auROC: 0.9376\n",
      "Epoch 135/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2205 - acc: 0.9579 - auROC: 0.9676 - val_loss: 0.2589 - val_acc: 0.9276 - val_auROC: 0.9366\n",
      "Epoch 136/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2196 - acc: 0.9579 - auROC: 0.9677 - val_loss: 0.2566 - val_acc: 0.9276 - val_auROC: 0.9363\n",
      "Epoch 137/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2186 - acc: 0.9583 - auROC: 0.9673 - val_loss: 0.2531 - val_acc: 0.9276 - val_auROC: 0.9376\n",
      "Epoch 138/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2174 - acc: 0.9583 - auROC: 0.9677 - val_loss: 0.2509 - val_acc: 0.9345 - val_auROC: 0.9389\n",
      "Epoch 139/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2168 - acc: 0.9579 - auROC: 0.9679 - val_loss: 0.2506 - val_acc: 0.9310 - val_auROC: 0.9379\n",
      "Epoch 140/1002\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2161 - acc: 0.9579 - auROC: 0.9677 - val_loss: 0.2502 - val_acc: 0.9310 - val_auROC: 0.9376\n",
      "Epoch 141/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2151 - acc: 0.9579 - auROC: 0.9678 - val_loss: 0.2503 - val_acc: 0.9310 - val_auROC: 0.9380\n",
      "Epoch 142/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2143 - acc: 0.9579 - auROC: 0.9677 - val_loss: 0.2507 - val_acc: 0.9310 - val_auROC: 0.9373\n",
      "Epoch 143/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2136 - acc: 0.9579 - auROC: 0.9679 - val_loss: 0.2500 - val_acc: 0.9310 - val_auROC: 0.9376\n",
      "Epoch 144/1002\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2127 - acc: 0.9587 - auROC: 0.9680 - val_loss: 0.2463 - val_acc: 0.9379 - val_auROC: 0.9387\n",
      "Epoch 145/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2113 - acc: 0.9598 - auROC: 0.9684 - val_loss: 0.2434 - val_acc: 0.9379 - val_auROC: 0.9401\n",
      "Epoch 146/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2105 - acc: 0.9606 - auROC: 0.9684 - val_loss: 0.2437 - val_acc: 0.9345 - val_auROC: 0.9375\n",
      "Epoch 147/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2090 - acc: 0.9614 - auROC: 0.9688 - val_loss: 0.2419 - val_acc: 0.9379 - val_auROC: 0.9393\n",
      "Epoch 148/1002\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2081 - acc: 0.9641 - auROC: 0.9687 - val_loss: 0.2423 - val_acc: 0.9345 - val_auROC: 0.9392\n",
      "Epoch 149/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2087 - acc: 0.9622 - auROC: 0.9683 - val_loss: 0.2474 - val_acc: 0.9310 - val_auROC: 0.9391\n",
      "Epoch 150/1002\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2100 - acc: 0.9626 - auROC: 0.9686 - val_loss: 0.2413 - val_acc: 0.9345 - val_auROC: 0.9390\n",
      "Epoch 151/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2059 - acc: 0.9645 - auROC: 0.9686 - val_loss: 0.2433 - val_acc: 0.9345 - val_auROC: 0.9376\n",
      "Epoch 152/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2076 - acc: 0.9618 - auROC: 0.9673 - val_loss: 0.2494 - val_acc: 0.9310 - val_auROC: 0.9307\n",
      "Epoch 153/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2095 - acc: 0.9579 - auROC: 0.9673 - val_loss: 0.2428 - val_acc: 0.9345 - val_auROC: 0.9378\n",
      "Epoch 154/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2054 - acc: 0.9614 - auROC: 0.9683 - val_loss: 0.2493 - val_acc: 0.9241 - val_auROC: 0.9296\n",
      "Epoch 155/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2084 - acc: 0.9610 - auROC: 0.9651\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2084 - acc: 0.9610 - auROC: 0.9651 - val_loss: 0.2462 - val_acc: 0.9310 - val_auROC: 0.9385\n",
      "Epoch 156/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2074 - acc: 0.9630 - auROC: 0.9676 - val_loss: 0.2462 - val_acc: 0.9310 - val_auROC: 0.9386\n",
      "Epoch 157/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2075 - acc: 0.9630 - auROC: 0.9677 - val_loss: 0.2450 - val_acc: 0.9345 - val_auROC: 0.9389\n",
      "Epoch 158/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2061 - acc: 0.9634 - auROC: 0.9682 - val_loss: 0.2430 - val_acc: 0.9345 - val_auROC: 0.9421\n",
      "Epoch 159/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2051 - acc: 0.9641 - auROC: 0.9682 - val_loss: 0.2412 - val_acc: 0.9345 - val_auROC: 0.9429\n",
      "Epoch 160/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2046 - acc: 0.9641 - auROC: 0.9683 - val_loss: 0.2401 - val_acc: 0.9345 - val_auROC: 0.9430\n",
      "Epoch 161/1002\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2045 - acc: 0.9637 - auROC: 0.9678 - val_loss: 0.2394 - val_acc: 0.9310 - val_auROC: 0.9431\n",
      "Epoch 162/1002\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2046 - acc: 0.9637 - auROC: 0.9672 - val_loss: 0.2387 - val_acc: 0.9310 - val_auROC: 0.9432\n",
      "Epoch 163/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2046 - acc: 0.9637 - auROC: 0.9671 - val_loss: 0.2379 - val_acc: 0.9345 - val_auROC: 0.9439\n",
      "Epoch 164/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2045 - acc: 0.9641 - auROC: 0.9671 - val_loss: 0.2372 - val_acc: 0.9379 - val_auROC: 0.9443\n",
      "Epoch 165/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2040 - acc: 0.9645 - auROC: 0.9673 - val_loss: 0.2365 - val_acc: 0.9379 - val_auROC: 0.9452\n",
      "Epoch 166/1002\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2033 - acc: 0.9653 - auROC: 0.9680 - val_loss: 0.2360 - val_acc: 0.9379 - val_auROC: 0.9453\n",
      "Epoch 167/1002\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2030 - acc: 0.9653 - auROC: 0.9683 - val_loss: 0.2357 - val_acc: 0.9379 - val_auROC: 0.9461\n",
      "Epoch 168/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2028 - acc: 0.9653 - auROC: 0.9685 - val_loss: 0.2356 - val_acc: 0.9379 - val_auROC: 0.9472\n",
      "Epoch 169/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2026 - acc: 0.9653 - auROC: 0.9686 - val_loss: 0.2356 - val_acc: 0.9379 - val_auROC: 0.9472\n",
      "Epoch 170/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2024 - acc: 0.9653 - auROC: 0.9689 - val_loss: 0.2358 - val_acc: 0.9345 - val_auROC: 0.9470\n",
      "Epoch 171/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2019 - acc: 0.9634 - auROC: 0.9698 - val_loss: 0.2360 - val_acc: 0.9345 - val_auROC: 0.9470\n",
      "Epoch 172/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2018 - acc: 0.9630 - auROC: 0.9698 - val_loss: 0.2361 - val_acc: 0.9345 - val_auROC: 0.9470\n",
      "Epoch 173/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2018 - acc: 0.9626 - auROC: 0.9698\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2018 - acc: 0.9626 - auROC: 0.9698 - val_loss: 0.2361 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 174/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2361 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 175/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2360 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 176/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2360 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 177/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2359 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 178/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2359 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 179/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2017 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2358 - val_acc: 0.9345 - val_auROC: 0.9469\n",
      "Epoch 180/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2016 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2357 - val_acc: 0.9345 - val_auROC: 0.9470\n",
      "Epoch 181/1002\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2016 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2357 - val_acc: 0.9345 - val_auROC: 0.9470\n",
      "Epoch 182/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2016 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2356 - val_acc: 0.9345 - val_auROC: 0.9471\n",
      "Epoch 183/1002\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2016 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2355 - val_acc: 0.9345 - val_auROC: 0.9471\n",
      "Epoch 184/1002\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2016 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2354 - val_acc: 0.9379 - val_auROC: 0.9472\n",
      "Epoch 185/1002\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.2015 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2353 - val_acc: 0.9379 - val_auROC: 0.9472\n",
      "Epoch 186/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2015 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2353 - val_acc: 0.9379 - val_auROC: 0.9472\n",
      "Epoch 187/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2015 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2352 - val_acc: 0.9379 - val_auROC: 0.9473\n",
      "Epoch 188/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2015 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2351 - val_acc: 0.9379 - val_auROC: 0.9473\n",
      "Epoch 189/1002\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2014 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2350 - val_acc: 0.9379 - val_auROC: 0.9473\n",
      "Epoch 190/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2014 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2350 - val_acc: 0.9379 - val_auROC: 0.9473\n",
      "Epoch 191/1002\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2013 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2349 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 192/1002\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2013 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2348 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 193/1002\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2012 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2348 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 194/1002\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2011 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2347 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 195/1002\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2010 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2347 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 196/1002\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2009 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2347 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 197/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2008 - acc: 0.9618 - auROC: 0.9698 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 198/1002\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2007 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 199/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2007 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9476\n",
      "Epoch 200/1002\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2006 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9477\n",
      "Epoch 201/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2005 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9478\n",
      "Epoch 202/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2005 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9478\n",
      "Epoch 203/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2004 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9477\n",
      "Epoch 204/1002\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2003 - acc: 0.9618 - auROC: 0.9699 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9477\n",
      "Epoch 205/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2003 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9478\n",
      "Epoch 206/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2002 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 207/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2001 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 208/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2001 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2345 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 209/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2001 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 210/1002\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 211/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 212/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 213/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 214/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 215/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 216/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9474\n",
      "Epoch 217/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 218/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9475\n",
      "Epoch 219/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700Restoring model weights from the end of the best epoch.\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.2000 - acc: 0.9618 - auROC: 0.9700 - val_loss: 0.2346 - val_acc: 0.9379 - val_auROC: 0.9476\n",
      "Epoch 00219: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "1     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "2     0.0    0.0    0.0 -0.003108  ...    0.0 -0.198135 -0.196532    0.0\n",
      "3     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "4     0.0    0.0    0.0  0.568506  ...    0.0 -0.198135 -0.196532    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0  3.113151  ...    0.0 -0.198135 -0.196532    0.0\n",
      "60    0.0    0.0    0.0 -0.350213  ...    0.0 -0.198135 -0.196532    0.0\n",
      "61    0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "62    0.0    0.0    0.0 -0.139348  ...    0.0 -0.198135 -0.196532    0.0\n",
      "63    0.0    0.0    0.0 -0.350680  ...    0.0 -0.198135 -0.196532    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  61   0   1  0.0161  1.0  ...  1.0  1.0  0.0161  0.0317   0.6917  0.0714\n",
      "0.01   0  61   0   1  0.0161  1.0  ...  1.0  1.0  0.0161  0.0317   0.6917  0.0714\n",
      "0.02   0  61   0   1  0.0161  1.0  ...  1.0  1.0  0.0161  0.0317   0.6917  0.0714\n",
      "0.03   0  61   0   1  0.0161  1.0  ...  1.0  1.0  0.0161  0.0317   0.6917  0.0714\n",
      "0.04   0  60   0   1  0.0164  1.0  ...  1.0  1.0  0.0164  0.0323   0.6917  0.0714\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  61   0   1   0  0.9839  0.0  ...  0.0  0.0  0.0000     NaN   0.6917  0.0714\n",
      "0.98  61   0   1   0  0.9839  0.0  ...  0.0  0.0  0.0000     NaN   0.6917  0.0714\n",
      "0.99  61   0   1   0  0.9839  0.0  ...  0.0  0.0  0.0000     NaN   0.6917  0.0714\n",
      "1.00  61   0   1   0  0.9839  0.0  ...  0.0  0.0  0.0000     NaN   0.6917  0.0714\n",
      "1.01  61   0   1   0  0.9839  0.0  ...  0.0  0.0  0.0000     NaN   0.6917  0.0714\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.01   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.02   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.03   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.04   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP    Acc   Sn  ...  FPR   Rc     Pr      F1  ROC-AUC  F-max\n",
      "t                                 ...                                         \n",
      "0.00   0  54   0   8  0.129  1.0  ...  1.0  1.0  0.129  0.2286      1.0    1.0\n",
      "0.01   0  54   0   8  0.129  1.0  ...  1.0  1.0  0.129  0.2286      1.0    1.0\n",
      "0.02   0  54   0   8  0.129  1.0  ...  1.0  1.0  0.129  0.2286      1.0    1.0\n",
      "0.03   0  54   0   8  0.129  1.0  ...  1.0  1.0  0.129  0.2286      1.0    1.0\n",
      "0.04   0  54   0   8  0.129  1.0  ...  1.0  1.0  0.129  0.2286      1.0    1.0\n",
      "...   ..  ..  ..  ..    ...  ...  ...  ...  ...    ...     ...      ...    ...\n",
      "0.97  54   0   8   0  0.871  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "0.98  54   0   8   0  0.871  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "0.99  54   0   8   0  0.871  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "1.00  54   0   8   0  0.871  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "1.01  54   0   8   0  0.871  0.0  ...  0.0  0.0  0.000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286   0.9542  0.7368\n",
      "0.01   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286   0.9542  0.7368\n",
      "0.02   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286   0.9542  0.7368\n",
      "0.03  21  32   0   8  0.4754  1.0  ...  0.6038  1.0  0.2000  0.3333   0.9542  0.7368\n",
      "0.04  22  31   0   8  0.4918  1.0  ...  0.5849  1.0  0.2051  0.3404   0.9542  0.7368\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.9542  0.7368\n",
      "0.98  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.9542  0.7368\n",
      "0.99  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.9542  0.7368\n",
      "1.00  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.9542  0.7368\n",
      "1.01  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.9542  0.7368\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  35   0  27  0.4355  1.0  ...  1.0000  1.0  0.4355  0.6067   0.9027  0.8936\n",
      "0.01   0  35   0  27  0.4355  1.0  ...  1.0000  1.0  0.4355  0.6067   0.9027  0.8936\n",
      "0.02   2  32   0  26  0.4667  1.0  ...  0.9412  1.0  0.4483  0.6190   0.9027  0.8936\n",
      "0.03   2  32   0  26  0.4667  1.0  ...  0.9412  1.0  0.4483  0.6190   0.9027  0.8936\n",
      "0.04   2  32   0  26  0.4667  1.0  ...  0.9412  1.0  0.4483  0.6190   0.9027  0.8936\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9027  0.8936\n",
      "0.98  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9027  0.8936\n",
      "0.99  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9027  0.8936\n",
      "1.00  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9027  0.8936\n",
      "1.01  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9027  0.8936\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "Total correct samples: 571?571\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015589  0.044423\n",
      "4      0.015568  0.044409\n",
      "...         ...       ...\n",
      "18013  0.000055  0.000231\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002367  0.011945\n",
      "18016  0.002302  0.011713\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.6768 - acc: 0.6027 - auROC: 0.4772 - val_loss: 0.6068 - val_acc: 0.6793 - val_auROC: 0.5664\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.5882 - acc: 0.7021 - auROC: 0.5631 - val_loss: 0.5279 - val_acc: 0.7655 - val_auROC: 0.6768\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.5423 - acc: 0.7435 - auROC: 0.6063 - val_loss: 0.5205 - val_acc: 0.7552 - val_auROC: 0.6321\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.5183 - acc: 0.7626 - auROC: 0.6424 - val_loss: 0.4965 - val_acc: 0.8172 - val_auROC: 0.6916\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.4920 - acc: 0.7813 - auROC: 0.7063 - val_loss: 0.4806 - val_acc: 0.8069 - val_auROC: 0.7137\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4749 - acc: 0.7922 - auROC: 0.7218 - val_loss: 0.4907 - val_acc: 0.7966 - val_auROC: 0.6759\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4588 - acc: 0.8058 - auROC: 0.7541 - val_loss: 0.4603 - val_acc: 0.8138 - val_auROC: 0.7379\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4420 - acc: 0.8172 - auROC: 0.7814 - val_loss: 0.4598 - val_acc: 0.8172 - val_auROC: 0.7341\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.4342 - acc: 0.8191 - auROC: 0.7926 - val_loss: 0.4565 - val_acc: 0.8172 - val_auROC: 0.7358\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4315 - acc: 0.8250 - auROC: 0.7912 - val_loss: 0.4426 - val_acc: 0.8241 - val_auROC: 0.7744\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.4252 - acc: 0.8187 - auROC: 0.8051 - val_loss: 0.4374 - val_acc: 0.8241 - val_auROC: 0.7782\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.4211 - acc: 0.8187 - auROC: 0.8044 - val_loss: 0.4290 - val_acc: 0.8207 - val_auROC: 0.7841\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4124 - acc: 0.8250 - auROC: 0.8145 - val_loss: 0.4119 - val_acc: 0.8310 - val_auROC: 0.8213\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4091 - acc: 0.8304 - auROC: 0.8133 - val_loss: 0.4249 - val_acc: 0.8172 - val_auROC: 0.7875\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4065 - acc: 0.8343 - auROC: 0.8121 - val_loss: 0.4258 - val_acc: 0.8000 - val_auROC: 0.7834\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4058 - acc: 0.8366 - auROC: 0.8108 - val_loss: 0.4098 - val_acc: 0.8103 - val_auROC: 0.8111\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3940 - acc: 0.8448 - auROC: 0.8290 - val_loss: 0.4063 - val_acc: 0.8069 - val_auROC: 0.8197\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3826 - acc: 0.8491 - auROC: 0.8471 - val_loss: 0.3943 - val_acc: 0.8207 - val_auROC: 0.8344\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3761 - acc: 0.8511 - auROC: 0.8556 - val_loss: 0.3883 - val_acc: 0.8276 - val_auROC: 0.8438\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3734 - acc: 0.8526 - auROC: 0.8564 - val_loss: 0.3848 - val_acc: 0.8379 - val_auROC: 0.8446\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3679 - acc: 0.8530 - auROC: 0.8646 - val_loss: 0.3871 - val_acc: 0.8379 - val_auROC: 0.8343\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3624 - acc: 0.8519 - auROC: 0.8730 - val_loss: 0.3839 - val_acc: 0.8448 - val_auROC: 0.8444\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3641 - acc: 0.8534 - auROC: 0.8731 - val_loss: 0.3991 - val_acc: 0.8379 - val_auROC: 0.8084\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3722 - acc: 0.8495 - auROC: 0.8485 - val_loss: 0.3965 - val_acc: 0.8241 - val_auROC: 0.8188\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3625 - acc: 0.8589 - auROC: 0.8635 - val_loss: 0.3828 - val_acc: 0.8207 - val_auROC: 0.8419\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3528 - acc: 0.8639 - auROC: 0.8795 - val_loss: 0.3645 - val_acc: 0.8379 - val_auROC: 0.8800\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3429 - acc: 0.8694 - auROC: 0.8961 - val_loss: 0.3620 - val_acc: 0.8414 - val_auROC: 0.8741\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3358 - acc: 0.8838 - auROC: 0.8995 - val_loss: 0.3541 - val_acc: 0.8517 - val_auROC: 0.8790\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3272 - acc: 0.8865 - auROC: 0.9072 - val_loss: 0.3636 - val_acc: 0.8379 - val_auROC: 0.8668\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3790 - acc: 0.8635 - auROC: 0.8278 - val_loss: 0.3854 - val_acc: 0.8448 - val_auROC: 0.8224\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3741 - acc: 0.8585 - auROC: 0.8416 - val_loss: 0.3562 - val_acc: 0.8552 - val_auROC: 0.8695\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3567 - acc: 0.8639 - auROC: 0.8669 - val_loss: 0.3394 - val_acc: 0.8655 - val_auROC: 0.8927\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3418 - acc: 0.8772 - auROC: 0.8858 - val_loss: 0.3375 - val_acc: 0.8586 - val_auROC: 0.8957\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3354 - acc: 0.8756 - auROC: 0.8924 - val_loss: 0.3387 - val_acc: 0.8621 - val_auROC: 0.8924\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3227 - acc: 0.8877 - auROC: 0.9061 - val_loss: 0.3344 - val_acc: 0.8690 - val_auROC: 0.8898\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3140 - acc: 0.8908 - auROC: 0.9128 - val_loss: 0.3294 - val_acc: 0.8690 - val_auROC: 0.8919\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3078 - acc: 0.8936 - auROC: 0.9180 - val_loss: 0.3234 - val_acc: 0.8724 - val_auROC: 0.8934\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3053 - acc: 0.8947 - auROC: 0.9192 - val_loss: 0.3248 - val_acc: 0.8828 - val_auROC: 0.8922\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2980 - acc: 0.8986 - auROC: 0.9255 - val_loss: 0.3178 - val_acc: 0.8931 - val_auROC: 0.8978\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2943 - acc: 0.8998 - auROC: 0.9271 - val_loss: 0.3142 - val_acc: 0.8897 - val_auROC: 0.9009\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2895 - acc: 0.9025 - auROC: 0.9302 - val_loss: 0.3128 - val_acc: 0.8897 - val_auROC: 0.9018\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2868 - acc: 0.9018 - auROC: 0.9320 - val_loss: 0.3107 - val_acc: 0.8966 - val_auROC: 0.9032\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2836 - acc: 0.9053 - auROC: 0.9337 - val_loss: 0.3091 - val_acc: 0.8897 - val_auROC: 0.9045\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2799 - acc: 0.9064 - auROC: 0.9371 - val_loss: 0.3037 - val_acc: 0.8966 - val_auROC: 0.9105\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2805 - acc: 0.9072 - auROC: 0.9382 - val_loss: 0.3009 - val_acc: 0.9000 - val_auROC: 0.9112\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2833 - acc: 0.9041 - auROC: 0.9364 - val_loss: 0.3084 - val_acc: 0.8931 - val_auROC: 0.9045\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2853 - acc: 0.9053 - auROC: 0.9292 - val_loss: 0.3099 - val_acc: 0.8966 - val_auROC: 0.9003\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2797 - acc: 0.9076 - auROC: 0.9348 - val_loss: 0.3213 - val_acc: 0.8793 - val_auROC: 0.8867\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2748 - acc: 0.9080 - auROC: 0.9393 - val_loss: 0.3076 - val_acc: 0.8931 - val_auROC: 0.8994\n",
      "Epoch 50/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2718 - acc: 0.9085 - auROC: 0.9425\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2686 - acc: 0.9123 - auROC: 0.9446 - val_loss: 0.3010 - val_acc: 0.9034 - val_auROC: 0.9055\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2679 - acc: 0.9131 - auROC: 0.9457 - val_loss: 0.3029 - val_acc: 0.8966 - val_auROC: 0.9056\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2637 - acc: 0.9142 - auROC: 0.9489 - val_loss: 0.3042 - val_acc: 0.8931 - val_auROC: 0.9043\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2620 - acc: 0.9154 - auROC: 0.9503 - val_loss: 0.3063 - val_acc: 0.8966 - val_auROC: 0.9004\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2602 - acc: 0.9162 - auROC: 0.9516 - val_loss: 0.3067 - val_acc: 0.8931 - val_auROC: 0.8999\n",
      "Epoch 55/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2586 - acc: 0.9156 - auROC: 0.9526\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2585 - acc: 0.9158 - auROC: 0.9527 - val_loss: 0.3016 - val_acc: 0.9000 - val_auROC: 0.9051\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2573 - acc: 0.9173 - auROC: 0.9539 - val_loss: 0.3013 - val_acc: 0.9000 - val_auROC: 0.9064\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2572 - acc: 0.9173 - auROC: 0.9538 - val_loss: 0.3013 - val_acc: 0.9000 - val_auROC: 0.9059\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2570 - acc: 0.9173 - auROC: 0.9539 - val_loss: 0.3011 - val_acc: 0.9000 - val_auROC: 0.9060\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2569 - acc: 0.9177 - auROC: 0.9540 - val_loss: 0.3006 - val_acc: 0.9000 - val_auROC: 0.9060\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2568 - acc: 0.9177 - auROC: 0.9541 - val_loss: 0.3004 - val_acc: 0.9000 - val_auROC: 0.9063\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.2567 - acc: 0.9177 - auROC: 0.9542 - val_loss: 0.2999 - val_acc: 0.9000 - val_auROC: 0.9068\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2566 - acc: 0.9181 - auROC: 0.9543 - val_loss: 0.2996 - val_acc: 0.9000 - val_auROC: 0.9069\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2565 - acc: 0.9181 - auROC: 0.9544 - val_loss: 0.2995 - val_acc: 0.9000 - val_auROC: 0.9070\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2563 - acc: 0.9181 - auROC: 0.9545 - val_loss: 0.2993 - val_acc: 0.9000 - val_auROC: 0.9071\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2562 - acc: 0.9185 - auROC: 0.9546 - val_loss: 0.2992 - val_acc: 0.9000 - val_auROC: 0.9071\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2562 - acc: 0.9185 - auROC: 0.9546 - val_loss: 0.2991 - val_acc: 0.9000 - val_auROC: 0.9063\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2560 - acc: 0.9193 - auROC: 0.9547 - val_loss: 0.2990 - val_acc: 0.9000 - val_auROC: 0.9063\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2559 - acc: 0.9193 - auROC: 0.9547 - val_loss: 0.2989 - val_acc: 0.9000 - val_auROC: 0.9063\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2559 - acc: 0.9193 - auROC: 0.9547 - val_loss: 0.2990 - val_acc: 0.9000 - val_auROC: 0.9063\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2557 - acc: 0.9197 - auROC: 0.9548 - val_loss: 0.2987 - val_acc: 0.8966 - val_auROC: 0.9062\n",
      "Epoch 71/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2556 - acc: 0.9197 - auROC: 0.9550 - val_loss: 0.2986 - val_acc: 0.8966 - val_auROC: 0.9064\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2555 - acc: 0.9201 - auROC: 0.9550 - val_loss: 0.2984 - val_acc: 0.8966 - val_auROC: 0.9067\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2554 - acc: 0.9201 - auROC: 0.9552 - val_loss: 0.2984 - val_acc: 0.9000 - val_auROC: 0.9072\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2553 - acc: 0.9201 - auROC: 0.9554 - val_loss: 0.2983 - val_acc: 0.9000 - val_auROC: 0.9073\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2552 - acc: 0.9201 - auROC: 0.9554 - val_loss: 0.2982 - val_acc: 0.9000 - val_auROC: 0.9073\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2551 - acc: 0.9201 - auROC: 0.9555 - val_loss: 0.2981 - val_acc: 0.9000 - val_auROC: 0.9072\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2550 - acc: 0.9201 - auROC: 0.9556 - val_loss: 0.2979 - val_acc: 0.9000 - val_auROC: 0.9073\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.2549 - acc: 0.9201 - auROC: 0.9556 - val_loss: 0.2978 - val_acc: 0.9000 - val_auROC: 0.9073\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2548 - acc: 0.9201 - auROC: 0.9557 - val_loss: 0.2976 - val_acc: 0.9000 - val_auROC: 0.9074\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2547 - acc: 0.9201 - auROC: 0.9559 - val_loss: 0.2975 - val_acc: 0.9000 - val_auROC: 0.9074\n",
      "Epoch 81/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.2546 - acc: 0.9201 - auROC: 0.9559 - val_loss: 0.2974 - val_acc: 0.9000 - val_auROC: 0.9076\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.2545 - acc: 0.9201 - auROC: 0.9560 - val_loss: 0.2973 - val_acc: 0.9000 - val_auROC: 0.9084\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.2545 - acc: 0.9201 - auROC: 0.9560 - val_loss: 0.2972 - val_acc: 0.9000 - val_auROC: 0.9084\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2544 - acc: 0.9201 - auROC: 0.9561 - val_loss: 0.2971 - val_acc: 0.9000 - val_auROC: 0.9083\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2543 - acc: 0.9205 - auROC: 0.9563 - val_loss: 0.2970 - val_acc: 0.9000 - val_auROC: 0.9083\n",
      "Epoch 86/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2542 - acc: 0.9201 - auROC: 0.9564 - val_loss: 0.2974 - val_acc: 0.9000 - val_auROC: 0.9082\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2541 - acc: 0.9201 - auROC: 0.9563 - val_loss: 0.2974 - val_acc: 0.9000 - val_auROC: 0.9083\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2540 - acc: 0.9205 - auROC: 0.9564 - val_loss: 0.2972 - val_acc: 0.9000 - val_auROC: 0.9087\n",
      "Epoch 89/300\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 0.2570 - acc: 0.9156 - auROC: 0.9552\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2539 - acc: 0.9205 - auROC: 0.9565 - val_loss: 0.2970 - val_acc: 0.8966 - val_auROC: 0.9087\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2538 - acc: 0.9209 - auROC: 0.9566 - val_loss: 0.2968 - val_acc: 0.8966 - val_auROC: 0.9089\n",
      "Epoch 91/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2537 - acc: 0.9209 - auROC: 0.9566 - val_loss: 0.2967 - val_acc: 0.8966 - val_auROC: 0.9090\n",
      "Epoch 92/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2536 - acc: 0.9209 - auROC: 0.9567 - val_loss: 0.2967 - val_acc: 0.8966 - val_auROC: 0.9090\n",
      "Epoch 93/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2535 - acc: 0.9209 - auROC: 0.9568 - val_loss: 0.2966 - val_acc: 0.8966 - val_auROC: 0.9091\n",
      "Epoch 94/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2534 - acc: 0.9209 - auROC: 0.9569 - val_loss: 0.2965 - val_acc: 0.8966 - val_auROC: 0.9095\n",
      "Epoch 95/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2533 - acc: 0.9209 - auROC: 0.9570 - val_loss: 0.2963 - val_acc: 0.8966 - val_auROC: 0.9095\n",
      "Epoch 96/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2531 - acc: 0.9209 - auROC: 0.9571 - val_loss: 0.2962 - val_acc: 0.8966 - val_auROC: 0.9095\n",
      "Epoch 97/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.2531 - acc: 0.9209 - auROC: 0.9572 - val_loss: 0.2962 - val_acc: 0.8966 - val_auROC: 0.9096\n",
      "Epoch 98/300\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.2530 - acc: 0.9205 - auROC: 0.9573 - val_loss: 0.2961 - val_acc: 0.8966 - val_auROC: 0.9097\n",
      "Epoch 99/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.2529 - acc: 0.9205 - auROC: 0.9574 - val_loss: 0.2960 - val_acc: 0.8966 - val_auROC: 0.9099\n",
      "Epoch 100/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2528 - acc: 0.9205 - auROC: 0.9574 - val_loss: 0.2959 - val_acc: 0.8966 - val_auROC: 0.9106\n",
      "Epoch 101/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2527 - acc: 0.9205 - auROC: 0.9575 - val_loss: 0.2958 - val_acc: 0.8966 - val_auROC: 0.9106\n",
      "Epoch 102/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2526 - acc: 0.9205 - auROC: 0.9576 - val_loss: 0.2959 - val_acc: 0.8966 - val_auROC: 0.9102\n",
      "Epoch 103/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2526 - acc: 0.9209 - auROC: 0.9574 - val_loss: 0.2965 - val_acc: 0.8966 - val_auROC: 0.9108\n",
      "Epoch 104/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2527 - acc: 0.9201 - auROC: 0.9573 - val_loss: 0.2965 - val_acc: 0.8966 - val_auROC: 0.9108\n",
      "Epoch 105/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2527 - acc: 0.9201 - auROC: 0.9573 - val_loss: 0.2961 - val_acc: 0.8966 - val_auROC: 0.9120\n",
      "Epoch 106/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2528 - acc: 0.9209 - auROC: 0.9573 - val_loss: 0.2958 - val_acc: 0.8966 - val_auROC: 0.9124\n",
      "Epoch 107/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2527 - acc: 0.9209 - auROC: 0.9574 - val_loss: 0.2954 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 108/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2525 - acc: 0.9209 - auROC: 0.9575 - val_loss: 0.2952 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 109/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2523 - acc: 0.9209 - auROC: 0.9577 - val_loss: 0.2951 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 110/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2521 - acc: 0.9209 - auROC: 0.9577 - val_loss: 0.2950 - val_acc: 0.8966 - val_auROC: 0.9113\n",
      "Epoch 111/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2520 - acc: 0.9209 - auROC: 0.9578 - val_loss: 0.2949 - val_acc: 0.8966 - val_auROC: 0.9114\n",
      "Epoch 112/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2519 - acc: 0.9209 - auROC: 0.9578 - val_loss: 0.2950 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 113/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2518 - acc: 0.9209 - auROC: 0.9578 - val_loss: 0.2953 - val_acc: 0.8966 - val_auROC: 0.9113\n",
      "Epoch 114/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2517 - acc: 0.9209 - auROC: 0.9580 - val_loss: 0.2955 - val_acc: 0.8966 - val_auROC: 0.9113\n",
      "Epoch 115/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2517 - acc: 0.9209 - auROC: 0.9581 - val_loss: 0.2957 - val_acc: 0.8966 - val_auROC: 0.9112\n",
      "Epoch 116/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2516 - acc: 0.9209 - auROC: 0.9582 - val_loss: 0.2958 - val_acc: 0.8966 - val_auROC: 0.9113\n",
      "Epoch 117/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2515 - acc: 0.9209 - auROC: 0.9582 - val_loss: 0.2957 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 118/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2514 - acc: 0.9212 - auROC: 0.9583 - val_loss: 0.2956 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 119/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2513 - acc: 0.9212 - auROC: 0.9583 - val_loss: 0.2956 - val_acc: 0.8966 - val_auROC: 0.9114\n",
      "Epoch 120/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2513 - acc: 0.9216 - auROC: 0.9584 - val_loss: 0.2956 - val_acc: 0.8966 - val_auROC: 0.9114\n",
      "Epoch 121/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2512 - acc: 0.9216 - auROC: 0.9584 - val_loss: 0.2955 - val_acc: 0.8966 - val_auROC: 0.9115\n",
      "Epoch 122/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2511 - acc: 0.9216 - auROC: 0.9584 - val_loss: 0.2954 - val_acc: 0.8966 - val_auROC: 0.9113\n",
      "Epoch 123/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2511 - acc: 0.9216 - auROC: 0.9584 - val_loss: 0.2954 - val_acc: 0.8966 - val_auROC: 0.9113\n",
      "Epoch 124/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2510 - acc: 0.9216 - auROC: 0.9585 - val_loss: 0.2954 - val_acc: 0.8966 - val_auROC: 0.9112\n",
      "Epoch 125/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2509 - acc: 0.9216 - auROC: 0.9585 - val_loss: 0.2954 - val_acc: 0.8966 - val_auROC: 0.9112\n",
      "Epoch 126/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2451 - acc: 0.9250 - auROC: 0.9622Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2508 - acc: 0.9216 - auROC: 0.9585 - val_loss: 0.2954 - val_acc: 0.8966 - val_auROC: 0.9112\n",
      "Epoch 00126: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 21,795\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 126/425\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 0.2519 - acc: 0.9209 - auROC: 0.9578 - val_loss: 0.2948 - val_acc: 0.8966 - val_auROC: 0.9130\n",
      "Epoch 127/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2496 - acc: 0.9236 - auROC: 0.9593 - val_loss: 0.2933 - val_acc: 0.8966 - val_auROC: 0.9158\n",
      "Epoch 128/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2482 - acc: 0.9248 - auROC: 0.9602 - val_loss: 0.2928 - val_acc: 0.9000 - val_auROC: 0.9164\n",
      "Epoch 129/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2473 - acc: 0.9248 - auROC: 0.9608 - val_loss: 0.2908 - val_acc: 0.9000 - val_auROC: 0.9180\n",
      "Epoch 130/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2470 - acc: 0.9244 - auROC: 0.9611 - val_loss: 0.2901 - val_acc: 0.9000 - val_auROC: 0.9190\n",
      "Epoch 131/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2467 - acc: 0.9255 - auROC: 0.9610 - val_loss: 0.2903 - val_acc: 0.9000 - val_auROC: 0.9188\n",
      "Epoch 132/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2458 - acc: 0.9248 - auROC: 0.9612 - val_loss: 0.2896 - val_acc: 0.9000 - val_auROC: 0.9187\n",
      "Epoch 133/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2451 - acc: 0.9244 - auROC: 0.9617 - val_loss: 0.2882 - val_acc: 0.9000 - val_auROC: 0.9189\n",
      "Epoch 134/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2446 - acc: 0.9244 - auROC: 0.9622 - val_loss: 0.2873 - val_acc: 0.9000 - val_auROC: 0.9193\n",
      "Epoch 135/425\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2440 - acc: 0.9248 - auROC: 0.9626 - val_loss: 0.2871 - val_acc: 0.9000 - val_auROC: 0.9195\n",
      "Epoch 136/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2436 - acc: 0.9251 - auROC: 0.9629 - val_loss: 0.2870 - val_acc: 0.9000 - val_auROC: 0.9194\n",
      "Epoch 137/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2433 - acc: 0.9251 - auROC: 0.9630 - val_loss: 0.2869 - val_acc: 0.9000 - val_auROC: 0.9192\n",
      "Epoch 138/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2428 - acc: 0.9251 - auROC: 0.9632 - val_loss: 0.2868 - val_acc: 0.9000 - val_auROC: 0.9183\n",
      "Epoch 139/425\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2425 - acc: 0.9255 - auROC: 0.9635 - val_loss: 0.2866 - val_acc: 0.9000 - val_auROC: 0.9187\n",
      "Epoch 140/425\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2421 - acc: 0.9251 - auROC: 0.9636 - val_loss: 0.2864 - val_acc: 0.9000 - val_auROC: 0.9203\n",
      "Epoch 141/425\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2419 - acc: 0.9251 - auROC: 0.9639 - val_loss: 0.2865 - val_acc: 0.9000 - val_auROC: 0.9203\n",
      "Epoch 142/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2417 - acc: 0.9255 - auROC: 0.9640 - val_loss: 0.2864 - val_acc: 0.9000 - val_auROC: 0.9203\n",
      "Epoch 143/425\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.2411 - acc: 0.9251 - auROC: 0.9644 - val_loss: 0.2860 - val_acc: 0.9000 - val_auROC: 0.9204\n",
      "Epoch 144/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2406 - acc: 0.9255 - auROC: 0.9646 - val_loss: 0.2860 - val_acc: 0.9000 - val_auROC: 0.9202\n",
      "Epoch 145/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2402 - acc: 0.9267 - auROC: 0.9647 - val_loss: 0.2859 - val_acc: 0.9034 - val_auROC: 0.9201\n",
      "Epoch 146/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2402 - acc: 0.9263 - auROC: 0.9648 - val_loss: 0.2853 - val_acc: 0.9034 - val_auROC: 0.9205\n",
      "Epoch 147/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2395 - acc: 0.9267 - auROC: 0.9651 - val_loss: 0.2848 - val_acc: 0.9034 - val_auROC: 0.9207\n",
      "Epoch 148/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2392 - acc: 0.9263 - auROC: 0.9654 - val_loss: 0.2839 - val_acc: 0.9034 - val_auROC: 0.9210\n",
      "Epoch 149/425\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2389 - acc: 0.9267 - auROC: 0.9656 - val_loss: 0.2838 - val_acc: 0.9034 - val_auROC: 0.9209\n",
      "Epoch 150/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2384 - acc: 0.9271 - auROC: 0.9658 - val_loss: 0.2839 - val_acc: 0.9034 - val_auROC: 0.9207\n",
      "Epoch 151/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2381 - acc: 0.9275 - auROC: 0.9660 - val_loss: 0.2838 - val_acc: 0.9034 - val_auROC: 0.9212\n",
      "Epoch 152/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2377 - acc: 0.9283 - auROC: 0.9661 - val_loss: 0.2834 - val_acc: 0.9034 - val_auROC: 0.9213\n",
      "Epoch 153/425\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2374 - acc: 0.9290 - auROC: 0.9662 - val_loss: 0.2827 - val_acc: 0.9034 - val_auROC: 0.9217\n",
      "Epoch 154/425\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.2369 - acc: 0.9290 - auROC: 0.9663 - val_loss: 0.2825 - val_acc: 0.9034 - val_auROC: 0.9219\n",
      "Epoch 155/425\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2364 - acc: 0.9302 - auROC: 0.9664 - val_loss: 0.2843 - val_acc: 0.9000 - val_auROC: 0.9207\n",
      "Epoch 156/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2369 - acc: 0.9290 - auROC: 0.9660 - val_loss: 0.2891 - val_acc: 0.8931 - val_auROC: 0.9170\n",
      "Epoch 157/425\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2375 - acc: 0.9290 - auROC: 0.9654 - val_loss: 0.2880 - val_acc: 0.8931 - val_auROC: 0.9175\n",
      "Epoch 158/425\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2366 - acc: 0.9290 - auROC: 0.9660 - val_loss: 0.2859 - val_acc: 0.8966 - val_auROC: 0.9183\n",
      "Epoch 159/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2358 - acc: 0.9294 - auROC: 0.9664 - val_loss: 0.2847 - val_acc: 0.9034 - val_auROC: 0.9189\n",
      "Epoch 160/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2356 - acc: 0.9302 - auROC: 0.9665 - val_loss: 0.2838 - val_acc: 0.9034 - val_auROC: 0.9194\n",
      "Epoch 161/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2356 - acc: 0.9298 - auROC: 0.9663 - val_loss: 0.2833 - val_acc: 0.9034 - val_auROC: 0.9187\n",
      "Epoch 162/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2355 - acc: 0.9306 - auROC: 0.9663 - val_loss: 0.2841 - val_acc: 0.9034 - val_auROC: 0.9194\n",
      "Epoch 163/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2354 - acc: 0.9306 - auROC: 0.9664 - val_loss: 0.2833 - val_acc: 0.9034 - val_auROC: 0.9204\n",
      "Epoch 164/425\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2348 - acc: 0.9306 - auROC: 0.9666 - val_loss: 0.2822 - val_acc: 0.9069 - val_auROC: 0.9209\n",
      "Epoch 165/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2342 - acc: 0.9310 - auROC: 0.9670 - val_loss: 0.2819 - val_acc: 0.9069 - val_auROC: 0.9210\n",
      "Epoch 166/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2337 - acc: 0.9314 - auROC: 0.9673 - val_loss: 0.2809 - val_acc: 0.9069 - val_auROC: 0.9209\n",
      "Epoch 167/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2332 - acc: 0.9314 - auROC: 0.9675 - val_loss: 0.2804 - val_acc: 0.9069 - val_auROC: 0.9208\n",
      "Epoch 168/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2329 - acc: 0.9318 - auROC: 0.9675 - val_loss: 0.2800 - val_acc: 0.9069 - val_auROC: 0.9212\n",
      "Epoch 169/425\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2326 - acc: 0.9322 - auROC: 0.9677 - val_loss: 0.2801 - val_acc: 0.9069 - val_auROC: 0.9214\n",
      "Epoch 170/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2324 - acc: 0.9322 - auROC: 0.9677 - val_loss: 0.2801 - val_acc: 0.9069 - val_auROC: 0.9213\n",
      "Epoch 171/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2321 - acc: 0.9322 - auROC: 0.9678 - val_loss: 0.2796 - val_acc: 0.9069 - val_auROC: 0.9217\n",
      "Epoch 172/425\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2320 - acc: 0.9322 - auROC: 0.9679 - val_loss: 0.2789 - val_acc: 0.9103 - val_auROC: 0.9223\n",
      "Epoch 173/425\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2319 - acc: 0.9322 - auROC: 0.9679 - val_loss: 0.2793 - val_acc: 0.9103 - val_auROC: 0.9221\n",
      "Epoch 174/425\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2316 - acc: 0.9322 - auROC: 0.9680 - val_loss: 0.2794 - val_acc: 0.9069 - val_auROC: 0.9220\n",
      "Epoch 175/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2314 - acc: 0.9326 - auROC: 0.9680 - val_loss: 0.2793 - val_acc: 0.9069 - val_auROC: 0.9218\n",
      "Epoch 176/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2310 - acc: 0.9326 - auROC: 0.9680 - val_loss: 0.2797 - val_acc: 0.9069 - val_auROC: 0.9213\n",
      "Epoch 177/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2308 - acc: 0.9329 - auROC: 0.9682 - val_loss: 0.2804 - val_acc: 0.9069 - val_auROC: 0.9211\n",
      "Epoch 178/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2305 - acc: 0.9329 - auROC: 0.9682 - val_loss: 0.2803 - val_acc: 0.9069 - val_auROC: 0.9222\n",
      "Epoch 179/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2303 - acc: 0.9326 - auROC: 0.9682 - val_loss: 0.2796 - val_acc: 0.9069 - val_auROC: 0.9222\n",
      "Epoch 180/425\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2302 - acc: 0.9326 - auROC: 0.9683 - val_loss: 0.2790 - val_acc: 0.9069 - val_auROC: 0.9223\n",
      "Epoch 181/425\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2300 - acc: 0.9326 - auROC: 0.9683 - val_loss: 0.2788 - val_acc: 0.9069 - val_auROC: 0.9226\n",
      "Epoch 182/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2299 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2787 - val_acc: 0.9069 - val_auROC: 0.9230\n",
      "Epoch 183/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2297 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2786 - val_acc: 0.9069 - val_auROC: 0.9228\n",
      "Epoch 184/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2297 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2787 - val_acc: 0.9069 - val_auROC: 0.9227\n",
      "Epoch 185/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2306 - acc: 0.9318 - auROC: 0.9678 - val_loss: 0.2789 - val_acc: 0.9069 - val_auROC: 0.9221\n",
      "Epoch 186/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2311 - acc: 0.9322 - auROC: 0.9675 - val_loss: 0.2785 - val_acc: 0.9103 - val_auROC: 0.9223\n",
      "Epoch 187/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2303 - acc: 0.9322 - auROC: 0.9679 - val_loss: 0.2775 - val_acc: 0.9103 - val_auROC: 0.9234\n",
      "Epoch 188/425\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2297 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2772 - val_acc: 0.9103 - val_auROC: 0.9227\n",
      "Epoch 189/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2295 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2770 - val_acc: 0.9069 - val_auROC: 0.9234\n",
      "Epoch 190/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2294 - acc: 0.9326 - auROC: 0.9685 - val_loss: 0.2775 - val_acc: 0.9069 - val_auROC: 0.9229\n",
      "Epoch 191/425\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2298 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2777 - val_acc: 0.9069 - val_auROC: 0.9232\n",
      "Epoch 192/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2296 - acc: 0.9326 - auROC: 0.9684 - val_loss: 0.2773 - val_acc: 0.9069 - val_auROC: 0.9238\n",
      "Epoch 193/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2291 - acc: 0.9333 - auROC: 0.9686 - val_loss: 0.2771 - val_acc: 0.9103 - val_auROC: 0.9240\n",
      "Epoch 194/425\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2288 - acc: 0.9333 - auROC: 0.9687 - val_loss: 0.2763 - val_acc: 0.9138 - val_auROC: 0.9242\n",
      "Epoch 195/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2284 - acc: 0.9341 - auROC: 0.9689 - val_loss: 0.2749 - val_acc: 0.9138 - val_auROC: 0.9247\n",
      "Epoch 196/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2286 - acc: 0.9337 - auROC: 0.9691 - val_loss: 0.2769 - val_acc: 0.9138 - val_auROC: 0.9229\n",
      "Epoch 197/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2294 - acc: 0.9337 - auROC: 0.9689 - val_loss: 0.2773 - val_acc: 0.9138 - val_auROC: 0.9222\n",
      "Epoch 198/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2289 - acc: 0.9341 - auROC: 0.9690 - val_loss: 0.2765 - val_acc: 0.9103 - val_auROC: 0.9238\n",
      "Epoch 199/425\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2283 - acc: 0.9341 - auROC: 0.9691 - val_loss: 0.2755 - val_acc: 0.9103 - val_auROC: 0.9249\n",
      "Epoch 200/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2274 - acc: 0.9349 - auROC: 0.9693 - val_loss: 0.2745 - val_acc: 0.9069 - val_auROC: 0.9253\n",
      "Epoch 201/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2272 - acc: 0.9349 - auROC: 0.9694 - val_loss: 0.2739 - val_acc: 0.9103 - val_auROC: 0.9262\n",
      "Epoch 202/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2270 - acc: 0.9353 - auROC: 0.9694 - val_loss: 0.2734 - val_acc: 0.9103 - val_auROC: 0.9261\n",
      "Epoch 203/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2268 - acc: 0.9353 - auROC: 0.9695 - val_loss: 0.2739 - val_acc: 0.9103 - val_auROC: 0.9258\n",
      "Epoch 204/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2267 - acc: 0.9353 - auROC: 0.9695 - val_loss: 0.2736 - val_acc: 0.9103 - val_auROC: 0.9257\n",
      "Epoch 205/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2270 - acc: 0.9353 - auROC: 0.9693 - val_loss: 0.2770 - val_acc: 0.9138 - val_auROC: 0.9241\n",
      "Epoch 206/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2276 - acc: 0.9353 - auROC: 0.9691 - val_loss: 0.2758 - val_acc: 0.9138 - val_auROC: 0.9242\n",
      "Epoch 207/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2269 - acc: 0.9353 - auROC: 0.9694 - val_loss: 0.2739 - val_acc: 0.9138 - val_auROC: 0.9259\n",
      "Epoch 208/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2264 - acc: 0.9353 - auROC: 0.9694 - val_loss: 0.2733 - val_acc: 0.9138 - val_auROC: 0.9264\n",
      "Epoch 209/425\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2263 - acc: 0.9353 - auROC: 0.9695 - val_loss: 0.2725 - val_acc: 0.9138 - val_auROC: 0.9266\n",
      "Epoch 210/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2263 - acc: 0.9353 - auROC: 0.9694 - val_loss: 0.2704 - val_acc: 0.9172 - val_auROC: 0.9278\n",
      "Epoch 211/425\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2265 - acc: 0.9353 - auROC: 0.9692 - val_loss: 0.2701 - val_acc: 0.9172 - val_auROC: 0.9278\n",
      "Epoch 212/425\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2270 - acc: 0.9349 - auROC: 0.9686 - val_loss: 0.2692 - val_acc: 0.9172 - val_auROC: 0.9288\n",
      "Epoch 213/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2269 - acc: 0.9353 - auROC: 0.9687 - val_loss: 0.2694 - val_acc: 0.9172 - val_auROC: 0.9284\n",
      "Epoch 214/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2261 - acc: 0.9357 - auROC: 0.9693 - val_loss: 0.2704 - val_acc: 0.9172 - val_auROC: 0.9276\n",
      "Epoch 215/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2259 - acc: 0.9357 - auROC: 0.9694 - val_loss: 0.2712 - val_acc: 0.9138 - val_auROC: 0.9269\n",
      "Epoch 216/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2256 - acc: 0.9361 - auROC: 0.9695 - val_loss: 0.2717 - val_acc: 0.9103 - val_auROC: 0.9269\n",
      "Epoch 217/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2254 - acc: 0.9361 - auROC: 0.9695 - val_loss: 0.2719 - val_acc: 0.9103 - val_auROC: 0.9271\n",
      "Epoch 218/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2253 - acc: 0.9361 - auROC: 0.9696 - val_loss: 0.2717 - val_acc: 0.9103 - val_auROC: 0.9271\n",
      "Epoch 219/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2251 - acc: 0.9361 - auROC: 0.9695 - val_loss: 0.2716 - val_acc: 0.9103 - val_auROC: 0.9272\n",
      "Epoch 220/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2250 - acc: 0.9361 - auROC: 0.9696 - val_loss: 0.2713 - val_acc: 0.9103 - val_auROC: 0.9272\n",
      "Epoch 221/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2249 - acc: 0.9361 - auROC: 0.9695 - val_loss: 0.2712 - val_acc: 0.9103 - val_auROC: 0.9278\n",
      "Epoch 222/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2248 - acc: 0.9361 - auROC: 0.9695 - val_loss: 0.2710 - val_acc: 0.9138 - val_auROC: 0.9277\n",
      "Epoch 223/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2248 - acc: 0.9361 - auROC: 0.9695 - val_loss: 0.2708 - val_acc: 0.9138 - val_auROC: 0.9284\n",
      "Epoch 224/425\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2247 - acc: 0.9361 - auROC: 0.9696 - val_loss: 0.2707 - val_acc: 0.9138 - val_auROC: 0.9284\n",
      "Epoch 225/425\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2246 - acc: 0.9361 - auROC: 0.9696 - val_loss: 0.2706 - val_acc: 0.9138 - val_auROC: 0.9285\n",
      "Epoch 226/425\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2245 - acc: 0.9361 - auROC: 0.9696 - val_loss: 0.2705 - val_acc: 0.9138 - val_auROC: 0.9280\n",
      "Epoch 227/425\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.2244 - acc: 0.9361 - auROC: 0.9696Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2244 - acc: 0.9361 - auROC: 0.9696 - val_loss: 0.2703 - val_acc: 0.9138 - val_auROC: 0.9280\n",
      "Epoch 00227: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196531    0.0\n",
      "1     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196531    0.0\n",
      "2     0.0    0.0    0.0 -0.003108  ...    0.0 -0.198135 -0.196531    0.0\n",
      "3     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196531    0.0\n",
      "4     0.0    0.0    0.0  0.568506  ...    0.0 -0.198135 -0.196531    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0  3.113152  ...    0.0 -0.198135 -0.196531    0.0\n",
      "60    0.0    0.0    0.0 -0.350213  ...    0.0 -0.198135 -0.196531    0.0\n",
      "61    0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196531    0.0\n",
      "62    0.0    0.0    0.0 -0.139348  ...    0.0 -0.198135 -0.196531    0.0\n",
      "63    0.0    0.0    0.0 -0.350679  ...    0.0 -0.198135 -0.196531    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  61   0   1  0.0161  1.0  ...  1.0000  1.0  0.0161  0.0317      1.0    1.0\n",
      "0.01   0  61   0   1  0.0161  1.0  ...  1.0000  1.0  0.0161  0.0317      1.0    1.0\n",
      "0.02   4  56   0   1  0.0820  1.0  ...  0.9333  1.0  0.0175  0.0345      1.0    1.0\n",
      "0.03  22  38   0   1  0.3770  1.0  ...  0.6333  1.0  0.0256  0.0500      1.0    1.0\n",
      "0.04  41  19   0   1  0.6885  1.0  ...  0.3167  1.0  0.0500  0.0952      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896   0.9837  0.9333\n",
      "0.01   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896   0.9837  0.9333\n",
      "0.02   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896   0.9837  0.9333\n",
      "0.03   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896   0.9837  0.9333\n",
      "0.04   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896   0.9837  0.9333\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN   0.9837  0.9333\n",
      "0.98  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN   0.9837  0.9333\n",
      "0.99  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN   0.9837  0.9333\n",
      "1.00  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN   0.9837  0.9333\n",
      "1.01  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN   0.9837  0.9333\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.01   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.02   0  53   0   8  0.1311  1.0  ...  1.0000  1.0  0.1311  0.2319      1.0    1.0\n",
      "0.03   1  52   0   8  0.1475  1.0  ...  0.9811  1.0  0.1333  0.2353      1.0    1.0\n",
      "0.04  15  38   0   8  0.3770  1.0  ...  0.7170  1.0  0.1739  0.2963      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286   0.8396  0.7273\n",
      "0.01   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286   0.8396  0.7273\n",
      "0.02   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286   0.8396  0.7273\n",
      "0.03   2  51   0   8  0.1639  1.0  ...  0.9623  1.0  0.1356  0.2388   0.8396  0.7273\n",
      "0.04   5  48   0   8  0.2131  1.0  ...  0.9057  1.0  0.1429  0.2500   0.8396  0.7273\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.8396  0.7273\n",
      "0.98  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.8396  0.7273\n",
      "0.99  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.8396  0.7273\n",
      "1.00  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.8396  0.7273\n",
      "1.01  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN   0.8396  0.7273\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  35   0  27  0.4355  1.0  ...  1.0000  1.0  0.4355  0.6067   0.9926  0.9818\n",
      "0.01   0  35   0  27  0.4355  1.0  ...  1.0000  1.0  0.4355  0.6067   0.9926  0.9818\n",
      "0.02   0  35   0  27  0.4355  1.0  ...  1.0000  1.0  0.4355  0.6067   0.9926  0.9818\n",
      "0.03   0  34   0  27  0.4426  1.0  ...  1.0000  1.0  0.4426  0.6136   0.9926  0.9818\n",
      "0.04   1  33   0  27  0.4590  1.0  ...  0.9706  1.0  0.4500  0.6207   0.9926  0.9818\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9926  0.9818\n",
      "0.98  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9926  0.9818\n",
      "0.99  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9926  0.9818\n",
      "1.00  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9926  0.9818\n",
      "1.01  35   0  27   0  0.5645  0.0  ...  0.0000  0.0  0.0000     NaN   0.9926  0.9818\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "Total correct samples: 571?571\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015589  0.044423\n",
      "4      0.015568  0.044409\n",
      "...         ...       ...\n",
      "18013  0.000055  0.000231\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002367  0.011945\n",
      "18016  0.002302  0.011713\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.6548 - acc: 0.6062 - auROC: 0.5205 - val_loss: 0.6176 - val_acc: 0.6759 - val_auROC: 0.5288\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.5957 - acc: 0.7111 - auROC: 0.5797 - val_loss: 0.5770 - val_acc: 0.7621 - val_auROC: 0.6005\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.5547 - acc: 0.7747 - auROC: 0.6557 - val_loss: 0.5363 - val_acc: 0.7897 - val_auROC: 0.6636\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.5137 - acc: 0.7914 - auROC: 0.6587 - val_loss: 0.5164 - val_acc: 0.7862 - val_auROC: 0.6355\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4881 - acc: 0.8133 - auROC: 0.6905 - val_loss: 0.5011 - val_acc: 0.7828 - val_auROC: 0.6591\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4671 - acc: 0.8183 - auROC: 0.7253 - val_loss: 0.4844 - val_acc: 0.7966 - val_auROC: 0.6943\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4517 - acc: 0.8183 - auROC: 0.7595 - val_loss: 0.4678 - val_acc: 0.7862 - val_auROC: 0.7397\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4401 - acc: 0.8195 - auROC: 0.7793 - val_loss: 0.4581 - val_acc: 0.7966 - val_auROC: 0.7535\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4285 - acc: 0.8207 - auROC: 0.7980 - val_loss: 0.4478 - val_acc: 0.8172 - val_auROC: 0.7532\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4211 - acc: 0.8207 - auROC: 0.8041 - val_loss: 0.4507 - val_acc: 0.8172 - val_auROC: 0.7309\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4164 - acc: 0.8273 - auROC: 0.7975 - val_loss: 0.4421 - val_acc: 0.8000 - val_auROC: 0.7588\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4101 - acc: 0.8191 - auROC: 0.8140 - val_loss: 0.4303 - val_acc: 0.8034 - val_auROC: 0.7785\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4092 - acc: 0.8199 - auROC: 0.8166 - val_loss: 0.4263 - val_acc: 0.8172 - val_auROC: 0.7833\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4020 - acc: 0.8324 - auROC: 0.8172 - val_loss: 0.4276 - val_acc: 0.8103 - val_auROC: 0.7774\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4008 - acc: 0.8269 - auROC: 0.8286 - val_loss: 0.4349 - val_acc: 0.8034 - val_auROC: 0.7528\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4337 - acc: 0.8261 - auROC: 0.7529 - val_loss: 0.4226 - val_acc: 0.8034 - val_auROC: 0.7833\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4186 - acc: 0.8125 - auROC: 0.7857 - val_loss: 0.4349 - val_acc: 0.8034 - val_auROC: 0.7650\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4092 - acc: 0.8265 - auROC: 0.7915 - val_loss: 0.4342 - val_acc: 0.8000 - val_auROC: 0.7473\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4003 - acc: 0.8378 - auROC: 0.8064 - val_loss: 0.4228 - val_acc: 0.8000 - val_auROC: 0.7782\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3868 - acc: 0.8288 - auROC: 0.8339 - val_loss: 0.4256 - val_acc: 0.8241 - val_auROC: 0.7619\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3835 - acc: 0.8398 - auROC: 0.8298 - val_loss: 0.4078 - val_acc: 0.8172 - val_auROC: 0.8037\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3733 - acc: 0.8433 - auROC: 0.8475 - val_loss: 0.4015 - val_acc: 0.8172 - val_auROC: 0.8137\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3691 - acc: 0.8437 - auROC: 0.8535 - val_loss: 0.3989 - val_acc: 0.8241 - val_auROC: 0.8158\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.3729 - acc: 0.8448 - auROC: 0.8454 - val_loss: 0.3946 - val_acc: 0.8379 - val_auROC: 0.8145\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3614 - acc: 0.8515 - auROC: 0.8603 - val_loss: 0.3889 - val_acc: 0.8276 - val_auROC: 0.8257\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3524 - acc: 0.8620 - auROC: 0.8727 - val_loss: 0.3838 - val_acc: 0.8207 - val_auROC: 0.8366\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3611 - acc: 0.8452 - auROC: 0.8666 - val_loss: 0.4001 - val_acc: 0.8172 - val_auROC: 0.8019\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3524 - acc: 0.8565 - auROC: 0.8661 - val_loss: 0.3900 - val_acc: 0.8310 - val_auROC: 0.8187\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3490 - acc: 0.8593 - auROC: 0.8770 - val_loss: 0.3875 - val_acc: 0.8276 - val_auROC: 0.8207\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3485 - acc: 0.8706 - auROC: 0.8689 - val_loss: 0.3880 - val_acc: 0.8379 - val_auROC: 0.8178\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3517 - acc: 0.8667 - auROC: 0.8672 - val_loss: 0.3741 - val_acc: 0.8276 - val_auROC: 0.8435\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3441 - acc: 0.8710 - auROC: 0.8774 - val_loss: 0.3692 - val_acc: 0.8414 - val_auROC: 0.8381\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3363 - acc: 0.8791 - auROC: 0.8862 - val_loss: 0.3535 - val_acc: 0.8379 - val_auROC: 0.8645\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3333 - acc: 0.8752 - auROC: 0.8865 - val_loss: 0.3759 - val_acc: 0.8448 - val_auROC: 0.8419\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.3396 - acc: 0.8690 - auROC: 0.8782 - val_loss: 0.3498 - val_acc: 0.8276 - val_auROC: 0.8906\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3219 - acc: 0.8788 - auROC: 0.8986 - val_loss: 0.3805 - val_acc: 0.8621 - val_auROC: 0.8292\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3169 - acc: 0.8897 - auROC: 0.9036 - val_loss: 0.3398 - val_acc: 0.8310 - val_auROC: 0.8896\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3036 - acc: 0.8912 - auROC: 0.9174 - val_loss: 0.3323 - val_acc: 0.8552 - val_auROC: 0.8877\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3015 - acc: 0.8920 - auROC: 0.9197 - val_loss: 0.3230 - val_acc: 0.8517 - val_auROC: 0.8986\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2928 - acc: 0.9002 - auROC: 0.9270 - val_loss: 0.3698 - val_acc: 0.8517 - val_auROC: 0.8309\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3387 - acc: 0.8674 - auROC: 0.8701 - val_loss: 0.3696 - val_acc: 0.8276 - val_auROC: 0.8532\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3434 - acc: 0.8600 - auROC: 0.8724 - val_loss: 0.3308 - val_acc: 0.8793 - val_auROC: 0.8915\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3388 - acc: 0.8608 - auROC: 0.8732 - val_loss: 0.3319 - val_acc: 0.8207 - val_auROC: 0.9058\n",
      "Epoch 44/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3344 - acc: 0.8562 - auROC: 0.8865\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3340 - acc: 0.8565 - auROC: 0.8868 - val_loss: 0.3264 - val_acc: 0.8621 - val_auROC: 0.8940\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3148 - acc: 0.8756 - auROC: 0.8995 - val_loss: 0.3226 - val_acc: 0.8621 - val_auROC: 0.9003\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3087 - acc: 0.8819 - auROC: 0.9075 - val_loss: 0.3225 - val_acc: 0.8586 - val_auROC: 0.8989\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3068 - acc: 0.8862 - auROC: 0.9101 - val_loss: 0.3198 - val_acc: 0.8655 - val_auROC: 0.9051\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3042 - acc: 0.8815 - auROC: 0.9151 - val_loss: 0.3229 - val_acc: 0.8655 - val_auROC: 0.9032\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3031 - acc: 0.8811 - auROC: 0.9156 - val_loss: 0.3358 - val_acc: 0.8759 - val_auROC: 0.8796\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3029 - acc: 0.8846 - auROC: 0.9136 - val_loss: 0.3379 - val_acc: 0.8655 - val_auROC: 0.8721\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2983 - acc: 0.8869 - auROC: 0.9177 - val_loss: 0.3294 - val_acc: 0.8655 - val_auROC: 0.8828\n",
      "Epoch 52/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2955 - acc: 0.8926 - auROC: 0.9193\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2959 - acc: 0.8924 - auROC: 0.9189 - val_loss: 0.3239 - val_acc: 0.8690 - val_auROC: 0.8901\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2945 - acc: 0.8936 - auROC: 0.9197 - val_loss: 0.3234 - val_acc: 0.8690 - val_auROC: 0.8903\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2942 - acc: 0.8932 - auROC: 0.9199 - val_loss: 0.3233 - val_acc: 0.8690 - val_auROC: 0.8893\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2936 - acc: 0.8936 - auROC: 0.9204 - val_loss: 0.3237 - val_acc: 0.8690 - val_auROC: 0.8897\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2932 - acc: 0.8940 - auROC: 0.9207 - val_loss: 0.3237 - val_acc: 0.8655 - val_auROC: 0.8886\n",
      "Epoch 57/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.2930 - acc: 0.8941 - auROC: 0.9206\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2929 - acc: 0.8943 - auROC: 0.9207 - val_loss: 0.3235 - val_acc: 0.8655 - val_auROC: 0.8885\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2926 - acc: 0.8936 - auROC: 0.9211 - val_loss: 0.3233 - val_acc: 0.8655 - val_auROC: 0.8895\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2923 - acc: 0.8936 - auROC: 0.9213 - val_loss: 0.3230 - val_acc: 0.8655 - val_auROC: 0.8897\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2920 - acc: 0.8936 - auROC: 0.9216 - val_loss: 0.3226 - val_acc: 0.8655 - val_auROC: 0.8906\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2917 - acc: 0.8940 - auROC: 0.9219 - val_loss: 0.3220 - val_acc: 0.8690 - val_auROC: 0.8919\n",
      "Epoch 62/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2940 - acc: 0.8942 - auROC: 0.9200Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2914 - acc: 0.8951 - auROC: 0.9225 - val_loss: 0.3217 - val_acc: 0.8690 - val_auROC: 0.8918\n",
      "Epoch 00062: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 21,795\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 62/361\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.3033 - acc: 0.8850 - auROC: 0.9151 - val_loss: 0.3071 - val_acc: 0.8724 - val_auROC: 0.9195\n",
      "Epoch 63/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2935 - acc: 0.8908 - auROC: 0.9257 - val_loss: 0.3026 - val_acc: 0.8793 - val_auROC: 0.9228\n",
      "Epoch 64/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2879 - acc: 0.8994 - auROC: 0.9301 - val_loss: 0.2998 - val_acc: 0.8759 - val_auROC: 0.9266\n",
      "Epoch 65/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2830 - acc: 0.9025 - auROC: 0.9338 - val_loss: 0.2977 - val_acc: 0.8862 - val_auROC: 0.9289\n",
      "Epoch 66/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2787 - acc: 0.9088 - auROC: 0.9373 - val_loss: 0.2947 - val_acc: 0.8931 - val_auROC: 0.9317\n",
      "Epoch 67/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2752 - acc: 0.9119 - auROC: 0.9404 - val_loss: 0.2902 - val_acc: 0.9000 - val_auROC: 0.9358\n",
      "Epoch 68/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2722 - acc: 0.9150 - auROC: 0.9425 - val_loss: 0.2872 - val_acc: 0.9069 - val_auROC: 0.9380\n",
      "Epoch 69/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2693 - acc: 0.9193 - auROC: 0.9444 - val_loss: 0.2828 - val_acc: 0.9069 - val_auROC: 0.9406\n",
      "Epoch 70/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2664 - acc: 0.9216 - auROC: 0.9469 - val_loss: 0.2811 - val_acc: 0.9069 - val_auROC: 0.9417\n",
      "Epoch 71/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2642 - acc: 0.9224 - auROC: 0.9482 - val_loss: 0.2801 - val_acc: 0.9034 - val_auROC: 0.9425\n",
      "Epoch 72/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2632 - acc: 0.9240 - auROC: 0.9486 - val_loss: 0.2834 - val_acc: 0.9034 - val_auROC: 0.9362\n",
      "Epoch 73/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2625 - acc: 0.9259 - auROC: 0.9487 - val_loss: 0.2788 - val_acc: 0.9103 - val_auROC: 0.9403\n",
      "Epoch 74/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2604 - acc: 0.9283 - auROC: 0.9502 - val_loss: 0.2739 - val_acc: 0.9172 - val_auROC: 0.9435\n",
      "Epoch 75/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2590 - acc: 0.9279 - auROC: 0.9510 - val_loss: 0.2694 - val_acc: 0.9207 - val_auROC: 0.9470\n",
      "Epoch 76/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2577 - acc: 0.9298 - auROC: 0.9518 - val_loss: 0.2680 - val_acc: 0.9241 - val_auROC: 0.9482\n",
      "Epoch 77/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2561 - acc: 0.9326 - auROC: 0.9525 - val_loss: 0.2689 - val_acc: 0.9207 - val_auROC: 0.9483\n",
      "Epoch 78/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2547 - acc: 0.9361 - auROC: 0.9531 - val_loss: 0.2682 - val_acc: 0.9172 - val_auROC: 0.9495\n",
      "Epoch 79/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2536 - acc: 0.9365 - auROC: 0.9540 - val_loss: 0.2686 - val_acc: 0.9138 - val_auROC: 0.9494\n",
      "Epoch 80/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2524 - acc: 0.9372 - auROC: 0.9546 - val_loss: 0.2664 - val_acc: 0.9138 - val_auROC: 0.9508\n",
      "Epoch 81/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2513 - acc: 0.9400 - auROC: 0.9555 - val_loss: 0.2639 - val_acc: 0.9207 - val_auROC: 0.9519\n",
      "Epoch 82/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2502 - acc: 0.9404 - auROC: 0.9560 - val_loss: 0.2622 - val_acc: 0.9172 - val_auROC: 0.9528\n",
      "Epoch 83/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2489 - acc: 0.9407 - auROC: 0.9566 - val_loss: 0.2612 - val_acc: 0.9207 - val_auROC: 0.9541\n",
      "Epoch 84/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2477 - acc: 0.9411 - auROC: 0.9572 - val_loss: 0.2606 - val_acc: 0.9207 - val_auROC: 0.9545\n",
      "Epoch 85/361\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.2467 - acc: 0.9411 - auROC: 0.9577 - val_loss: 0.2601 - val_acc: 0.9241 - val_auROC: 0.9544\n",
      "Epoch 86/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2459 - acc: 0.9404 - auROC: 0.9580 - val_loss: 0.2594 - val_acc: 0.9241 - val_auROC: 0.9556\n",
      "Epoch 87/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2449 - acc: 0.9411 - auROC: 0.9586 - val_loss: 0.2586 - val_acc: 0.9207 - val_auROC: 0.9569\n",
      "Epoch 88/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2440 - acc: 0.9411 - auROC: 0.9592 - val_loss: 0.2576 - val_acc: 0.9207 - val_auROC: 0.9568\n",
      "Epoch 89/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2437 - acc: 0.9419 - auROC: 0.9596 - val_loss: 0.2562 - val_acc: 0.9241 - val_auROC: 0.9574\n",
      "Epoch 90/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2434 - acc: 0.9415 - auROC: 0.9599 - val_loss: 0.2554 - val_acc: 0.9241 - val_auROC: 0.9583\n",
      "Epoch 91/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2425 - acc: 0.9423 - auROC: 0.9602 - val_loss: 0.2547 - val_acc: 0.9241 - val_auROC: 0.9587\n",
      "Epoch 92/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2417 - acc: 0.9442 - auROC: 0.9605 - val_loss: 0.2538 - val_acc: 0.9241 - val_auROC: 0.9593\n",
      "Epoch 93/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2410 - acc: 0.9442 - auROC: 0.9609 - val_loss: 0.2531 - val_acc: 0.9276 - val_auROC: 0.9597\n",
      "Epoch 94/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2401 - acc: 0.9442 - auROC: 0.9612 - val_loss: 0.2526 - val_acc: 0.9241 - val_auROC: 0.9602\n",
      "Epoch 95/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2397 - acc: 0.9431 - auROC: 0.9612 - val_loss: 0.2511 - val_acc: 0.9241 - val_auROC: 0.9596\n",
      "Epoch 96/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2395 - acc: 0.9431 - auROC: 0.9610 - val_loss: 0.2509 - val_acc: 0.9241 - val_auROC: 0.9606\n",
      "Epoch 97/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2394 - acc: 0.9439 - auROC: 0.9605 - val_loss: 0.2489 - val_acc: 0.9345 - val_auROC: 0.9617\n",
      "Epoch 98/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2392 - acc: 0.9454 - auROC: 0.9605 - val_loss: 0.2487 - val_acc: 0.9345 - val_auROC: 0.9617\n",
      "Epoch 99/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2382 - acc: 0.9470 - auROC: 0.9607 - val_loss: 0.2482 - val_acc: 0.9345 - val_auROC: 0.9608\n",
      "Epoch 100/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2370 - acc: 0.9470 - auROC: 0.9612 - val_loss: 0.2484 - val_acc: 0.9345 - val_auROC: 0.9593\n",
      "Epoch 101/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2359 - acc: 0.9481 - auROC: 0.9618 - val_loss: 0.2466 - val_acc: 0.9345 - val_auROC: 0.9632\n",
      "Epoch 102/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2354 - acc: 0.9485 - auROC: 0.9621 - val_loss: 0.2468 - val_acc: 0.9345 - val_auROC: 0.9641\n",
      "Epoch 103/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2347 - acc: 0.9505 - auROC: 0.9628 - val_loss: 0.2463 - val_acc: 0.9345 - val_auROC: 0.9642\n",
      "Epoch 104/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2339 - acc: 0.9509 - auROC: 0.9631 - val_loss: 0.2453 - val_acc: 0.9345 - val_auROC: 0.9643\n",
      "Epoch 105/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2329 - acc: 0.9513 - auROC: 0.9637 - val_loss: 0.2444 - val_acc: 0.9310 - val_auROC: 0.9640\n",
      "Epoch 106/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2324 - acc: 0.9517 - auROC: 0.9640 - val_loss: 0.2431 - val_acc: 0.9345 - val_auROC: 0.9647\n",
      "Epoch 107/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2316 - acc: 0.9517 - auROC: 0.9650 - val_loss: 0.2430 - val_acc: 0.9241 - val_auROC: 0.9656\n",
      "Epoch 108/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2312 - acc: 0.9505 - auROC: 0.9663 - val_loss: 0.2488 - val_acc: 0.9310 - val_auROC: 0.9645\n",
      "Epoch 109/361\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2321 - acc: 0.9497 - auROC: 0.9662 - val_loss: 0.2483 - val_acc: 0.9276 - val_auROC: 0.9672\n",
      "Epoch 110/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2313 - acc: 0.9493 - auROC: 0.9664 - val_loss: 0.2464 - val_acc: 0.9276 - val_auROC: 0.9675\n",
      "Epoch 111/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2302 - acc: 0.9513 - auROC: 0.9670 - val_loss: 0.2449 - val_acc: 0.9276 - val_auROC: 0.9681\n",
      "Epoch 112/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2294 - acc: 0.9520 - auROC: 0.9672 - val_loss: 0.2437 - val_acc: 0.9241 - val_auROC: 0.9689\n",
      "Epoch 113/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2287 - acc: 0.9532 - auROC: 0.9676 - val_loss: 0.2427 - val_acc: 0.9241 - val_auROC: 0.9684\n",
      "Epoch 114/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2284 - acc: 0.9528 - auROC: 0.9681 - val_loss: 0.2431 - val_acc: 0.9276 - val_auROC: 0.9666\n",
      "Epoch 115/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2284 - acc: 0.9517 - auROC: 0.9689 - val_loss: 0.2422 - val_acc: 0.9276 - val_auROC: 0.9679\n",
      "Epoch 116/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2276 - acc: 0.9520 - auROC: 0.9688 - val_loss: 0.2407 - val_acc: 0.9276 - val_auROC: 0.9695\n",
      "Epoch 117/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2270 - acc: 0.9517 - auROC: 0.9689 - val_loss: 0.2381 - val_acc: 0.9241 - val_auROC: 0.9697\n",
      "Epoch 118/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2268 - acc: 0.9513 - auROC: 0.9690 - val_loss: 0.2368 - val_acc: 0.9241 - val_auROC: 0.9712\n",
      "Epoch 119/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2261 - acc: 0.9532 - auROC: 0.9691 - val_loss: 0.2362 - val_acc: 0.9276 - val_auROC: 0.9723\n",
      "Epoch 120/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2255 - acc: 0.9544 - auROC: 0.9691 - val_loss: 0.2361 - val_acc: 0.9276 - val_auROC: 0.9732\n",
      "Epoch 121/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2250 - acc: 0.9544 - auROC: 0.9691 - val_loss: 0.2356 - val_acc: 0.9276 - val_auROC: 0.9731\n",
      "Epoch 122/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2246 - acc: 0.9540 - auROC: 0.9693 - val_loss: 0.2353 - val_acc: 0.9276 - val_auROC: 0.9731\n",
      "Epoch 123/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2241 - acc: 0.9536 - auROC: 0.9695 - val_loss: 0.2350 - val_acc: 0.9276 - val_auROC: 0.9734\n",
      "Epoch 124/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2237 - acc: 0.9540 - auROC: 0.9696 - val_loss: 0.2345 - val_acc: 0.9310 - val_auROC: 0.9735\n",
      "Epoch 125/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2233 - acc: 0.9544 - auROC: 0.9698 - val_loss: 0.2341 - val_acc: 0.9310 - val_auROC: 0.9735\n",
      "Epoch 126/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2229 - acc: 0.9544 - auROC: 0.9699 - val_loss: 0.2335 - val_acc: 0.9310 - val_auROC: 0.9740\n",
      "Epoch 127/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2226 - acc: 0.9552 - auROC: 0.9701 - val_loss: 0.2331 - val_acc: 0.9310 - val_auROC: 0.9741\n",
      "Epoch 128/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2222 - acc: 0.9552 - auROC: 0.9702 - val_loss: 0.2326 - val_acc: 0.9310 - val_auROC: 0.9745\n",
      "Epoch 129/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2219 - acc: 0.9552 - auROC: 0.9701 - val_loss: 0.2318 - val_acc: 0.9345 - val_auROC: 0.9751\n",
      "Epoch 130/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2218 - acc: 0.9552 - auROC: 0.9701 - val_loss: 0.2318 - val_acc: 0.9345 - val_auROC: 0.9753\n",
      "Epoch 131/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2213 - acc: 0.9556 - auROC: 0.9703 - val_loss: 0.2316 - val_acc: 0.9345 - val_auROC: 0.9746\n",
      "Epoch 132/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2209 - acc: 0.9552 - auROC: 0.9706 - val_loss: 0.2310 - val_acc: 0.9345 - val_auROC: 0.9748\n",
      "Epoch 133/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2206 - acc: 0.9556 - auROC: 0.9708 - val_loss: 0.2305 - val_acc: 0.9345 - val_auROC: 0.9745\n",
      "Epoch 134/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2204 - acc: 0.9559 - auROC: 0.9708 - val_loss: 0.2304 - val_acc: 0.9345 - val_auROC: 0.9747\n",
      "Epoch 135/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2202 - acc: 0.9563 - auROC: 0.9709 - val_loss: 0.2314 - val_acc: 0.9345 - val_auROC: 0.9746\n",
      "Epoch 136/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2198 - acc: 0.9556 - auROC: 0.9705 - val_loss: 0.2298 - val_acc: 0.9345 - val_auROC: 0.9757\n",
      "Epoch 137/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2212 - acc: 0.9540 - auROC: 0.9700 - val_loss: 0.2292 - val_acc: 0.9345 - val_auROC: 0.9754\n",
      "Epoch 138/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2229 - acc: 0.9528 - auROC: 0.9696 - val_loss: 0.2292 - val_acc: 0.9345 - val_auROC: 0.9761\n",
      "Epoch 139/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2220 - acc: 0.9524 - auROC: 0.9702 - val_loss: 0.2285 - val_acc: 0.9345 - val_auROC: 0.9765\n",
      "Epoch 140/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2203 - acc: 0.9544 - auROC: 0.9706 - val_loss: 0.2284 - val_acc: 0.9345 - val_auROC: 0.9763\n",
      "Epoch 141/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2190 - acc: 0.9548 - auROC: 0.9713 - val_loss: 0.2279 - val_acc: 0.9310 - val_auROC: 0.9762\n",
      "Epoch 142/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2183 - acc: 0.9567 - auROC: 0.9715 - val_loss: 0.2273 - val_acc: 0.9379 - val_auROC: 0.9764\n",
      "Epoch 143/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2179 - acc: 0.9571 - auROC: 0.9716 - val_loss: 0.2269 - val_acc: 0.9448 - val_auROC: 0.9766\n",
      "Epoch 144/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2177 - acc: 0.9571 - auROC: 0.9716 - val_loss: 0.2261 - val_acc: 0.9448 - val_auROC: 0.9766\n",
      "Epoch 145/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2172 - acc: 0.9591 - auROC: 0.9719 - val_loss: 0.2257 - val_acc: 0.9448 - val_auROC: 0.9773\n",
      "Epoch 146/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2166 - acc: 0.9610 - auROC: 0.9723 - val_loss: 0.2254 - val_acc: 0.9448 - val_auROC: 0.9771\n",
      "Epoch 147/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2162 - acc: 0.9602 - auROC: 0.9725 - val_loss: 0.2250 - val_acc: 0.9448 - val_auROC: 0.9776\n",
      "Epoch 148/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2158 - acc: 0.9595 - auROC: 0.9726 - val_loss: 0.2249 - val_acc: 0.9414 - val_auROC: 0.9772\n",
      "Epoch 149/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2155 - acc: 0.9606 - auROC: 0.9730 - val_loss: 0.2247 - val_acc: 0.9448 - val_auROC: 0.9772\n",
      "Epoch 150/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2151 - acc: 0.9614 - auROC: 0.9731 - val_loss: 0.2243 - val_acc: 0.9483 - val_auROC: 0.9772\n",
      "Epoch 151/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2147 - acc: 0.9618 - auROC: 0.9733 - val_loss: 0.2240 - val_acc: 0.9448 - val_auROC: 0.9774\n",
      "Epoch 152/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2144 - acc: 0.9622 - auROC: 0.9733 - val_loss: 0.2237 - val_acc: 0.9448 - val_auROC: 0.9773\n",
      "Epoch 153/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2142 - acc: 0.9618 - auROC: 0.9733 - val_loss: 0.2236 - val_acc: 0.9448 - val_auROC: 0.9770\n",
      "Epoch 154/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2139 - acc: 0.9618 - auROC: 0.9735 - val_loss: 0.2231 - val_acc: 0.9414 - val_auROC: 0.9769\n",
      "Epoch 155/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2138 - acc: 0.9622 - auROC: 0.9733 - val_loss: 0.2222 - val_acc: 0.9414 - val_auROC: 0.9772\n",
      "Epoch 156/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2135 - acc: 0.9626 - auROC: 0.9735 - val_loss: 0.2226 - val_acc: 0.9414 - val_auROC: 0.9774\n",
      "Epoch 157/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2132 - acc: 0.9630 - auROC: 0.9738 - val_loss: 0.2225 - val_acc: 0.9483 - val_auROC: 0.9773\n",
      "Epoch 158/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2127 - acc: 0.9630 - auROC: 0.9738 - val_loss: 0.2219 - val_acc: 0.9448 - val_auROC: 0.9778\n",
      "Epoch 159/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2126 - acc: 0.9622 - auROC: 0.9736 - val_loss: 0.2203 - val_acc: 0.9379 - val_auROC: 0.9784\n",
      "Epoch 160/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2125 - acc: 0.9622 - auROC: 0.9735 - val_loss: 0.2196 - val_acc: 0.9414 - val_auROC: 0.9784\n",
      "Epoch 161/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2119 - acc: 0.9634 - auROC: 0.9738 - val_loss: 0.2194 - val_acc: 0.9414 - val_auROC: 0.9781\n",
      "Epoch 162/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2116 - acc: 0.9634 - auROC: 0.9741 - val_loss: 0.2192 - val_acc: 0.9448 - val_auROC: 0.9781\n",
      "Epoch 163/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2115 - acc: 0.9637 - auROC: 0.9742 - val_loss: 0.2187 - val_acc: 0.9483 - val_auROC: 0.9780\n",
      "Epoch 164/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2117 - acc: 0.9653 - auROC: 0.9741 - val_loss: 0.2188 - val_acc: 0.9483 - val_auROC: 0.9798\n",
      "Epoch 165/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2122 - acc: 0.9657 - auROC: 0.9744 - val_loss: 0.2187 - val_acc: 0.9483 - val_auROC: 0.9800\n",
      "Epoch 166/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2120 - acc: 0.9649 - auROC: 0.9744 - val_loss: 0.2179 - val_acc: 0.9483 - val_auROC: 0.9785\n",
      "Epoch 167/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2110 - acc: 0.9630 - auROC: 0.9745 - val_loss: 0.2179 - val_acc: 0.9448 - val_auROC: 0.9785\n",
      "Epoch 168/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2106 - acc: 0.9622 - auROC: 0.9746 - val_loss: 0.2178 - val_acc: 0.9448 - val_auROC: 0.9784\n",
      "Epoch 169/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2102 - acc: 0.9630 - auROC: 0.9747 - val_loss: 0.2176 - val_acc: 0.9448 - val_auROC: 0.9782\n",
      "Epoch 170/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2099 - acc: 0.9630 - auROC: 0.9746 - val_loss: 0.2177 - val_acc: 0.9483 - val_auROC: 0.9781\n",
      "Epoch 171/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2096 - acc: 0.9634 - auROC: 0.9746 - val_loss: 0.2177 - val_acc: 0.9483 - val_auROC: 0.9777\n",
      "Epoch 172/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2093 - acc: 0.9630 - auROC: 0.9746 - val_loss: 0.2178 - val_acc: 0.9483 - val_auROC: 0.9777\n",
      "Epoch 173/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2091 - acc: 0.9634 - auROC: 0.9747 - val_loss: 0.2175 - val_acc: 0.9483 - val_auROC: 0.9779\n",
      "Epoch 174/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2089 - acc: 0.9634 - auROC: 0.9747 - val_loss: 0.2177 - val_acc: 0.9483 - val_auROC: 0.9778\n",
      "Epoch 175/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2087 - acc: 0.9634 - auROC: 0.9748 - val_loss: 0.2175 - val_acc: 0.9483 - val_auROC: 0.9791\n",
      "Epoch 176/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2085 - acc: 0.9634 - auROC: 0.9747 - val_loss: 0.2172 - val_acc: 0.9483 - val_auROC: 0.9785\n",
      "Epoch 177/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2083 - acc: 0.9630 - auROC: 0.9747 - val_loss: 0.2166 - val_acc: 0.9483 - val_auROC: 0.9784\n",
      "Epoch 178/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2081 - acc: 0.9637 - auROC: 0.9750 - val_loss: 0.2158 - val_acc: 0.9517 - val_auROC: 0.9791\n",
      "Epoch 179/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2080 - acc: 0.9637 - auROC: 0.9750 - val_loss: 0.2155 - val_acc: 0.9517 - val_auROC: 0.9790\n",
      "Epoch 180/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2078 - acc: 0.9637 - auROC: 0.9750 - val_loss: 0.2158 - val_acc: 0.9483 - val_auROC: 0.9776\n",
      "Epoch 181/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2076 - acc: 0.9637 - auROC: 0.9749 - val_loss: 0.2160 - val_acc: 0.9483 - val_auROC: 0.9774\n",
      "Epoch 182/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2073 - acc: 0.9634 - auROC: 0.9750 - val_loss: 0.2160 - val_acc: 0.9483 - val_auROC: 0.9774\n",
      "Epoch 183/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2071 - acc: 0.9649 - auROC: 0.9751 - val_loss: 0.2158 - val_acc: 0.9483 - val_auROC: 0.9776\n",
      "Epoch 184/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2069 - acc: 0.9645 - auROC: 0.9753 - val_loss: 0.2156 - val_acc: 0.9483 - val_auROC: 0.9782\n",
      "Epoch 185/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2067 - acc: 0.9641 - auROC: 0.9753 - val_loss: 0.2153 - val_acc: 0.9483 - val_auROC: 0.9785\n",
      "Epoch 186/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2066 - acc: 0.9649 - auROC: 0.9754 - val_loss: 0.2148 - val_acc: 0.9483 - val_auROC: 0.9789\n",
      "Epoch 187/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2068 - acc: 0.9649 - auROC: 0.9754 - val_loss: 0.2148 - val_acc: 0.9483 - val_auROC: 0.9787\n",
      "Epoch 188/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2067 - acc: 0.9653 - auROC: 0.9754 - val_loss: 0.2158 - val_acc: 0.9517 - val_auROC: 0.9795\n",
      "Epoch 189/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2065 - acc: 0.9653 - auROC: 0.9754 - val_loss: 0.2153 - val_acc: 0.9552 - val_auROC: 0.9795\n",
      "Epoch 190/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2061 - acc: 0.9653 - auROC: 0.9755 - val_loss: 0.2146 - val_acc: 0.9483 - val_auROC: 0.9785\n",
      "Epoch 191/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2058 - acc: 0.9653 - auROC: 0.9755 - val_loss: 0.2141 - val_acc: 0.9483 - val_auROC: 0.9787\n",
      "Epoch 192/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2055 - acc: 0.9669 - auROC: 0.9758 - val_loss: 0.2138 - val_acc: 0.9552 - val_auROC: 0.9787\n",
      "Epoch 193/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2054 - acc: 0.9661 - auROC: 0.9759 - val_loss: 0.2142 - val_acc: 0.9448 - val_auROC: 0.9784\n",
      "Epoch 194/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2050 - acc: 0.9657 - auROC: 0.9761 - val_loss: 0.2145 - val_acc: 0.9448 - val_auROC: 0.9785\n",
      "Epoch 195/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2048 - acc: 0.9661 - auROC: 0.9767 - val_loss: 0.2145 - val_acc: 0.9517 - val_auROC: 0.9784\n",
      "Epoch 196/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2046 - acc: 0.9661 - auROC: 0.9769 - val_loss: 0.2141 - val_acc: 0.9483 - val_auROC: 0.9785\n",
      "Epoch 197/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2042 - acc: 0.9665 - auROC: 0.9769 - val_loss: 0.2135 - val_acc: 0.9517 - val_auROC: 0.9793\n",
      "Epoch 198/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2040 - acc: 0.9669 - auROC: 0.9768 - val_loss: 0.2132 - val_acc: 0.9517 - val_auROC: 0.9803\n",
      "Epoch 199/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2038 - acc: 0.9669 - auROC: 0.9770 - val_loss: 0.2131 - val_acc: 0.9552 - val_auROC: 0.9801\n",
      "Epoch 200/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2036 - acc: 0.9661 - auROC: 0.9769 - val_loss: 0.2122 - val_acc: 0.9517 - val_auROC: 0.9804\n",
      "Epoch 201/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2035 - acc: 0.9661 - auROC: 0.9768 - val_loss: 0.2119 - val_acc: 0.9552 - val_auROC: 0.9806\n",
      "Epoch 202/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2033 - acc: 0.9673 - auROC: 0.9769 - val_loss: 0.2125 - val_acc: 0.9586 - val_auROC: 0.9805\n",
      "Epoch 203/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2032 - acc: 0.9676 - auROC: 0.9774 - val_loss: 0.2135 - val_acc: 0.9586 - val_auROC: 0.9800\n",
      "Epoch 204/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2031 - acc: 0.9665 - auROC: 0.9773 - val_loss: 0.2121 - val_acc: 0.9552 - val_auROC: 0.9803\n",
      "Epoch 205/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2031 - acc: 0.9645 - auROC: 0.9772 - val_loss: 0.2116 - val_acc: 0.9552 - val_auROC: 0.9805\n",
      "Epoch 206/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2029 - acc: 0.9641 - auROC: 0.9772 - val_loss: 0.2112 - val_acc: 0.9552 - val_auROC: 0.9807\n",
      "Epoch 207/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2026 - acc: 0.9653 - auROC: 0.9772 - val_loss: 0.2116 - val_acc: 0.9552 - val_auROC: 0.9807\n",
      "Epoch 208/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2024 - acc: 0.9657 - auROC: 0.9776 - val_loss: 0.2120 - val_acc: 0.9621 - val_auROC: 0.9805\n",
      "Epoch 209/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2022 - acc: 0.9665 - auROC: 0.9777 - val_loss: 0.2119 - val_acc: 0.9621 - val_auROC: 0.9804\n",
      "Epoch 210/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2020 - acc: 0.9665 - auROC: 0.9777 - val_loss: 0.2117 - val_acc: 0.9517 - val_auROC: 0.9796\n",
      "Epoch 211/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2018 - acc: 0.9665 - auROC: 0.9778 - val_loss: 0.2119 - val_acc: 0.9517 - val_auROC: 0.9796\n",
      "Epoch 212/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2016 - acc: 0.9665 - auROC: 0.9779 - val_loss: 0.2131 - val_acc: 0.9517 - val_auROC: 0.9802\n",
      "Epoch 213/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2010 - acc: 0.9665 - auROC: 0.9781 - val_loss: 0.2121 - val_acc: 0.9517 - val_auROC: 0.9804\n",
      "Epoch 214/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2007 - acc: 0.9665 - auROC: 0.9783 - val_loss: 0.2114 - val_acc: 0.9586 - val_auROC: 0.9804\n",
      "Epoch 215/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2006 - acc: 0.9669 - auROC: 0.9783 - val_loss: 0.2120 - val_acc: 0.9586 - val_auROC: 0.9801\n",
      "Epoch 216/361\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2006 - acc: 0.9669 - auROC: 0.9783 - val_loss: 0.2112 - val_acc: 0.9586 - val_auROC: 0.9804\n",
      "Epoch 217/361\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2002 - acc: 0.9676 - auROC: 0.9784 - val_loss: 0.2107 - val_acc: 0.9586 - val_auROC: 0.9804\n",
      "Epoch 218/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2000 - acc: 0.9680 - auROC: 0.9785 - val_loss: 0.2107 - val_acc: 0.9586 - val_auROC: 0.9805\n",
      "Epoch 219/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1999 - acc: 0.9684 - auROC: 0.9786 - val_loss: 0.2104 - val_acc: 0.9586 - val_auROC: 0.9805\n",
      "Epoch 220/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1997 - acc: 0.9684 - auROC: 0.9787 - val_loss: 0.2103 - val_acc: 0.9586 - val_auROC: 0.9808\n",
      "Epoch 221/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1997 - acc: 0.9688 - auROC: 0.9787 - val_loss: 0.2095 - val_acc: 0.9655 - val_auROC: 0.9810\n",
      "Epoch 222/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1997 - acc: 0.9684 - auROC: 0.9787 - val_loss: 0.2088 - val_acc: 0.9586 - val_auROC: 0.9810\n",
      "Epoch 223/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1996 - acc: 0.9673 - auROC: 0.9787 - val_loss: 0.2085 - val_acc: 0.9586 - val_auROC: 0.9809\n",
      "Epoch 224/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1992 - acc: 0.9684 - auROC: 0.9786 - val_loss: 0.2087 - val_acc: 0.9621 - val_auROC: 0.9809\n",
      "Epoch 225/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1990 - acc: 0.9684 - auROC: 0.9789 - val_loss: 0.2084 - val_acc: 0.9621 - val_auROC: 0.9810\n",
      "Epoch 226/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1988 - acc: 0.9680 - auROC: 0.9788 - val_loss: 0.2081 - val_acc: 0.9621 - val_auROC: 0.9810\n",
      "Epoch 227/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1987 - acc: 0.9684 - auROC: 0.9789 - val_loss: 0.2083 - val_acc: 0.9621 - val_auROC: 0.9811\n",
      "Epoch 228/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1986 - acc: 0.9688 - auROC: 0.9791 - val_loss: 0.2082 - val_acc: 0.9621 - val_auROC: 0.9802\n",
      "Epoch 229/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1983 - acc: 0.9688 - auROC: 0.9791 - val_loss: 0.2078 - val_acc: 0.9621 - val_auROC: 0.9806\n",
      "Epoch 230/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1982 - acc: 0.9676 - auROC: 0.9791 - val_loss: 0.2075 - val_acc: 0.9621 - val_auROC: 0.9807\n",
      "Epoch 231/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1981 - acc: 0.9696 - auROC: 0.9791 - val_loss: 0.2076 - val_acc: 0.9655 - val_auROC: 0.9809\n",
      "Epoch 232/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1981 - acc: 0.9719 - auROC: 0.9793 - val_loss: 0.2076 - val_acc: 0.9655 - val_auROC: 0.9811\n",
      "Epoch 233/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1979 - acc: 0.9712 - auROC: 0.9794 - val_loss: 0.2078 - val_acc: 0.9655 - val_auROC: 0.9816\n",
      "Epoch 234/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1977 - acc: 0.9704 - auROC: 0.9795 - val_loss: 0.2078 - val_acc: 0.9655 - val_auROC: 0.9814\n",
      "Epoch 235/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1975 - acc: 0.9696 - auROC: 0.9794 - val_loss: 0.2074 - val_acc: 0.9655 - val_auROC: 0.9806\n",
      "Epoch 236/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1973 - acc: 0.9696 - auROC: 0.9794 - val_loss: 0.2073 - val_acc: 0.9655 - val_auROC: 0.9807\n",
      "Epoch 237/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1972 - acc: 0.9696 - auROC: 0.9795 - val_loss: 0.2071 - val_acc: 0.9655 - val_auROC: 0.9810\n",
      "Epoch 238/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1970 - acc: 0.9696 - auROC: 0.9796 - val_loss: 0.2069 - val_acc: 0.9655 - val_auROC: 0.9810\n",
      "Epoch 239/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1968 - acc: 0.9688 - auROC: 0.9797 - val_loss: 0.2068 - val_acc: 0.9621 - val_auROC: 0.9809\n",
      "Epoch 240/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1966 - acc: 0.9688 - auROC: 0.9798 - val_loss: 0.2067 - val_acc: 0.9621 - val_auROC: 0.9809\n",
      "Epoch 241/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1965 - acc: 0.9688 - auROC: 0.9798 - val_loss: 0.2066 - val_acc: 0.9621 - val_auROC: 0.9810\n",
      "Epoch 242/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1963 - acc: 0.9692 - auROC: 0.9799 - val_loss: 0.2066 - val_acc: 0.9621 - val_auROC: 0.9811\n",
      "Epoch 243/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1962 - acc: 0.9692 - auROC: 0.9801 - val_loss: 0.2066 - val_acc: 0.9621 - val_auROC: 0.9813\n",
      "Epoch 244/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1960 - acc: 0.9692 - auROC: 0.9801 - val_loss: 0.2063 - val_acc: 0.9655 - val_auROC: 0.9814\n",
      "Epoch 245/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1959 - acc: 0.9692 - auROC: 0.9802 - val_loss: 0.2062 - val_acc: 0.9655 - val_auROC: 0.9813\n",
      "Epoch 246/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1957 - acc: 0.9692 - auROC: 0.9803 - val_loss: 0.2061 - val_acc: 0.9621 - val_auROC: 0.9812\n",
      "Epoch 247/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1956 - acc: 0.9692 - auROC: 0.9803 - val_loss: 0.2057 - val_acc: 0.9655 - val_auROC: 0.9813\n",
      "Epoch 248/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1955 - acc: 0.9700 - auROC: 0.9801 - val_loss: 0.2052 - val_acc: 0.9655 - val_auROC: 0.9813\n",
      "Epoch 249/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1953 - acc: 0.9692 - auROC: 0.9802 - val_loss: 0.2051 - val_acc: 0.9655 - val_auROC: 0.9812\n",
      "Epoch 250/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1952 - acc: 0.9692 - auROC: 0.9802 - val_loss: 0.2052 - val_acc: 0.9690 - val_auROC: 0.9803\n",
      "Epoch 251/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1950 - acc: 0.9704 - auROC: 0.9803 - val_loss: 0.2056 - val_acc: 0.9690 - val_auROC: 0.9799\n",
      "Epoch 252/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1947 - acc: 0.9708 - auROC: 0.9806 - val_loss: 0.2054 - val_acc: 0.9690 - val_auROC: 0.9799\n",
      "Epoch 253/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1946 - acc: 0.9708 - auROC: 0.9807 - val_loss: 0.2051 - val_acc: 0.9690 - val_auROC: 0.9799\n",
      "Epoch 254/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1945 - acc: 0.9700 - auROC: 0.9807 - val_loss: 0.2053 - val_acc: 0.9690 - val_auROC: 0.9798\n",
      "Epoch 255/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1943 - acc: 0.9696 - auROC: 0.9807 - val_loss: 0.2052 - val_acc: 0.9690 - val_auROC: 0.9798\n",
      "Epoch 256/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1942 - acc: 0.9696 - auROC: 0.9807 - val_loss: 0.2045 - val_acc: 0.9690 - val_auROC: 0.9799\n",
      "Epoch 257/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1940 - acc: 0.9696 - auROC: 0.9807 - val_loss: 0.2043 - val_acc: 0.9690 - val_auROC: 0.9812\n",
      "Epoch 258/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1938 - acc: 0.9700 - auROC: 0.9808 - val_loss: 0.2046 - val_acc: 0.9690 - val_auROC: 0.9814\n",
      "Epoch 259/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1940 - acc: 0.9692 - auROC: 0.9809 - val_loss: 0.2040 - val_acc: 0.9655 - val_auROC: 0.9815\n",
      "Epoch 260/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1942 - acc: 0.9680 - auROC: 0.9808 - val_loss: 0.2037 - val_acc: 0.9655 - val_auROC: 0.9813\n",
      "Epoch 261/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1939 - acc: 0.9692 - auROC: 0.9808 - val_loss: 0.2034 - val_acc: 0.9655 - val_auROC: 0.9806\n",
      "Epoch 262/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1935 - acc: 0.9712 - auROC: 0.9808 - val_loss: 0.2029 - val_acc: 0.9690 - val_auROC: 0.9818\n",
      "Epoch 263/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1935 - acc: 0.9715 - auROC: 0.9807 - val_loss: 0.2025 - val_acc: 0.9690 - val_auROC: 0.9825\n",
      "Epoch 264/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1934 - acc: 0.9715 - auROC: 0.9804 - val_loss: 0.2023 - val_acc: 0.9690 - val_auROC: 0.9824\n",
      "Epoch 265/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1931 - acc: 0.9712 - auROC: 0.9806 - val_loss: 0.2026 - val_acc: 0.9690 - val_auROC: 0.9815\n",
      "Epoch 266/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1929 - acc: 0.9712 - auROC: 0.9808 - val_loss: 0.2030 - val_acc: 0.9690 - val_auROC: 0.9821\n",
      "Epoch 267/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1927 - acc: 0.9719 - auROC: 0.9809 - val_loss: 0.2032 - val_acc: 0.9690 - val_auROC: 0.9821\n",
      "Epoch 268/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1926 - acc: 0.9719 - auROC: 0.9809 - val_loss: 0.2029 - val_acc: 0.9690 - val_auROC: 0.9822\n",
      "Epoch 269/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1925 - acc: 0.9723 - auROC: 0.9808 - val_loss: 0.2024 - val_acc: 0.9690 - val_auROC: 0.9823\n",
      "Epoch 270/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1923 - acc: 0.9727 - auROC: 0.9809 - val_loss: 0.2020 - val_acc: 0.9690 - val_auROC: 0.9825\n",
      "Epoch 271/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1921 - acc: 0.9712 - auROC: 0.9810 - val_loss: 0.2018 - val_acc: 0.9690 - val_auROC: 0.9826\n",
      "Epoch 272/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1920 - acc: 0.9727 - auROC: 0.9809 - val_loss: 0.2014 - val_acc: 0.9690 - val_auROC: 0.9827\n",
      "Epoch 273/361\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.1919 - acc: 0.9727 - auROC: 0.9809 - val_loss: 0.2013 - val_acc: 0.9690 - val_auROC: 0.9827\n",
      "Epoch 274/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1917 - acc: 0.9723 - auROC: 0.9809 - val_loss: 0.2011 - val_acc: 0.9690 - val_auROC: 0.9827\n",
      "Epoch 275/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1916 - acc: 0.9723 - auROC: 0.9809 - val_loss: 0.2009 - val_acc: 0.9690 - val_auROC: 0.9827\n",
      "Epoch 276/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1918 - acc: 0.9692 - auROC: 0.9809 - val_loss: 0.2008 - val_acc: 0.9655 - val_auROC: 0.9826\n",
      "Epoch 277/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1919 - acc: 0.9688 - auROC: 0.9807 - val_loss: 0.2008 - val_acc: 0.9655 - val_auROC: 0.9829\n",
      "Epoch 278/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1914 - acc: 0.9712 - auROC: 0.9807 - val_loss: 0.2010 - val_acc: 0.9690 - val_auROC: 0.9829\n",
      "Epoch 279/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1911 - acc: 0.9723 - auROC: 0.9810 - val_loss: 0.2008 - val_acc: 0.9690 - val_auROC: 0.9819\n",
      "Epoch 280/361\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.1909 - acc: 0.9723 - auROC: 0.9812 - val_loss: 0.2006 - val_acc: 0.9724 - val_auROC: 0.9820\n",
      "Epoch 281/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1907 - acc: 0.9727 - auROC: 0.9813 - val_loss: 0.2006 - val_acc: 0.9724 - val_auROC: 0.9828\n",
      "Epoch 282/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1907 - acc: 0.9723 - auROC: 0.9815 - val_loss: 0.2014 - val_acc: 0.9655 - val_auROC: 0.9825\n",
      "Epoch 283/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1908 - acc: 0.9723 - auROC: 0.9817 - val_loss: 0.2008 - val_acc: 0.9690 - val_auROC: 0.9823\n",
      "Epoch 284/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1905 - acc: 0.9727 - auROC: 0.9816 - val_loss: 0.2004 - val_acc: 0.9690 - val_auROC: 0.9824\n",
      "Epoch 285/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1902 - acc: 0.9727 - auROC: 0.9817 - val_loss: 0.2002 - val_acc: 0.9690 - val_auROC: 0.9826\n",
      "Epoch 286/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1900 - acc: 0.9727 - auROC: 0.9818 - val_loss: 0.1997 - val_acc: 0.9690 - val_auROC: 0.9828\n",
      "Epoch 287/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1897 - acc: 0.9735 - auROC: 0.9819 - val_loss: 0.1991 - val_acc: 0.9724 - val_auROC: 0.9828\n",
      "Epoch 288/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1896 - acc: 0.9739 - auROC: 0.9818 - val_loss: 0.1988 - val_acc: 0.9724 - val_auROC: 0.9830\n",
      "Epoch 289/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1895 - acc: 0.9747 - auROC: 0.9820 - val_loss: 0.1981 - val_acc: 0.9724 - val_auROC: 0.9831\n",
      "Epoch 290/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1893 - acc: 0.9739 - auROC: 0.9822 - val_loss: 0.1980 - val_acc: 0.9724 - val_auROC: 0.9831\n",
      "Epoch 291/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.1890 - acc: 0.9739 - auROC: 0.9822 - val_loss: 0.1979 - val_acc: 0.9724 - val_auROC: 0.9831\n",
      "Epoch 292/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1887 - acc: 0.9735 - auROC: 0.9822 - val_loss: 0.1980 - val_acc: 0.9759 - val_auROC: 0.9830\n",
      "Epoch 293/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1883 - acc: 0.9743 - auROC: 0.9820 - val_loss: 0.1980 - val_acc: 0.9759 - val_auROC: 0.9830\n",
      "Epoch 294/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1879 - acc: 0.9750 - auROC: 0.9822 - val_loss: 0.1979 - val_acc: 0.9759 - val_auROC: 0.9830\n",
      "Epoch 295/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1874 - acc: 0.9766 - auROC: 0.9824 - val_loss: 0.1984 - val_acc: 0.9759 - val_auROC: 0.9829\n",
      "Epoch 296/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1878 - acc: 0.9747 - auROC: 0.9827 - val_loss: 0.2055 - val_acc: 0.9621 - val_auROC: 0.9826\n",
      "Epoch 297/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1889 - acc: 0.9727 - auROC: 0.9824 - val_loss: 0.2055 - val_acc: 0.9621 - val_auROC: 0.9827\n",
      "Epoch 298/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1876 - acc: 0.9750 - auROC: 0.9832 - val_loss: 0.2022 - val_acc: 0.9621 - val_auROC: 0.9834\n",
      "Epoch 299/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1862 - acc: 0.9782 - auROC: 0.9833 - val_loss: 0.2004 - val_acc: 0.9759 - val_auROC: 0.9830\n",
      "Epoch 300/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1856 - acc: 0.9782 - auROC: 0.9832 - val_loss: 0.1982 - val_acc: 0.9759 - val_auROC: 0.9831\n",
      "Epoch 301/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1858 - acc: 0.9758 - auROC: 0.9828 - val_loss: 0.1957 - val_acc: 0.9690 - val_auROC: 0.9818\n",
      "Epoch 302/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1859 - acc: 0.9758 - auROC: 0.9824 - val_loss: 0.1948 - val_acc: 0.9690 - val_auROC: 0.9827\n",
      "Epoch 303/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1849 - acc: 0.9782 - auROC: 0.9826 - val_loss: 0.1938 - val_acc: 0.9793 - val_auROC: 0.9848\n",
      "Epoch 304/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1844 - acc: 0.9789 - auROC: 0.9827 - val_loss: 0.1937 - val_acc: 0.9793 - val_auROC: 0.9848\n",
      "Epoch 305/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1840 - acc: 0.9786 - auROC: 0.9829 - val_loss: 0.1939 - val_acc: 0.9759 - val_auROC: 0.9841\n",
      "Epoch 306/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1835 - acc: 0.9789 - auROC: 0.9832 - val_loss: 0.1947 - val_acc: 0.9724 - val_auROC: 0.9840\n",
      "Epoch 307/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1834 - acc: 0.9797 - auROC: 0.9832 - val_loss: 0.1945 - val_acc: 0.9759 - val_auROC: 0.9829\n",
      "Epoch 308/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1831 - acc: 0.9797 - auROC: 0.9834 - val_loss: 0.1945 - val_acc: 0.9759 - val_auROC: 0.9822\n",
      "Epoch 309/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1829 - acc: 0.9797 - auROC: 0.9836 - val_loss: 0.1943 - val_acc: 0.9793 - val_auROC: 0.9823\n",
      "Epoch 310/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1827 - acc: 0.9801 - auROC: 0.9836 - val_loss: 0.1940 - val_acc: 0.9793 - val_auROC: 0.9822\n",
      "Epoch 311/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1825 - acc: 0.9797 - auROC: 0.9836 - val_loss: 0.1938 - val_acc: 0.9793 - val_auROC: 0.9822\n",
      "Epoch 312/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1823 - acc: 0.9797 - auROC: 0.9836 - val_loss: 0.1940 - val_acc: 0.9759 - val_auROC: 0.9823\n",
      "Epoch 313/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1822 - acc: 0.9797 - auROC: 0.9837 - val_loss: 0.1939 - val_acc: 0.9759 - val_auROC: 0.9823\n",
      "Epoch 314/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1820 - acc: 0.9797 - auROC: 0.9837 - val_loss: 0.1936 - val_acc: 0.9793 - val_auROC: 0.9826\n",
      "Epoch 315/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1818 - acc: 0.9797 - auROC: 0.9837 - val_loss: 0.1933 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 316/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1817 - acc: 0.9801 - auROC: 0.9837 - val_loss: 0.1930 - val_acc: 0.9793 - val_auROC: 0.9833\n",
      "Epoch 317/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1816 - acc: 0.9801 - auROC: 0.9837 - val_loss: 0.1929 - val_acc: 0.9793 - val_auROC: 0.9833\n",
      "Epoch 318/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1814 - acc: 0.9801 - auROC: 0.9837 - val_loss: 0.1927 - val_acc: 0.9793 - val_auROC: 0.9837\n",
      "Epoch 319/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1814 - acc: 0.9797 - auROC: 0.9837 - val_loss: 0.1930 - val_acc: 0.9759 - val_auROC: 0.9828\n",
      "Epoch 320/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1814 - acc: 0.9801 - auROC: 0.9836 - val_loss: 0.1929 - val_acc: 0.9793 - val_auROC: 0.9829\n",
      "Epoch 321/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1812 - acc: 0.9801 - auROC: 0.9837 - val_loss: 0.1927 - val_acc: 0.9793 - val_auROC: 0.9835\n",
      "Epoch 322/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1811 - acc: 0.9797 - auROC: 0.9838 - val_loss: 0.1926 - val_acc: 0.9793 - val_auROC: 0.9839\n",
      "Epoch 323/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1810 - acc: 0.9801 - auROC: 0.9836 - val_loss: 0.1921 - val_acc: 0.9793 - val_auROC: 0.9831\n",
      "Epoch 324/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1809 - acc: 0.9801 - auROC: 0.9838 - val_loss: 0.1921 - val_acc: 0.9793 - val_auROC: 0.9831\n",
      "Epoch 325/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1807 - acc: 0.9805 - auROC: 0.9839 - val_loss: 0.1920 - val_acc: 0.9793 - val_auROC: 0.9831\n",
      "Epoch 326/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1807 - acc: 0.9789 - auROC: 0.9839 - val_loss: 0.1925 - val_acc: 0.9724 - val_auROC: 0.9827\n",
      "Epoch 327/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1807 - acc: 0.9778 - auROC: 0.9838 - val_loss: 0.1924 - val_acc: 0.9724 - val_auROC: 0.9837\n",
      "Epoch 328/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1804 - acc: 0.9789 - auROC: 0.9839 - val_loss: 0.1922 - val_acc: 0.9793 - val_auROC: 0.9841\n",
      "Epoch 329/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1802 - acc: 0.9793 - auROC: 0.9840 - val_loss: 0.1920 - val_acc: 0.9793 - val_auROC: 0.9831\n",
      "Epoch 330/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1801 - acc: 0.9801 - auROC: 0.9841 - val_loss: 0.1922 - val_acc: 0.9793 - val_auROC: 0.9833\n",
      "Epoch 331/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1800 - acc: 0.9805 - auROC: 0.9841 - val_loss: 0.1920 - val_acc: 0.9793 - val_auROC: 0.9832\n",
      "Epoch 332/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1798 - acc: 0.9805 - auROC: 0.9841 - val_loss: 0.1916 - val_acc: 0.9793 - val_auROC: 0.9839\n",
      "Epoch 333/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1797 - acc: 0.9805 - auROC: 0.9841 - val_loss: 0.1914 - val_acc: 0.9793 - val_auROC: 0.9847\n",
      "Epoch 334/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1796 - acc: 0.9805 - auROC: 0.9842 - val_loss: 0.1912 - val_acc: 0.9793 - val_auROC: 0.9848\n",
      "Epoch 335/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1795 - acc: 0.9805 - auROC: 0.9842 - val_loss: 0.1910 - val_acc: 0.9793 - val_auROC: 0.9848\n",
      "Epoch 336/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1794 - acc: 0.9805 - auROC: 0.9842 - val_loss: 0.1909 - val_acc: 0.9793 - val_auROC: 0.9849\n",
      "Epoch 337/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1793 - acc: 0.9805 - auROC: 0.9842 - val_loss: 0.1911 - val_acc: 0.9793 - val_auROC: 0.9850\n",
      "Epoch 338/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1791 - acc: 0.9805 - auROC: 0.9842 - val_loss: 0.1911 - val_acc: 0.9793 - val_auROC: 0.9850\n",
      "Epoch 339/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1790 - acc: 0.9805 - auROC: 0.9842 - val_loss: 0.1912 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 340/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1789 - acc: 0.9809 - auROC: 0.9843 - val_loss: 0.1912 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 341/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.1788 - acc: 0.9809 - auROC: 0.9842 - val_loss: 0.1911 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 342/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1786 - acc: 0.9813 - auROC: 0.9843 - val_loss: 0.1910 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 343/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1785 - acc: 0.9813 - auROC: 0.9843 - val_loss: 0.1908 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 344/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1784 - acc: 0.9813 - auROC: 0.9843 - val_loss: 0.1906 - val_acc: 0.9793 - val_auROC: 0.9834\n",
      "Epoch 345/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1783 - acc: 0.9813 - auROC: 0.9843 - val_loss: 0.1906 - val_acc: 0.9793 - val_auROC: 0.9841\n",
      "Epoch 346/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1782 - acc: 0.9813 - auROC: 0.9843 - val_loss: 0.1908 - val_acc: 0.9793 - val_auROC: 0.9842\n",
      "Epoch 347/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1781 - acc: 0.9813 - auROC: 0.9843 - val_loss: 0.1904 - val_acc: 0.9793 - val_auROC: 0.9844\n",
      "Epoch 348/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1780 - acc: 0.9809 - auROC: 0.9844 - val_loss: 0.1898 - val_acc: 0.9793 - val_auROC: 0.9844\n",
      "Epoch 349/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1779 - acc: 0.9801 - auROC: 0.9844 - val_loss: 0.1896 - val_acc: 0.9793 - val_auROC: 0.9845\n",
      "Epoch 350/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1778 - acc: 0.9801 - auROC: 0.9844 - val_loss: 0.1895 - val_acc: 0.9793 - val_auROC: 0.9846\n",
      "Epoch 351/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1777 - acc: 0.9805 - auROC: 0.9844 - val_loss: 0.1894 - val_acc: 0.9828 - val_auROC: 0.9845\n",
      "Epoch 352/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1776 - acc: 0.9805 - auROC: 0.9844 - val_loss: 0.1894 - val_acc: 0.9828 - val_auROC: 0.9844\n",
      "Epoch 353/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1774 - acc: 0.9817 - auROC: 0.9844 - val_loss: 0.1894 - val_acc: 0.9828 - val_auROC: 0.9844\n",
      "Epoch 354/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1773 - acc: 0.9817 - auROC: 0.9844 - val_loss: 0.1892 - val_acc: 0.9828 - val_auROC: 0.9845\n",
      "Epoch 355/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1772 - acc: 0.9821 - auROC: 0.9844 - val_loss: 0.1889 - val_acc: 0.9828 - val_auROC: 0.9847\n",
      "Epoch 356/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1771 - acc: 0.9821 - auROC: 0.9845 - val_loss: 0.1887 - val_acc: 0.9828 - val_auROC: 0.9845\n",
      "Epoch 357/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1770 - acc: 0.9825 - auROC: 0.9844 - val_loss: 0.1884 - val_acc: 0.9828 - val_auROC: 0.9848\n",
      "Epoch 358/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1769 - acc: 0.9825 - auROC: 0.9845 - val_loss: 0.1879 - val_acc: 0.9828 - val_auROC: 0.9856\n",
      "Epoch 359/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1768 - acc: 0.9825 - auROC: 0.9845 - val_loss: 0.1874 - val_acc: 0.9828 - val_auROC: 0.9855\n",
      "Epoch 360/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1766 - acc: 0.9825 - auROC: 0.9844 - val_loss: 0.1872 - val_acc: 0.9828 - val_auROC: 0.9847\n",
      "Epoch 361/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1765 - acc: 0.9825 - auROC: 0.9844 - val_loss: 0.1872 - val_acc: 0.9828 - val_auROC: 0.9847\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "1     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "2     0.0    0.0    0.0 -0.003108  ...    0.0 -0.198135 -0.196532    0.0\n",
      "3     0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "4     0.0    0.0    0.0  0.568505  ...    0.0 -0.198135 -0.196532    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0  3.113146  ...    0.0 -0.198135 -0.196532    0.0\n",
      "60    0.0    0.0    0.0 -0.350212  ...    0.0 -0.198135 -0.196532    0.0\n",
      "61    0.0    0.0    0.0 -0.350911  ...    0.0 -0.198135 -0.196532    0.0\n",
      "62    0.0    0.0    0.0 -0.139348  ...    0.0 -0.198135 -0.196532    0.0\n",
      "63    0.0    0.0    0.0 -0.350679  ...    0.0 -0.198135 -0.196532    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  61   0   1  0.0161  1.0  ...  1.0000  1.0  0.0161  0.0317      1.0    1.0\n",
      "0.01   0  61   0   1  0.0161  1.0  ...  1.0000  1.0  0.0161  0.0317      1.0    1.0\n",
      "0.02   1  59   0   1  0.0328  1.0  ...  0.9833  1.0  0.0167  0.0328      1.0    1.0\n",
      "0.03  17  43   0   1  0.2951  1.0  ...  0.7167  1.0  0.0227  0.0444      1.0    1.0\n",
      "0.04  39  21   0   1  0.6557  1.0  ...  0.3500  1.0  0.0455  0.0870      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  61   0   1   0  0.9839  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.01   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.02   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.03   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "0.04   0  47   0  15  0.2419  1.0  ...  1.0  1.0  0.2419  0.3896      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  47   0  15   0  0.7581  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  54   0   8  0.1290  1.0  ...  1.0  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.01   0  54   0   8  0.1290  1.0  ...  1.0  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.02   0  54   0   8  0.1290  1.0  ...  1.0  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.03   0  54   0   8  0.1290  1.0  ...  1.0  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.04   0  53   0   8  0.1311  1.0  ...  1.0  1.0  0.1311  0.2319      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  54   0   8   0  0.8710  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  54   0   8   0  0.8710  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  54   0   8   0  0.8710  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  54   0   8   0  0.8710  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  54   0   8   0  0.8710  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.01   0  54   0   8  0.1290  1.0  ...  1.0000  1.0  0.1290  0.2286      1.0    1.0\n",
      "0.02   0  53   0   8  0.1311  1.0  ...  1.0000  1.0  0.1311  0.2319      1.0    1.0\n",
      "0.03  26  27   0   8  0.5574  1.0  ...  0.5094  1.0  0.2286  0.3721      1.0    1.0\n",
      "0.04  27  26   0   8  0.5738  1.0  ...  0.4906  1.0  0.2353  0.3810      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  54   0   8   0  0.8710  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  35   0  27  0.4355  1.0  ...  1.0  1.0  0.4355  0.6067   0.9808  0.9804\n",
      "0.01   0  35   0  27  0.4355  1.0  ...  1.0  1.0  0.4355  0.6067   0.9808  0.9804\n",
      "0.02   0  35   0  27  0.4355  1.0  ...  1.0  1.0  0.4355  0.6067   0.9808  0.9804\n",
      "0.03   0  35   0  27  0.4355  1.0  ...  1.0  1.0  0.4355  0.6067   0.9808  0.9804\n",
      "0.04   0  35   0  27  0.4355  1.0  ...  1.0  1.0  0.4355  0.6067   0.9808  0.9804\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  35   0  27   0  0.5645  0.0  ...  0.0  0.0  0.0000     NaN   0.9808  0.9804\n",
      "0.98  35   0  27   0  0.5645  0.0  ...  0.0  0.0  0.0000     NaN   0.9808  0.9804\n",
      "0.99  35   0  27   0  0.5645  0.0  ...  0.0  0.0  0.0000     NaN   0.9808  0.9804\n",
      "1.00  35   0  27   0  0.5645  0.0  ...  0.0  0.0  0.0000     NaN   0.9808  0.9804\n",
      "1.01  35   0  27   0  0.5645  0.0  ...  0.0  0.0  0.0000     NaN   0.9808  0.9804\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "N. NaN in input features: 0\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015718  0.044470\n",
      "4      0.015697  0.044456\n",
      "...         ...       ...\n",
      "18013  0.000059  0.000240\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002371  0.011928\n",
      "18016  0.002307  0.011699\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Pre-training using Adam with lr=1e-05...\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.7041 - acc: 0.4862 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6933 - acc: 0.5041 - val_loss: 0.6908 - val_acc: 0.5069\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 18,998,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training using Adam with lr=0.001...\n",
      "Epoch 3/1002\n",
      "1/1 [==============================] - 1s 705ms/step - loss: 0.6843 - acc: 0.5220 - auROC: 0.4501 - val_loss: 0.6967 - val_acc: 0.6310 - val_auROC: 0.4801\n",
      "Epoch 4/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6680 - acc: 0.6448 - auROC: 0.5211 - val_loss: 0.6494 - val_acc: 0.5690 - val_auROC: 0.5370\n",
      "Epoch 5/1002\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6270 - acc: 0.6023 - auROC: 0.5729 - val_loss: 0.6258 - val_acc: 0.6621 - val_auROC: 0.5268\n",
      "Epoch 6/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5980 - acc: 0.6749 - auROC: 0.5828 - val_loss: 0.5937 - val_acc: 0.7000 - val_auROC: 0.5862\n",
      "Epoch 7/1002\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5773 - acc: 0.7014 - auROC: 0.6262 - val_loss: 0.5726 - val_acc: 0.7207 - val_auROC: 0.5920\n",
      "Epoch 8/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.5479 - acc: 0.7306 - auROC: 0.6551 - val_loss: 0.5595 - val_acc: 0.7138 - val_auROC: 0.5912\n",
      "Epoch 9/1002\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5371 - acc: 0.7193 - auROC: 0.6670 - val_loss: 0.5684 - val_acc: 0.7069 - val_auROC: 0.5816\n",
      "Epoch 10/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5276 - acc: 0.7259 - auROC: 0.6706 - val_loss: 0.5422 - val_acc: 0.7448 - val_auROC: 0.6146\n",
      "Epoch 11/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5044 - acc: 0.7618 - auROC: 0.7004 - val_loss: 0.5307 - val_acc: 0.7414 - val_auROC: 0.6394\n",
      "Epoch 12/1002\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4972 - acc: 0.7762 - auROC: 0.7279 - val_loss: 0.5289 - val_acc: 0.7793 - val_auROC: 0.6549\n",
      "Epoch 13/1002\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.4911 - acc: 0.7981 - auROC: 0.7585 - val_loss: 0.5204 - val_acc: 0.7828 - val_auROC: 0.6563\n",
      "Epoch 14/1002\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.4817 - acc: 0.8136 - auROC: 0.7627 - val_loss: 0.5126 - val_acc: 0.7793 - val_auROC: 0.6665\n",
      "Epoch 15/1002\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4734 - acc: 0.8222 - auROC: 0.7629 - val_loss: 0.5074 - val_acc: 0.7724 - val_auROC: 0.6702\n",
      "Epoch 16/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4661 - acc: 0.8250 - auROC: 0.7692 - val_loss: 0.5029 - val_acc: 0.7690 - val_auROC: 0.6854\n",
      "Epoch 17/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4605 - acc: 0.8191 - auROC: 0.7741 - val_loss: 0.4967 - val_acc: 0.7862 - val_auROC: 0.6974\n",
      "Epoch 18/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4544 - acc: 0.8238 - auROC: 0.7903 - val_loss: 0.4892 - val_acc: 0.8000 - val_auROC: 0.7063\n",
      "Epoch 19/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.4474 - acc: 0.8464 - auROC: 0.8086 - val_loss: 0.4850 - val_acc: 0.8103 - val_auROC: 0.7109\n",
      "Epoch 20/1002\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.4415 - acc: 0.8519 - auROC: 0.8145 - val_loss: 0.4783 - val_acc: 0.8103 - val_auROC: 0.7237\n",
      "Epoch 21/1002\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4392 - acc: 0.8499 - auROC: 0.8151 - val_loss: 0.4800 - val_acc: 0.7966 - val_auROC: 0.7163\n",
      "Epoch 22/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4344 - acc: 0.8487 - auROC: 0.8168 - val_loss: 0.4785 - val_acc: 0.7897 - val_auROC: 0.7178\n",
      "Epoch 23/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.4289 - acc: 0.8522 - auROC: 0.8228 - val_loss: 0.4773 - val_acc: 0.7931 - val_auROC: 0.7224\n",
      "Epoch 24/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.4254 - acc: 0.8589 - auROC: 0.8297 - val_loss: 0.4744 - val_acc: 0.7862 - val_auROC: 0.7252\n",
      "Epoch 25/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.4218 - acc: 0.8624 - auROC: 0.8342 - val_loss: 0.4663 - val_acc: 0.7931 - val_auROC: 0.7359\n",
      "Epoch 26/1002\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4179 - acc: 0.8620 - auROC: 0.8377 - val_loss: 0.4660 - val_acc: 0.8000 - val_auROC: 0.7405\n",
      "Epoch 27/1002\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4156 - acc: 0.8593 - auROC: 0.8374 - val_loss: 0.4618 - val_acc: 0.7897 - val_auROC: 0.7462\n",
      "Epoch 28/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4123 - acc: 0.8585 - auROC: 0.8420 - val_loss: 0.4587 - val_acc: 0.8034 - val_auROC: 0.7555\n",
      "Epoch 29/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4082 - acc: 0.8690 - auROC: 0.8505 - val_loss: 0.4512 - val_acc: 0.8241 - val_auROC: 0.7652\n",
      "Epoch 30/1002\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.4047 - acc: 0.8667 - auROC: 0.8566 - val_loss: 0.4479 - val_acc: 0.8138 - val_auROC: 0.7663\n",
      "Epoch 31/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4021 - acc: 0.8608 - auROC: 0.8565 - val_loss: 0.4482 - val_acc: 0.8138 - val_auROC: 0.7705\n",
      "Epoch 32/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3985 - acc: 0.8655 - auROC: 0.8622 - val_loss: 0.4463 - val_acc: 0.8241 - val_auROC: 0.7659\n",
      "Epoch 33/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3968 - acc: 0.8698 - auROC: 0.8623 - val_loss: 0.4431 - val_acc: 0.8172 - val_auROC: 0.7691\n",
      "Epoch 34/1002\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.3939 - acc: 0.8632 - auROC: 0.8643 - val_loss: 0.4413 - val_acc: 0.8172 - val_auROC: 0.7720\n",
      "Epoch 35/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3901 - acc: 0.8671 - auROC: 0.8687 - val_loss: 0.4421 - val_acc: 0.8276 - val_auROC: 0.7683\n",
      "Epoch 36/1002\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.3881 - acc: 0.8784 - auROC: 0.8721 - val_loss: 0.4362 - val_acc: 0.8172 - val_auROC: 0.7798\n",
      "Epoch 37/1002\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3848 - acc: 0.8791 - auROC: 0.8756 - val_loss: 0.4319 - val_acc: 0.8138 - val_auROC: 0.7891\n",
      "Epoch 38/1002\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.3829 - acc: 0.8752 - auROC: 0.8754 - val_loss: 0.4286 - val_acc: 0.8310 - val_auROC: 0.7944\n",
      "Epoch 39/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3792 - acc: 0.8854 - auROC: 0.8804 - val_loss: 0.4293 - val_acc: 0.8379 - val_auROC: 0.7924\n",
      "Epoch 40/1002\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3775 - acc: 0.8904 - auROC: 0.8831 - val_loss: 0.4265 - val_acc: 0.8276 - val_auROC: 0.7936\n",
      "Epoch 41/1002\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.3739 - acc: 0.8873 - auROC: 0.8840 - val_loss: 0.4198 - val_acc: 0.8207 - val_auROC: 0.8041\n",
      "Epoch 42/1002\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.3715 - acc: 0.8881 - auROC: 0.8847 - val_loss: 0.4148 - val_acc: 0.8345 - val_auROC: 0.8198\n",
      "Epoch 43/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3689 - acc: 0.8963 - auROC: 0.8919 - val_loss: 0.4185 - val_acc: 0.8483 - val_auROC: 0.8076\n",
      "Epoch 44/1002\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3665 - acc: 0.9025 - auROC: 0.8926 - val_loss: 0.4142 - val_acc: 0.8483 - val_auROC: 0.8135\n",
      "Epoch 45/1002\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.3647 - acc: 0.9006 - auROC: 0.8929 - val_loss: 0.4104 - val_acc: 0.8483 - val_auROC: 0.8185\n",
      "Epoch 46/1002\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3615 - acc: 0.8990 - auROC: 0.8945 - val_loss: 0.4081 - val_acc: 0.8379 - val_auROC: 0.8195\n",
      "Epoch 47/1002\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3587 - acc: 0.8904 - auROC: 0.8952 - val_loss: 0.3970 - val_acc: 0.8414 - val_auROC: 0.8464\n",
      "Epoch 48/1002\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3545 - acc: 0.8943 - auROC: 0.9006 - val_loss: 0.3947 - val_acc: 0.8483 - val_auROC: 0.8488\n",
      "Epoch 49/1002\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3519 - acc: 0.8975 - auROC: 0.9015 - val_loss: 0.4002 - val_acc: 0.8552 - val_auROC: 0.8354\n",
      "Epoch 50/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3497 - acc: 0.8955 - auROC: 0.9050 - val_loss: 0.4039 - val_acc: 0.8483 - val_auROC: 0.8291\n",
      "Epoch 51/1002\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3477 - acc: 0.8943 - auROC: 0.9049 - val_loss: 0.4051 - val_acc: 0.8448 - val_auROC: 0.8245\n",
      "Epoch 52/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3453 - acc: 0.8959 - auROC: 0.9056 - val_loss: 0.4022 - val_acc: 0.8483 - val_auROC: 0.8328\n",
      "Epoch 53/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3435 - acc: 0.8971 - auROC: 0.9102\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3435 - acc: 0.8971 - auROC: 0.9102 - val_loss: 0.3949 - val_acc: 0.8517 - val_auROC: 0.8466\n",
      "Epoch 54/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3425 - acc: 0.9018 - auROC: 0.9103 - val_loss: 0.3950 - val_acc: 0.8517 - val_auROC: 0.8458\n",
      "Epoch 55/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3410 - acc: 0.9025 - auROC: 0.9116 - val_loss: 0.3986 - val_acc: 0.8552 - val_auROC: 0.8385\n",
      "Epoch 56/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3396 - acc: 0.9025 - auROC: 0.9130 - val_loss: 0.3994 - val_acc: 0.8552 - val_auROC: 0.8374\n",
      "Epoch 57/1002\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3393 - acc: 0.9010 - auROC: 0.9130 - val_loss: 0.3992 - val_acc: 0.8552 - val_auROC: 0.8372\n",
      "Epoch 58/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3393 - acc: 0.9006 - auROC: 0.9122\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3393 - acc: 0.9006 - auROC: 0.9122 - val_loss: 0.3988 - val_acc: 0.8586 - val_auROC: 0.8368\n",
      "Epoch 59/1002\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3390 - acc: 0.9010 - auROC: 0.9123 - val_loss: 0.3988 - val_acc: 0.8586 - val_auROC: 0.8372\n",
      "Epoch 60/1002\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3389 - acc: 0.9010 - auROC: 0.9123 - val_loss: 0.3987 - val_acc: 0.8586 - val_auROC: 0.8374\n",
      "Epoch 61/1002\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3388 - acc: 0.9014 - auROC: 0.9126 - val_loss: 0.3986 - val_acc: 0.8586 - val_auROC: 0.8378\n",
      "Epoch 62/1002\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3387 - acc: 0.9014 - auROC: 0.9128 - val_loss: 0.3985 - val_acc: 0.8586 - val_auROC: 0.8381\n",
      "Epoch 63/1002\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3385 - acc: 0.9018 - auROC: 0.9129\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3385 - acc: 0.9018 - auROC: 0.9129 - val_loss: 0.3984 - val_acc: 0.8586 - val_auROC: 0.8383\n",
      "Epoch 00063: early stopping\n",
      "    0      1      2         3      ...  18014     18015   18016  18017\n",
      "0     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "1     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "2     0.0    0.0    0.0  0.042439  ...    0.0 -0.198786 -0.1972    0.0\n",
      "3     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "4     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...     ...    ...\n",
      "59    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "60    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "61    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "62    0.0    0.0    0.0  1.044470  ...    0.0 -0.198786 -0.1972    0.0\n",
      "63    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198786 -0.1972    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP  Acc   Sn   Sp  TPR  FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                                                          \n",
      "0.00   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.04   0  62   0   0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ..      ...    ...\n",
      "0.97  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  62   0   0   0  1.0  0.0  1.0  0.0  0.0  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr     F1  ROC-AUC   F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.8863  0.8333\n",
      "0.01   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.8863  0.8333\n",
      "0.02   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.8863  0.8333\n",
      "0.03   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.8863  0.8333\n",
      "0.04   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.8863  0.8333\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...    ...      ...     ...\n",
      "0.97  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.8863  0.8333\n",
      "0.98  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.8863  0.8333\n",
      "0.99  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.8863  0.8333\n",
      "1.00  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.8863  0.8333\n",
      "1.01  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.8863  0.8333\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.9618   0.88\n",
      "0.01   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.9618   0.88\n",
      "0.02   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.9618   0.88\n",
      "0.03   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.9618   0.88\n",
      "0.04   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.9618   0.88\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.9618   0.88\n",
      "0.98  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.9618   0.88\n",
      "0.99  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.9618   0.88\n",
      "1.00  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.9618   0.88\n",
      "1.01  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.9618   0.88\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765   0.9873    0.8\n",
      "0.01   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765   0.9873    0.8\n",
      "0.02   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765   0.9873    0.8\n",
      "0.03   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765   0.9873    0.8\n",
      "0.04   8  47   0   6  0.2295  1.0  ...  0.8545  1.0  0.1132  0.2034   0.9873    0.8\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9873    0.8\n",
      "0.98  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9873    0.8\n",
      "0.99  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9873    0.8\n",
      "1.00  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9873    0.8\n",
      "1.01  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9873    0.8\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9504  0.8485\n",
      "0.01   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9504  0.8485\n",
      "0.02   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9504  0.8485\n",
      "0.03   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9504  0.8485\n",
      "0.04   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9504  0.8485\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9504  0.8485\n",
      "0.98  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9504  0.8485\n",
      "0.99  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9504  0.8485\n",
      "1.00  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9504  0.8485\n",
      "1.01  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9504  0.8485\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "Total correct samples: 571?571\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015718  0.044470\n",
      "4      0.015697  0.044456\n",
      "...         ...       ...\n",
      "18013  0.000059  0.000240\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002371  0.011928\n",
      "18016  0.002307  0.011699\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.6843 - acc: 0.5930 - auROC: 0.4701 - val_loss: 0.5995 - val_acc: 0.6793 - val_auROC: 0.5532\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.5967 - acc: 0.6893 - auROC: 0.5522 - val_loss: 0.5533 - val_acc: 0.7655 - val_auROC: 0.6192\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.5382 - acc: 0.7524 - auROC: 0.6371 - val_loss: 0.5257 - val_acc: 0.7724 - val_auROC: 0.6513\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.5119 - acc: 0.7758 - auROC: 0.6826 - val_loss: 0.5103 - val_acc: 0.7828 - val_auROC: 0.6541\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.4921 - acc: 0.7910 - auROC: 0.7032 - val_loss: 0.4828 - val_acc: 0.8034 - val_auROC: 0.7050\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4721 - acc: 0.7793 - auROC: 0.7528 - val_loss: 0.4790 - val_acc: 0.7966 - val_auROC: 0.6911\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4618 - acc: 0.8047 - auROC: 0.7480 - val_loss: 0.4629 - val_acc: 0.7966 - val_auROC: 0.7355\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4499 - acc: 0.8074 - auROC: 0.7807 - val_loss: 0.4515 - val_acc: 0.8103 - val_auROC: 0.7440\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.4349 - acc: 0.8148 - auROC: 0.7938 - val_loss: 0.4333 - val_acc: 0.8172 - val_auROC: 0.7711\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.4186 - acc: 0.8234 - auROC: 0.8173 - val_loss: 0.4231 - val_acc: 0.8207 - val_auROC: 0.7956\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.4071 - acc: 0.8261 - auROC: 0.8395 - val_loss: 0.4074 - val_acc: 0.8310 - val_auROC: 0.8182\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3980 - acc: 0.8253 - auROC: 0.8502 - val_loss: 0.4002 - val_acc: 0.8345 - val_auROC: 0.8306\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3869 - acc: 0.8413 - auROC: 0.8691 - val_loss: 0.4053 - val_acc: 0.8310 - val_auROC: 0.8159\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3750 - acc: 0.8476 - auROC: 0.8846 - val_loss: 0.3979 - val_acc: 0.8276 - val_auROC: 0.8284\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3766 - acc: 0.8417 - auROC: 0.8753 - val_loss: 0.4038 - val_acc: 0.8310 - val_auROC: 0.8156\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3671 - acc: 0.8526 - auROC: 0.8878 - val_loss: 0.3967 - val_acc: 0.8310 - val_auROC: 0.8289\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3621 - acc: 0.8667 - auROC: 0.8923 - val_loss: 0.3967 - val_acc: 0.8345 - val_auROC: 0.8220\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3613 - acc: 0.8608 - auROC: 0.8893 - val_loss: 0.3882 - val_acc: 0.8379 - val_auROC: 0.8321\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3526 - acc: 0.8655 - auROC: 0.9010 - val_loss: 0.3854 - val_acc: 0.8448 - val_auROC: 0.8364\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.3533 - acc: 0.8686 - auROC: 0.8955 - val_loss: 0.3886 - val_acc: 0.8379 - val_auROC: 0.8207\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3538 - acc: 0.8643 - auROC: 0.8887 - val_loss: 0.3970 - val_acc: 0.8345 - val_auROC: 0.8082\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3475 - acc: 0.8686 - auROC: 0.8982 - val_loss: 0.3987 - val_acc: 0.8483 - val_auROC: 0.7996\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3447 - acc: 0.8698 - auROC: 0.8960 - val_loss: 0.3882 - val_acc: 0.8448 - val_auROC: 0.8218\n",
      "Epoch 24/300\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 0.3760 - acc: 0.8590 - auROC: 0.8574\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3757 - acc: 0.8593 - auROC: 0.8578 - val_loss: 0.3970 - val_acc: 0.8517 - val_auROC: 0.8112\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3736 - acc: 0.8604 - auROC: 0.8592 - val_loss: 0.3957 - val_acc: 0.8448 - val_auROC: 0.8113\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3705 - acc: 0.8647 - auROC: 0.8629 - val_loss: 0.3886 - val_acc: 0.8483 - val_auROC: 0.8248\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3664 - acc: 0.8694 - auROC: 0.8676 - val_loss: 0.3848 - val_acc: 0.8517 - val_auROC: 0.8317\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3641 - acc: 0.8725 - auROC: 0.8701 - val_loss: 0.3826 - val_acc: 0.8448 - val_auROC: 0.8343\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3610 - acc: 0.8710 - auROC: 0.8736 - val_loss: 0.3798 - val_acc: 0.8483 - val_auROC: 0.8361\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3588 - acc: 0.8698 - auROC: 0.8771 - val_loss: 0.3788 - val_acc: 0.8483 - val_auROC: 0.8367\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3570 - acc: 0.8702 - auROC: 0.8790 - val_loss: 0.3778 - val_acc: 0.8483 - val_auROC: 0.8381\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3551 - acc: 0.8690 - auROC: 0.8813 - val_loss: 0.3770 - val_acc: 0.8483 - val_auROC: 0.8384\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3536 - acc: 0.8702 - auROC: 0.8831 - val_loss: 0.3764 - val_acc: 0.8483 - val_auROC: 0.8387\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3524 - acc: 0.8706 - auROC: 0.8841 - val_loss: 0.3760 - val_acc: 0.8448 - val_auROC: 0.8384\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3509 - acc: 0.8694 - auROC: 0.8858 - val_loss: 0.3760 - val_acc: 0.8448 - val_auROC: 0.8368\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3494 - acc: 0.8682 - auROC: 0.8872 - val_loss: 0.3753 - val_acc: 0.8483 - val_auROC: 0.8372\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3484 - acc: 0.8690 - auROC: 0.8882 - val_loss: 0.3745 - val_acc: 0.8483 - val_auROC: 0.8370\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3474 - acc: 0.8686 - auROC: 0.8893 - val_loss: 0.3735 - val_acc: 0.8483 - val_auROC: 0.8381\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3465 - acc: 0.8725 - auROC: 0.8905 - val_loss: 0.3726 - val_acc: 0.8483 - val_auROC: 0.8385\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3454 - acc: 0.8729 - auROC: 0.8908 - val_loss: 0.3724 - val_acc: 0.8448 - val_auROC: 0.8384\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3447 - acc: 0.8733 - auROC: 0.8908 - val_loss: 0.3728 - val_acc: 0.8448 - val_auROC: 0.8372\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3435 - acc: 0.8752 - auROC: 0.8922 - val_loss: 0.3725 - val_acc: 0.8483 - val_auROC: 0.8377\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3421 - acc: 0.8760 - auROC: 0.8943 - val_loss: 0.3708 - val_acc: 0.8483 - val_auROC: 0.8402\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3407 - acc: 0.8784 - auROC: 0.8961 - val_loss: 0.3685 - val_acc: 0.8483 - val_auROC: 0.8414\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3390 - acc: 0.8776 - auROC: 0.8982 - val_loss: 0.3683 - val_acc: 0.8483 - val_auROC: 0.8417\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3373 - acc: 0.8768 - auROC: 0.9000 - val_loss: 0.3680 - val_acc: 0.8483 - val_auROC: 0.8433\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.3357 - acc: 0.8772 - auROC: 0.9019 - val_loss: 0.3666 - val_acc: 0.8448 - val_auROC: 0.8435\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3340 - acc: 0.8772 - auROC: 0.9041 - val_loss: 0.3679 - val_acc: 0.8483 - val_auROC: 0.8407\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3331 - acc: 0.8795 - auROC: 0.9051 - val_loss: 0.3695 - val_acc: 0.8483 - val_auROC: 0.8388\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3324 - acc: 0.8780 - auROC: 0.9057 - val_loss: 0.3678 - val_acc: 0.8414 - val_auROC: 0.8410\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3324 - acc: 0.8803 - auROC: 0.9045 - val_loss: 0.3677 - val_acc: 0.8414 - val_auROC: 0.8418\n",
      "Epoch 52/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.3299 - acc: 0.8830 - auROC: 0.9076\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3306 - acc: 0.8823 - auROC: 0.9065 - val_loss: 0.3702 - val_acc: 0.8414 - val_auROC: 0.8373\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3289 - acc: 0.8834 - auROC: 0.9093 - val_loss: 0.3701 - val_acc: 0.8448 - val_auROC: 0.8372\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3288 - acc: 0.8834 - auROC: 0.9095 - val_loss: 0.3699 - val_acc: 0.8448 - val_auROC: 0.8372\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3287 - acc: 0.8834 - auROC: 0.9097 - val_loss: 0.3696 - val_acc: 0.8448 - val_auROC: 0.8374\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3285 - acc: 0.8834 - auROC: 0.9098 - val_loss: 0.3695 - val_acc: 0.8448 - val_auROC: 0.8374\n",
      "Epoch 57/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.3284 - acc: 0.8826 - auROC: 0.9092\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3284 - acc: 0.8830 - auROC: 0.9099 - val_loss: 0.3692 - val_acc: 0.8448 - val_auROC: 0.8378\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3283 - acc: 0.8830 - auROC: 0.9101 - val_loss: 0.3690 - val_acc: 0.8448 - val_auROC: 0.8378\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3282 - acc: 0.8830 - auROC: 0.9103 - val_loss: 0.3688 - val_acc: 0.8448 - val_auROC: 0.8386\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3280 - acc: 0.8830 - auROC: 0.9103 - val_loss: 0.3687 - val_acc: 0.8448 - val_auROC: 0.8385\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3279 - acc: 0.8830 - auROC: 0.9106 - val_loss: 0.3685 - val_acc: 0.8448 - val_auROC: 0.8383\n",
      "Epoch 62/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.3262 - acc: 0.8853 - auROC: 0.9124Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3278 - acc: 0.8834 - auROC: 0.9107 - val_loss: 0.3680 - val_acc: 0.8448 - val_auROC: 0.8388\n",
      "Epoch 00062: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 21,795\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 62/361\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.3340 - acc: 0.8776 - auROC: 0.9037 - val_loss: 0.3674 - val_acc: 0.8448 - val_auROC: 0.8420\n",
      "Epoch 63/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3316 - acc: 0.8795 - auROC: 0.9075 - val_loss: 0.3668 - val_acc: 0.8448 - val_auROC: 0.8424\n",
      "Epoch 64/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.3299 - acc: 0.8795 - auROC: 0.9097 - val_loss: 0.3650 - val_acc: 0.8483 - val_auROC: 0.8447\n",
      "Epoch 65/361\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 0.3279 - acc: 0.8811 - auROC: 0.9124 - val_loss: 0.3636 - val_acc: 0.8483 - val_auROC: 0.8471\n",
      "Epoch 66/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3269 - acc: 0.8827 - auROC: 0.9134 - val_loss: 0.3619 - val_acc: 0.8483 - val_auROC: 0.8490\n",
      "Epoch 67/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3260 - acc: 0.8830 - auROC: 0.9146 - val_loss: 0.3623 - val_acc: 0.8517 - val_auROC: 0.8494\n",
      "Epoch 68/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3252 - acc: 0.8823 - auROC: 0.9154 - val_loss: 0.3622 - val_acc: 0.8517 - val_auROC: 0.8481\n",
      "Epoch 69/361\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.3243 - acc: 0.8854 - auROC: 0.9163 - val_loss: 0.3614 - val_acc: 0.8552 - val_auROC: 0.8488\n",
      "Epoch 70/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3234 - acc: 0.8862 - auROC: 0.9170 - val_loss: 0.3607 - val_acc: 0.8552 - val_auROC: 0.8491\n",
      "Epoch 71/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3226 - acc: 0.8885 - auROC: 0.9176 - val_loss: 0.3599 - val_acc: 0.8552 - val_auROC: 0.8505\n",
      "Epoch 72/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3220 - acc: 0.8893 - auROC: 0.9182 - val_loss: 0.3592 - val_acc: 0.8483 - val_auROC: 0.8520\n",
      "Epoch 73/361\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.3213 - acc: 0.8897 - auROC: 0.9188 - val_loss: 0.3585 - val_acc: 0.8483 - val_auROC: 0.8528\n",
      "Epoch 74/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3207 - acc: 0.8897 - auROC: 0.9192 - val_loss: 0.3577 - val_acc: 0.8483 - val_auROC: 0.8528\n",
      "Epoch 75/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3197 - acc: 0.8908 - auROC: 0.9204 - val_loss: 0.3573 - val_acc: 0.8483 - val_auROC: 0.8536\n",
      "Epoch 76/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.3191 - acc: 0.8916 - auROC: 0.9208 - val_loss: 0.3570 - val_acc: 0.8483 - val_auROC: 0.8533\n",
      "Epoch 77/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3185 - acc: 0.8924 - auROC: 0.9212 - val_loss: 0.3560 - val_acc: 0.8517 - val_auROC: 0.8538\n",
      "Epoch 78/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3171 - acc: 0.8940 - auROC: 0.9228 - val_loss: 0.3544 - val_acc: 0.8552 - val_auROC: 0.8562\n",
      "Epoch 79/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.3161 - acc: 0.8947 - auROC: 0.9230 - val_loss: 0.3539 - val_acc: 0.8552 - val_auROC: 0.8571\n",
      "Epoch 80/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3137 - acc: 0.8967 - auROC: 0.9255 - val_loss: 0.3522 - val_acc: 0.8552 - val_auROC: 0.8585\n",
      "Epoch 81/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3121 - acc: 0.8975 - auROC: 0.9273 - val_loss: 0.3505 - val_acc: 0.8552 - val_auROC: 0.8603\n",
      "Epoch 82/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3110 - acc: 0.8986 - auROC: 0.9283 - val_loss: 0.3502 - val_acc: 0.8552 - val_auROC: 0.8594\n",
      "Epoch 83/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.3103 - acc: 0.8990 - auROC: 0.9289 - val_loss: 0.3499 - val_acc: 0.8552 - val_auROC: 0.8596\n",
      "Epoch 84/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3098 - acc: 0.8994 - auROC: 0.9292 - val_loss: 0.3496 - val_acc: 0.8552 - val_auROC: 0.8602\n",
      "Epoch 85/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3094 - acc: 0.9010 - auROC: 0.9292 - val_loss: 0.3499 - val_acc: 0.8552 - val_auROC: 0.8584\n",
      "Epoch 86/361\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3089 - acc: 0.9021 - auROC: 0.9296 - val_loss: 0.3497 - val_acc: 0.8552 - val_auROC: 0.8592\n",
      "Epoch 87/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3084 - acc: 0.9018 - auROC: 0.9303 - val_loss: 0.3496 - val_acc: 0.8552 - val_auROC: 0.8594\n",
      "Epoch 88/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3078 - acc: 0.9025 - auROC: 0.9307 - val_loss: 0.3490 - val_acc: 0.8552 - val_auROC: 0.8610\n",
      "Epoch 89/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3070 - acc: 0.9033 - auROC: 0.9315 - val_loss: 0.3486 - val_acc: 0.8552 - val_auROC: 0.8619\n",
      "Epoch 90/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.3064 - acc: 0.9021 - auROC: 0.9322 - val_loss: 0.3484 - val_acc: 0.8517 - val_auROC: 0.8615\n",
      "Epoch 91/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3059 - acc: 0.9021 - auROC: 0.9328 - val_loss: 0.3479 - val_acc: 0.8552 - val_auROC: 0.8623\n",
      "Epoch 92/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3057 - acc: 0.9014 - auROC: 0.9332 - val_loss: 0.3475 - val_acc: 0.8552 - val_auROC: 0.8623\n",
      "Epoch 93/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3055 - acc: 0.9014 - auROC: 0.9335 - val_loss: 0.3474 - val_acc: 0.8552 - val_auROC: 0.8628\n",
      "Epoch 94/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.3047 - acc: 0.9014 - auROC: 0.9344 - val_loss: 0.3473 - val_acc: 0.8517 - val_auROC: 0.8633\n",
      "Epoch 95/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3038 - acc: 0.9025 - auROC: 0.9351 - val_loss: 0.3466 - val_acc: 0.8448 - val_auROC: 0.8646\n",
      "Epoch 96/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3032 - acc: 0.9041 - auROC: 0.9358 - val_loss: 0.3459 - val_acc: 0.8483 - val_auROC: 0.8659\n",
      "Epoch 97/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3028 - acc: 0.9037 - auROC: 0.9362 - val_loss: 0.3459 - val_acc: 0.8483 - val_auROC: 0.8653\n",
      "Epoch 98/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3025 - acc: 0.9029 - auROC: 0.9366 - val_loss: 0.3459 - val_acc: 0.8483 - val_auROC: 0.8652\n",
      "Epoch 99/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3019 - acc: 0.9021 - auROC: 0.9373 - val_loss: 0.3455 - val_acc: 0.8483 - val_auROC: 0.8650\n",
      "Epoch 100/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3014 - acc: 0.9021 - auROC: 0.9379 - val_loss: 0.3445 - val_acc: 0.8483 - val_auROC: 0.8684\n",
      "Epoch 101/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.3009 - acc: 0.9037 - auROC: 0.9383 - val_loss: 0.3439 - val_acc: 0.8483 - val_auROC: 0.8685\n",
      "Epoch 102/361\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3006 - acc: 0.9041 - auROC: 0.9386 - val_loss: 0.3445 - val_acc: 0.8483 - val_auROC: 0.8654\n",
      "Epoch 103/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3003 - acc: 0.9045 - auROC: 0.9389 - val_loss: 0.3452 - val_acc: 0.8517 - val_auROC: 0.8641\n",
      "Epoch 104/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.3000 - acc: 0.9041 - auROC: 0.9391 - val_loss: 0.3440 - val_acc: 0.8517 - val_auROC: 0.8646\n",
      "Epoch 105/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2994 - acc: 0.9053 - auROC: 0.9398 - val_loss: 0.3431 - val_acc: 0.8483 - val_auROC: 0.8673\n",
      "Epoch 106/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2988 - acc: 0.9076 - auROC: 0.9408 - val_loss: 0.3421 - val_acc: 0.8517 - val_auROC: 0.8698\n",
      "Epoch 107/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2979 - acc: 0.9072 - auROC: 0.9427 - val_loss: 0.3411 - val_acc: 0.8517 - val_auROC: 0.8719\n",
      "Epoch 108/361\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.2971 - acc: 0.9057 - auROC: 0.9441 - val_loss: 0.3405 - val_acc: 0.8483 - val_auROC: 0.8749\n",
      "Epoch 109/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2966 - acc: 0.9057 - auROC: 0.9450 - val_loss: 0.3397 - val_acc: 0.8483 - val_auROC: 0.8760\n",
      "Epoch 110/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2960 - acc: 0.9049 - auROC: 0.9455 - val_loss: 0.3382 - val_acc: 0.8483 - val_auROC: 0.8775\n",
      "Epoch 111/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2953 - acc: 0.9045 - auROC: 0.9464 - val_loss: 0.3377 - val_acc: 0.8483 - val_auROC: 0.8780\n",
      "Epoch 112/361\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.2940 - acc: 0.9057 - auROC: 0.9481 - val_loss: 0.3370 - val_acc: 0.8483 - val_auROC: 0.8795\n",
      "Epoch 113/361\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.2932 - acc: 0.9068 - auROC: 0.9490 - val_loss: 0.3350 - val_acc: 0.8586 - val_auROC: 0.8816\n",
      "Epoch 114/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2926 - acc: 0.9076 - auROC: 0.9496 - val_loss: 0.3343 - val_acc: 0.8586 - val_auROC: 0.8840\n",
      "Epoch 115/361\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.2918 - acc: 0.9076 - auROC: 0.9499 - val_loss: 0.3341 - val_acc: 0.8586 - val_auROC: 0.8833\n",
      "Epoch 116/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2915 - acc: 0.9080 - auROC: 0.9503 - val_loss: 0.3329 - val_acc: 0.8621 - val_auROC: 0.8855\n",
      "Epoch 117/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2911 - acc: 0.9068 - auROC: 0.9507 - val_loss: 0.3323 - val_acc: 0.8586 - val_auROC: 0.8843\n",
      "Epoch 118/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2909 - acc: 0.9072 - auROC: 0.9507 - val_loss: 0.3325 - val_acc: 0.8621 - val_auROC: 0.8856\n",
      "Epoch 119/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2907 - acc: 0.9072 - auROC: 0.9508 - val_loss: 0.3331 - val_acc: 0.8655 - val_auROC: 0.8852\n",
      "Epoch 120/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2904 - acc: 0.9072 - auROC: 0.9510 - val_loss: 0.3330 - val_acc: 0.8655 - val_auROC: 0.8851\n",
      "Epoch 121/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2902 - acc: 0.9068 - auROC: 0.9511 - val_loss: 0.3321 - val_acc: 0.8621 - val_auROC: 0.8861\n",
      "Epoch 122/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2904 - acc: 0.9072 - auROC: 0.9508 - val_loss: 0.3343 - val_acc: 0.8621 - val_auROC: 0.8831\n",
      "Epoch 123/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2902 - acc: 0.9068 - auROC: 0.9511 - val_loss: 0.3336 - val_acc: 0.8621 - val_auROC: 0.8845\n",
      "Epoch 124/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2895 - acc: 0.9060 - auROC: 0.9519 - val_loss: 0.3318 - val_acc: 0.8655 - val_auROC: 0.8857\n",
      "Epoch 125/361\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2892 - acc: 0.9072 - auROC: 0.9521 - val_loss: 0.3322 - val_acc: 0.8621 - val_auROC: 0.8840\n",
      "Epoch 126/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2893 - acc: 0.9080 - auROC: 0.9518 - val_loss: 0.3326 - val_acc: 0.8621 - val_auROC: 0.8834\n",
      "Epoch 127/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2889 - acc: 0.9072 - auROC: 0.9519 - val_loss: 0.3327 - val_acc: 0.8621 - val_auROC: 0.8828\n",
      "Epoch 128/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2883 - acc: 0.9064 - auROC: 0.9523 - val_loss: 0.3328 - val_acc: 0.8621 - val_auROC: 0.8834\n",
      "Epoch 129/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2883 - acc: 0.9045 - auROC: 0.9520 - val_loss: 0.3330 - val_acc: 0.8655 - val_auROC: 0.8841\n",
      "Epoch 130/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2880 - acc: 0.9060 - auROC: 0.9521 - val_loss: 0.3325 - val_acc: 0.8690 - val_auROC: 0.8840\n",
      "Epoch 131/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2874 - acc: 0.9076 - auROC: 0.9528 - val_loss: 0.3325 - val_acc: 0.8655 - val_auROC: 0.8847\n",
      "Epoch 132/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2871 - acc: 0.9080 - auROC: 0.9531 - val_loss: 0.3322 - val_acc: 0.8690 - val_auROC: 0.8854\n",
      "Epoch 133/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2869 - acc: 0.9099 - auROC: 0.9534 - val_loss: 0.3320 - val_acc: 0.8655 - val_auROC: 0.8859\n",
      "Epoch 134/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2866 - acc: 0.9103 - auROC: 0.9536 - val_loss: 0.3319 - val_acc: 0.8655 - val_auROC: 0.8865\n",
      "Epoch 135/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2864 - acc: 0.9092 - auROC: 0.9537 - val_loss: 0.3321 - val_acc: 0.8655 - val_auROC: 0.8853\n",
      "Epoch 136/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2862 - acc: 0.9099 - auROC: 0.9538 - val_loss: 0.3315 - val_acc: 0.8690 - val_auROC: 0.8864\n",
      "Epoch 137/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2861 - acc: 0.9103 - auROC: 0.9538 - val_loss: 0.3312 - val_acc: 0.8690 - val_auROC: 0.8868\n",
      "Epoch 138/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2859 - acc: 0.9103 - auROC: 0.9539 - val_loss: 0.3312 - val_acc: 0.8690 - val_auROC: 0.8872\n",
      "Epoch 139/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2856 - acc: 0.9099 - auROC: 0.9542 - val_loss: 0.3310 - val_acc: 0.8690 - val_auROC: 0.8868\n",
      "Epoch 140/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2854 - acc: 0.9096 - auROC: 0.9545 - val_loss: 0.3310 - val_acc: 0.8655 - val_auROC: 0.8873\n",
      "Epoch 141/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2851 - acc: 0.9103 - auROC: 0.9548 - val_loss: 0.3315 - val_acc: 0.8690 - val_auROC: 0.8863\n",
      "Epoch 142/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2847 - acc: 0.9107 - auROC: 0.9554 - val_loss: 0.3315 - val_acc: 0.8690 - val_auROC: 0.8863\n",
      "Epoch 143/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2843 - acc: 0.9107 - auROC: 0.9556 - val_loss: 0.3311 - val_acc: 0.8655 - val_auROC: 0.8873\n",
      "Epoch 144/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2839 - acc: 0.9111 - auROC: 0.9560 - val_loss: 0.3302 - val_acc: 0.8690 - val_auROC: 0.8871\n",
      "Epoch 145/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2838 - acc: 0.9119 - auROC: 0.9560 - val_loss: 0.3296 - val_acc: 0.8724 - val_auROC: 0.8867\n",
      "Epoch 146/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2834 - acc: 0.9119 - auROC: 0.9564 - val_loss: 0.3296 - val_acc: 0.8724 - val_auROC: 0.8873\n",
      "Epoch 147/361\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 0.2831 - acc: 0.9115 - auROC: 0.9566 - val_loss: 0.3295 - val_acc: 0.8724 - val_auROC: 0.8874\n",
      "Epoch 148/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2829 - acc: 0.9115 - auROC: 0.9568 - val_loss: 0.3292 - val_acc: 0.8724 - val_auROC: 0.8886\n",
      "Epoch 149/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2826 - acc: 0.9127 - auROC: 0.9570 - val_loss: 0.3289 - val_acc: 0.8724 - val_auROC: 0.8888\n",
      "Epoch 150/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2826 - acc: 0.9131 - auROC: 0.9570 - val_loss: 0.3271 - val_acc: 0.8724 - val_auROC: 0.8930\n",
      "Epoch 151/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2827 - acc: 0.9135 - auROC: 0.9575 - val_loss: 0.3273 - val_acc: 0.8724 - val_auROC: 0.8921\n",
      "Epoch 152/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2818 - acc: 0.9138 - auROC: 0.9585 - val_loss: 0.3282 - val_acc: 0.8724 - val_auROC: 0.8911\n",
      "Epoch 153/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2813 - acc: 0.9138 - auROC: 0.9591 - val_loss: 0.3270 - val_acc: 0.8724 - val_auROC: 0.8941\n",
      "Epoch 154/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2808 - acc: 0.9138 - auROC: 0.9597 - val_loss: 0.3258 - val_acc: 0.8690 - val_auROC: 0.8951\n",
      "Epoch 155/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2805 - acc: 0.9142 - auROC: 0.9601 - val_loss: 0.3246 - val_acc: 0.8724 - val_auROC: 0.8967\n",
      "Epoch 156/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2803 - acc: 0.9135 - auROC: 0.9602 - val_loss: 0.3237 - val_acc: 0.8724 - val_auROC: 0.8984\n",
      "Epoch 157/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2800 - acc: 0.9135 - auROC: 0.9604 - val_loss: 0.3233 - val_acc: 0.8690 - val_auROC: 0.8990\n",
      "Epoch 158/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2799 - acc: 0.9135 - auROC: 0.9604 - val_loss: 0.3230 - val_acc: 0.8690 - val_auROC: 0.8992\n",
      "Epoch 159/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2797 - acc: 0.9135 - auROC: 0.9606 - val_loss: 0.3226 - val_acc: 0.8724 - val_auROC: 0.9002\n",
      "Epoch 160/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2796 - acc: 0.9123 - auROC: 0.9606 - val_loss: 0.3225 - val_acc: 0.8724 - val_auROC: 0.9008\n",
      "Epoch 161/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2798 - acc: 0.9123 - auROC: 0.9605 - val_loss: 0.3231 - val_acc: 0.8724 - val_auROC: 0.8993\n",
      "Epoch 162/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2799 - acc: 0.9131 - auROC: 0.9604 - val_loss: 0.3231 - val_acc: 0.8690 - val_auROC: 0.9001\n",
      "Epoch 163/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2798 - acc: 0.9138 - auROC: 0.9605 - val_loss: 0.3229 - val_acc: 0.8690 - val_auROC: 0.9001\n",
      "Epoch 164/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2796 - acc: 0.9138 - auROC: 0.9606 - val_loss: 0.3225 - val_acc: 0.8690 - val_auROC: 0.9002\n",
      "Epoch 165/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2795 - acc: 0.9135 - auROC: 0.9607 - val_loss: 0.3220 - val_acc: 0.8690 - val_auROC: 0.9006\n",
      "Epoch 166/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2793 - acc: 0.9138 - auROC: 0.9608 - val_loss: 0.3219 - val_acc: 0.8724 - val_auROC: 0.9007\n",
      "Epoch 167/361\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.2792 - acc: 0.9142 - auROC: 0.9608 - val_loss: 0.3218 - val_acc: 0.8724 - val_auROC: 0.9017\n",
      "Epoch 168/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2790 - acc: 0.9142 - auROC: 0.9609 - val_loss: 0.3215 - val_acc: 0.8724 - val_auROC: 0.9018\n",
      "Epoch 169/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2788 - acc: 0.9146 - auROC: 0.9610 - val_loss: 0.3213 - val_acc: 0.8724 - val_auROC: 0.9006\n",
      "Epoch 170/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2786 - acc: 0.9154 - auROC: 0.9612 - val_loss: 0.3213 - val_acc: 0.8724 - val_auROC: 0.9010\n",
      "Epoch 171/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2785 - acc: 0.9154 - auROC: 0.9613 - val_loss: 0.3207 - val_acc: 0.8724 - val_auROC: 0.9022\n",
      "Epoch 172/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2784 - acc: 0.9150 - auROC: 0.9613 - val_loss: 0.3200 - val_acc: 0.8759 - val_auROC: 0.9036\n",
      "Epoch 173/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2785 - acc: 0.9150 - auROC: 0.9611 - val_loss: 0.3205 - val_acc: 0.8759 - val_auROC: 0.9035\n",
      "Epoch 174/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2784 - acc: 0.9146 - auROC: 0.9612 - val_loss: 0.3210 - val_acc: 0.8724 - val_auROC: 0.9020\n",
      "Epoch 175/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2781 - acc: 0.9154 - auROC: 0.9612 - val_loss: 0.3210 - val_acc: 0.8724 - val_auROC: 0.9019\n",
      "Epoch 176/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2778 - acc: 0.9150 - auROC: 0.9615 - val_loss: 0.3207 - val_acc: 0.8724 - val_auROC: 0.9021\n",
      "Epoch 177/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2776 - acc: 0.9150 - auROC: 0.9616 - val_loss: 0.3202 - val_acc: 0.8690 - val_auROC: 0.9033\n",
      "Epoch 178/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2774 - acc: 0.9146 - auROC: 0.9616 - val_loss: 0.3202 - val_acc: 0.8690 - val_auROC: 0.9034\n",
      "Epoch 179/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2773 - acc: 0.9150 - auROC: 0.9617 - val_loss: 0.3199 - val_acc: 0.8690 - val_auROC: 0.9041\n",
      "Epoch 180/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2773 - acc: 0.9150 - auROC: 0.9618 - val_loss: 0.3198 - val_acc: 0.8724 - val_auROC: 0.9044\n",
      "Epoch 181/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2771 - acc: 0.9162 - auROC: 0.9618 - val_loss: 0.3199 - val_acc: 0.8759 - val_auROC: 0.9026\n",
      "Epoch 182/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2770 - acc: 0.9158 - auROC: 0.9618 - val_loss: 0.3199 - val_acc: 0.8724 - val_auROC: 0.9021\n",
      "Epoch 183/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2769 - acc: 0.9170 - auROC: 0.9618 - val_loss: 0.3200 - val_acc: 0.8724 - val_auROC: 0.9012\n",
      "Epoch 184/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2768 - acc: 0.9166 - auROC: 0.9619 - val_loss: 0.3203 - val_acc: 0.8724 - val_auROC: 0.9015\n",
      "Epoch 185/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2767 - acc: 0.9170 - auROC: 0.9619 - val_loss: 0.3196 - val_acc: 0.8724 - val_auROC: 0.9038\n",
      "Epoch 186/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2765 - acc: 0.9166 - auROC: 0.9620 - val_loss: 0.3190 - val_acc: 0.8759 - val_auROC: 0.9036\n",
      "Epoch 187/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2764 - acc: 0.9162 - auROC: 0.9620 - val_loss: 0.3189 - val_acc: 0.8759 - val_auROC: 0.9040\n",
      "Epoch 188/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2762 - acc: 0.9162 - auROC: 0.9621 - val_loss: 0.3187 - val_acc: 0.8724 - val_auROC: 0.9036\n",
      "Epoch 189/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2761 - acc: 0.9154 - auROC: 0.9620 - val_loss: 0.3185 - val_acc: 0.8724 - val_auROC: 0.9037\n",
      "Epoch 190/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2759 - acc: 0.9154 - auROC: 0.9620 - val_loss: 0.3183 - val_acc: 0.8724 - val_auROC: 0.9042\n",
      "Epoch 191/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2758 - acc: 0.9158 - auROC: 0.9620 - val_loss: 0.3180 - val_acc: 0.8724 - val_auROC: 0.9044\n",
      "Epoch 192/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2757 - acc: 0.9162 - auROC: 0.9619 - val_loss: 0.3177 - val_acc: 0.8759 - val_auROC: 0.9043\n",
      "Epoch 193/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2756 - acc: 0.9162 - auROC: 0.9619 - val_loss: 0.3176 - val_acc: 0.8759 - val_auROC: 0.9042\n",
      "Epoch 194/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2755 - acc: 0.9162 - auROC: 0.9619 - val_loss: 0.3175 - val_acc: 0.8724 - val_auROC: 0.9045\n",
      "Epoch 195/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2754 - acc: 0.9166 - auROC: 0.9619 - val_loss: 0.3173 - val_acc: 0.8724 - val_auROC: 0.9055\n",
      "Epoch 196/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2753 - acc: 0.9166 - auROC: 0.9619 - val_loss: 0.3167 - val_acc: 0.8724 - val_auROC: 0.9059\n",
      "Epoch 197/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2752 - acc: 0.9166 - auROC: 0.9619 - val_loss: 0.3166 - val_acc: 0.8724 - val_auROC: 0.9070\n",
      "Epoch 198/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2751 - acc: 0.9166 - auROC: 0.9619 - val_loss: 0.3169 - val_acc: 0.8724 - val_auROC: 0.9073\n",
      "Epoch 199/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2752 - acc: 0.9177 - auROC: 0.9621 - val_loss: 0.3176 - val_acc: 0.8724 - val_auROC: 0.9057\n",
      "Epoch 200/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2752 - acc: 0.9177 - auROC: 0.9618 - val_loss: 0.3174 - val_acc: 0.8724 - val_auROC: 0.9061\n",
      "Epoch 201/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2749 - acc: 0.9177 - auROC: 0.9618 - val_loss: 0.3180 - val_acc: 0.8655 - val_auROC: 0.9050\n",
      "Epoch 202/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2747 - acc: 0.9162 - auROC: 0.9622 - val_loss: 0.3173 - val_acc: 0.8655 - val_auROC: 0.9052\n",
      "Epoch 203/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2746 - acc: 0.9170 - auROC: 0.9622 - val_loss: 0.3164 - val_acc: 0.8655 - val_auROC: 0.9063\n",
      "Epoch 204/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2745 - acc: 0.9177 - auROC: 0.9622 - val_loss: 0.3159 - val_acc: 0.8724 - val_auROC: 0.9074\n",
      "Epoch 205/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2744 - acc: 0.9201 - auROC: 0.9623 - val_loss: 0.3157 - val_acc: 0.8793 - val_auROC: 0.9062\n",
      "Epoch 206/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2744 - acc: 0.9201 - auROC: 0.9623 - val_loss: 0.3155 - val_acc: 0.8828 - val_auROC: 0.9063\n",
      "Epoch 207/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2742 - acc: 0.9201 - auROC: 0.9623 - val_loss: 0.3155 - val_acc: 0.8793 - val_auROC: 0.9062\n",
      "Epoch 208/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2741 - acc: 0.9181 - auROC: 0.9623 - val_loss: 0.3154 - val_acc: 0.8793 - val_auROC: 0.9065\n",
      "Epoch 209/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2739 - acc: 0.9181 - auROC: 0.9624 - val_loss: 0.3154 - val_acc: 0.8793 - val_auROC: 0.9061\n",
      "Epoch 210/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2738 - acc: 0.9177 - auROC: 0.9624 - val_loss: 0.3154 - val_acc: 0.8793 - val_auROC: 0.9059\n",
      "Epoch 211/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2738 - acc: 0.9177 - auROC: 0.9624 - val_loss: 0.3153 - val_acc: 0.8793 - val_auROC: 0.9060\n",
      "Epoch 212/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2736 - acc: 0.9181 - auROC: 0.9625 - val_loss: 0.3153 - val_acc: 0.8793 - val_auROC: 0.9058\n",
      "Epoch 213/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2735 - acc: 0.9185 - auROC: 0.9625 - val_loss: 0.3153 - val_acc: 0.8793 - val_auROC: 0.9053\n",
      "Epoch 214/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2736 - acc: 0.9181 - auROC: 0.9625 - val_loss: 0.3155 - val_acc: 0.8793 - val_auROC: 0.9054\n",
      "Epoch 215/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2737 - acc: 0.9181 - auROC: 0.9625 - val_loss: 0.3155 - val_acc: 0.8793 - val_auROC: 0.9056\n",
      "Epoch 216/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2738 - acc: 0.9181 - auROC: 0.9624 - val_loss: 0.3173 - val_acc: 0.8759 - val_auROC: 0.9039\n",
      "Epoch 217/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2740 - acc: 0.9185 - auROC: 0.9622 - val_loss: 0.3159 - val_acc: 0.8759 - val_auROC: 0.9057\n",
      "Epoch 218/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2733 - acc: 0.9189 - auROC: 0.9627 - val_loss: 0.3146 - val_acc: 0.8793 - val_auROC: 0.9058\n",
      "Epoch 219/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2730 - acc: 0.9205 - auROC: 0.9626 - val_loss: 0.3141 - val_acc: 0.8828 - val_auROC: 0.9062\n",
      "Epoch 220/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2728 - acc: 0.9197 - auROC: 0.9627 - val_loss: 0.3142 - val_acc: 0.8793 - val_auROC: 0.9071\n",
      "Epoch 221/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2727 - acc: 0.9189 - auROC: 0.9628 - val_loss: 0.3143 - val_acc: 0.8793 - val_auROC: 0.9071\n",
      "Epoch 222/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2727 - acc: 0.9193 - auROC: 0.9629 - val_loss: 0.3143 - val_acc: 0.8828 - val_auROC: 0.9076\n",
      "Epoch 223/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2726 - acc: 0.9201 - auROC: 0.9629 - val_loss: 0.3140 - val_acc: 0.8828 - val_auROC: 0.9078\n",
      "Epoch 224/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2727 - acc: 0.9205 - auROC: 0.9629 - val_loss: 0.3135 - val_acc: 0.8828 - val_auROC: 0.9085\n",
      "Epoch 225/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2729 - acc: 0.9209 - auROC: 0.9627 - val_loss: 0.3134 - val_acc: 0.8828 - val_auROC: 0.9084\n",
      "Epoch 226/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2726 - acc: 0.9224 - auROC: 0.9629 - val_loss: 0.3132 - val_acc: 0.8828 - val_auROC: 0.9081\n",
      "Epoch 227/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2725 - acc: 0.9209 - auROC: 0.9630 - val_loss: 0.3131 - val_acc: 0.8828 - val_auROC: 0.9078\n",
      "Epoch 228/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2724 - acc: 0.9209 - auROC: 0.9631 - val_loss: 0.3133 - val_acc: 0.8828 - val_auROC: 0.9084\n",
      "Epoch 229/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2722 - acc: 0.9224 - auROC: 0.9632 - val_loss: 0.3137 - val_acc: 0.8828 - val_auROC: 0.9071\n",
      "Epoch 230/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2722 - acc: 0.9232 - auROC: 0.9632 - val_loss: 0.3136 - val_acc: 0.8828 - val_auROC: 0.9070\n",
      "Epoch 231/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2720 - acc: 0.9228 - auROC: 0.9633 - val_loss: 0.3135 - val_acc: 0.8828 - val_auROC: 0.9084\n",
      "Epoch 232/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2717 - acc: 0.9232 - auROC: 0.9632 - val_loss: 0.3135 - val_acc: 0.8828 - val_auROC: 0.9083\n",
      "Epoch 233/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2716 - acc: 0.9248 - auROC: 0.9631 - val_loss: 0.3134 - val_acc: 0.8828 - val_auROC: 0.9079\n",
      "Epoch 234/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2714 - acc: 0.9240 - auROC: 0.9632 - val_loss: 0.3134 - val_acc: 0.8828 - val_auROC: 0.9079\n",
      "Epoch 235/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2714 - acc: 0.9244 - auROC: 0.9631 - val_loss: 0.3135 - val_acc: 0.8828 - val_auROC: 0.9077\n",
      "Epoch 236/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2713 - acc: 0.9240 - auROC: 0.9630 - val_loss: 0.3134 - val_acc: 0.8828 - val_auROC: 0.9080\n",
      "Epoch 237/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2712 - acc: 0.9232 - auROC: 0.9631 - val_loss: 0.3135 - val_acc: 0.8759 - val_auROC: 0.9080\n",
      "Epoch 238/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2711 - acc: 0.9224 - auROC: 0.9632 - val_loss: 0.3134 - val_acc: 0.8759 - val_auROC: 0.9079\n",
      "Epoch 239/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2710 - acc: 0.9232 - auROC: 0.9632 - val_loss: 0.3133 - val_acc: 0.8793 - val_auROC: 0.9081\n",
      "Epoch 240/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2710 - acc: 0.9240 - auROC: 0.9633 - val_loss: 0.3132 - val_acc: 0.8828 - val_auROC: 0.9081\n",
      "Epoch 241/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2710 - acc: 0.9220 - auROC: 0.9633 - val_loss: 0.3130 - val_acc: 0.8828 - val_auROC: 0.9083\n",
      "Epoch 242/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2711 - acc: 0.9220 - auROC: 0.9633 - val_loss: 0.3131 - val_acc: 0.8828 - val_auROC: 0.9083\n",
      "Epoch 243/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2709 - acc: 0.9228 - auROC: 0.9633 - val_loss: 0.3131 - val_acc: 0.8828 - val_auROC: 0.9088\n",
      "Epoch 244/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2708 - acc: 0.9232 - auROC: 0.9633 - val_loss: 0.3131 - val_acc: 0.8828 - val_auROC: 0.9088\n",
      "Epoch 245/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2707 - acc: 0.9248 - auROC: 0.9633 - val_loss: 0.3129 - val_acc: 0.8828 - val_auROC: 0.9089\n",
      "Epoch 246/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2706 - acc: 0.9255 - auROC: 0.9633 - val_loss: 0.3128 - val_acc: 0.8828 - val_auROC: 0.9086\n",
      "Epoch 247/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2705 - acc: 0.9255 - auROC: 0.9633 - val_loss: 0.3131 - val_acc: 0.8793 - val_auROC: 0.9086\n",
      "Epoch 248/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2704 - acc: 0.9248 - auROC: 0.9633 - val_loss: 0.3132 - val_acc: 0.8793 - val_auROC: 0.9090\n",
      "Epoch 249/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2704 - acc: 0.9240 - auROC: 0.9633 - val_loss: 0.3130 - val_acc: 0.8793 - val_auROC: 0.9085\n",
      "Epoch 250/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2703 - acc: 0.9244 - auROC: 0.9633 - val_loss: 0.3130 - val_acc: 0.8793 - val_auROC: 0.9085\n",
      "Epoch 251/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2703 - acc: 0.9244 - auROC: 0.9633 - val_loss: 0.3129 - val_acc: 0.8793 - val_auROC: 0.9080\n",
      "Epoch 252/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2702 - acc: 0.9232 - auROC: 0.9634 - val_loss: 0.3127 - val_acc: 0.8759 - val_auROC: 0.9082\n",
      "Epoch 253/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2701 - acc: 0.9228 - auROC: 0.9635 - val_loss: 0.3126 - val_acc: 0.8793 - val_auROC: 0.9078\n",
      "Epoch 254/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2700 - acc: 0.9232 - auROC: 0.9636 - val_loss: 0.3126 - val_acc: 0.8793 - val_auROC: 0.9080\n",
      "Epoch 255/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2700 - acc: 0.9240 - auROC: 0.9636 - val_loss: 0.3131 - val_acc: 0.8828 - val_auROC: 0.9073\n",
      "Epoch 256/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2700 - acc: 0.9248 - auROC: 0.9635 - val_loss: 0.3130 - val_acc: 0.8828 - val_auROC: 0.9064\n",
      "Epoch 257/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2700 - acc: 0.9255 - auROC: 0.9635 - val_loss: 0.3137 - val_acc: 0.8793 - val_auROC: 0.9059\n",
      "Epoch 258/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2704 - acc: 0.9244 - auROC: 0.9634 - val_loss: 0.3140 - val_acc: 0.8759 - val_auROC: 0.9052\n",
      "Epoch 259/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2702 - acc: 0.9236 - auROC: 0.9635 - val_loss: 0.3132 - val_acc: 0.8759 - val_auROC: 0.9062\n",
      "Epoch 260/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2700 - acc: 0.9236 - auROC: 0.9636 - val_loss: 0.3129 - val_acc: 0.8759 - val_auROC: 0.9063\n",
      "Epoch 261/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2698 - acc: 0.9240 - auROC: 0.9636 - val_loss: 0.3126 - val_acc: 0.8759 - val_auROC: 0.9071\n",
      "Epoch 262/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2697 - acc: 0.9240 - auROC: 0.9636 - val_loss: 0.3121 - val_acc: 0.8759 - val_auROC: 0.9071\n",
      "Epoch 263/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2697 - acc: 0.9244 - auROC: 0.9636 - val_loss: 0.3120 - val_acc: 0.8759 - val_auROC: 0.9070\n",
      "Epoch 264/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2696 - acc: 0.9251 - auROC: 0.9636 - val_loss: 0.3122 - val_acc: 0.8793 - val_auROC: 0.9073\n",
      "Epoch 265/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2695 - acc: 0.9259 - auROC: 0.9636 - val_loss: 0.3123 - val_acc: 0.8793 - val_auROC: 0.9075\n",
      "Epoch 266/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2694 - acc: 0.9255 - auROC: 0.9636 - val_loss: 0.3122 - val_acc: 0.8793 - val_auROC: 0.9074\n",
      "Epoch 267/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2694 - acc: 0.9255 - auROC: 0.9636 - val_loss: 0.3118 - val_acc: 0.8793 - val_auROC: 0.9081\n",
      "Epoch 268/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2695 - acc: 0.9251 - auROC: 0.9636 - val_loss: 0.3117 - val_acc: 0.8793 - val_auROC: 0.9083\n",
      "Epoch 269/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2694 - acc: 0.9259 - auROC: 0.9636 - val_loss: 0.3117 - val_acc: 0.8793 - val_auROC: 0.9083\n",
      "Epoch 270/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2693 - acc: 0.9259 - auROC: 0.9637 - val_loss: 0.3116 - val_acc: 0.8793 - val_auROC: 0.9084\n",
      "Epoch 271/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2692 - acc: 0.9259 - auROC: 0.9637 - val_loss: 0.3116 - val_acc: 0.8793 - val_auROC: 0.9081\n",
      "Epoch 272/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2691 - acc: 0.9263 - auROC: 0.9637 - val_loss: 0.3115 - val_acc: 0.8793 - val_auROC: 0.9083\n",
      "Epoch 273/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2690 - acc: 0.9263 - auROC: 0.9637 - val_loss: 0.3114 - val_acc: 0.8793 - val_auROC: 0.9082\n",
      "Epoch 274/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2689 - acc: 0.9263 - auROC: 0.9638 - val_loss: 0.3109 - val_acc: 0.8828 - val_auROC: 0.9110\n",
      "Epoch 275/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2687 - acc: 0.9263 - auROC: 0.9640 - val_loss: 0.3109 - val_acc: 0.8828 - val_auROC: 0.9111\n",
      "Epoch 276/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2686 - acc: 0.9263 - auROC: 0.9640 - val_loss: 0.3109 - val_acc: 0.8828 - val_auROC: 0.9108\n",
      "Epoch 277/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2685 - acc: 0.9263 - auROC: 0.9640 - val_loss: 0.3111 - val_acc: 0.8828 - val_auROC: 0.9101\n",
      "Epoch 278/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2685 - acc: 0.9251 - auROC: 0.9640 - val_loss: 0.3120 - val_acc: 0.8793 - val_auROC: 0.9078\n",
      "Epoch 279/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2685 - acc: 0.9236 - auROC: 0.9641 - val_loss: 0.3117 - val_acc: 0.8759 - val_auROC: 0.9080\n",
      "Epoch 280/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2684 - acc: 0.9236 - auROC: 0.9642 - val_loss: 0.3114 - val_acc: 0.8793 - val_auROC: 0.9088\n",
      "Epoch 281/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2683 - acc: 0.9255 - auROC: 0.9641 - val_loss: 0.3114 - val_acc: 0.8793 - val_auROC: 0.9091\n",
      "Epoch 282/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2683 - acc: 0.9251 - auROC: 0.9640 - val_loss: 0.3115 - val_acc: 0.8793 - val_auROC: 0.9087\n",
      "Epoch 283/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2682 - acc: 0.9251 - auROC: 0.9641 - val_loss: 0.3109 - val_acc: 0.8828 - val_auROC: 0.9096\n",
      "Epoch 284/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2679 - acc: 0.9240 - auROC: 0.9644 - val_loss: 0.3104 - val_acc: 0.8828 - val_auROC: 0.9097\n",
      "Epoch 285/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2679 - acc: 0.9224 - auROC: 0.9645 - val_loss: 0.3103 - val_acc: 0.8828 - val_auROC: 0.9103\n",
      "Epoch 286/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2677 - acc: 0.9236 - auROC: 0.9646 - val_loss: 0.3103 - val_acc: 0.8828 - val_auROC: 0.9097\n",
      "Epoch 287/361\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2676 - acc: 0.9248 - auROC: 0.9646 - val_loss: 0.3102 - val_acc: 0.8828 - val_auROC: 0.9089\n",
      "Epoch 288/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2675 - acc: 0.9251 - auROC: 0.9647 - val_loss: 0.3102 - val_acc: 0.8828 - val_auROC: 0.9094\n",
      "Epoch 289/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2674 - acc: 0.9251 - auROC: 0.9647 - val_loss: 0.3104 - val_acc: 0.8828 - val_auROC: 0.9099\n",
      "Epoch 290/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2672 - acc: 0.9251 - auROC: 0.9649 - val_loss: 0.3102 - val_acc: 0.8828 - val_auROC: 0.9091\n",
      "Epoch 291/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2671 - acc: 0.9251 - auROC: 0.9649 - val_loss: 0.3102 - val_acc: 0.8828 - val_auROC: 0.9089\n",
      "Epoch 292/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2670 - acc: 0.9248 - auROC: 0.9649 - val_loss: 0.3104 - val_acc: 0.8793 - val_auROC: 0.9084\n",
      "Epoch 293/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2669 - acc: 0.9240 - auROC: 0.9649 - val_loss: 0.3106 - val_acc: 0.8793 - val_auROC: 0.9079\n",
      "Epoch 294/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2668 - acc: 0.9244 - auROC: 0.9649 - val_loss: 0.3106 - val_acc: 0.8828 - val_auROC: 0.9077\n",
      "Epoch 295/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2667 - acc: 0.9259 - auROC: 0.9649 - val_loss: 0.3108 - val_acc: 0.8828 - val_auROC: 0.9071\n",
      "Epoch 296/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2667 - acc: 0.9263 - auROC: 0.9650 - val_loss: 0.3108 - val_acc: 0.8828 - val_auROC: 0.9067\n",
      "Epoch 297/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2666 - acc: 0.9259 - auROC: 0.9650 - val_loss: 0.3109 - val_acc: 0.8828 - val_auROC: 0.9061\n",
      "Epoch 298/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2665 - acc: 0.9259 - auROC: 0.9651 - val_loss: 0.3105 - val_acc: 0.8828 - val_auROC: 0.9070\n",
      "Epoch 299/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2664 - acc: 0.9267 - auROC: 0.9651 - val_loss: 0.3103 - val_acc: 0.8828 - val_auROC: 0.9076\n",
      "Epoch 300/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2664 - acc: 0.9267 - auROC: 0.9651 - val_loss: 0.3101 - val_acc: 0.8828 - val_auROC: 0.9085\n",
      "Epoch 301/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2663 - acc: 0.9267 - auROC: 0.9651 - val_loss: 0.3098 - val_acc: 0.8828 - val_auROC: 0.9088\n",
      "Epoch 302/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2662 - acc: 0.9267 - auROC: 0.9651 - val_loss: 0.3097 - val_acc: 0.8828 - val_auROC: 0.9090\n",
      "Epoch 303/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2662 - acc: 0.9271 - auROC: 0.9650 - val_loss: 0.3097 - val_acc: 0.8828 - val_auROC: 0.9093\n",
      "Epoch 304/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2661 - acc: 0.9267 - auROC: 0.9651 - val_loss: 0.3097 - val_acc: 0.8828 - val_auROC: 0.9078\n",
      "Epoch 305/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2659 - acc: 0.9263 - auROC: 0.9652 - val_loss: 0.3092 - val_acc: 0.8828 - val_auROC: 0.9088\n",
      "Epoch 306/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2659 - acc: 0.9267 - auROC: 0.9652 - val_loss: 0.3089 - val_acc: 0.8828 - val_auROC: 0.9090\n",
      "Epoch 307/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2658 - acc: 0.9267 - auROC: 0.9652 - val_loss: 0.3087 - val_acc: 0.8828 - val_auROC: 0.9093\n",
      "Epoch 308/361\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.2657 - acc: 0.9267 - auROC: 0.9652 - val_loss: 0.3089 - val_acc: 0.8828 - val_auROC: 0.9084\n",
      "Epoch 309/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2657 - acc: 0.9271 - auROC: 0.9653 - val_loss: 0.3089 - val_acc: 0.8828 - val_auROC: 0.9084\n",
      "Epoch 310/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2656 - acc: 0.9271 - auROC: 0.9653 - val_loss: 0.3089 - val_acc: 0.8828 - val_auROC: 0.9086\n",
      "Epoch 311/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2655 - acc: 0.9267 - auROC: 0.9652 - val_loss: 0.3093 - val_acc: 0.8793 - val_auROC: 0.9084\n",
      "Epoch 312/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2655 - acc: 0.9267 - auROC: 0.9652 - val_loss: 0.3091 - val_acc: 0.8793 - val_auROC: 0.9083\n",
      "Epoch 313/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2654 - acc: 0.9271 - auROC: 0.9653 - val_loss: 0.3092 - val_acc: 0.8828 - val_auROC: 0.9083\n",
      "Epoch 314/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2654 - acc: 0.9275 - auROC: 0.9652 - val_loss: 0.3091 - val_acc: 0.8828 - val_auROC: 0.9082\n",
      "Epoch 315/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2653 - acc: 0.9275 - auROC: 0.9652 - val_loss: 0.3092 - val_acc: 0.8828 - val_auROC: 0.9076\n",
      "Epoch 316/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2653 - acc: 0.9267 - auROC: 0.9653 - val_loss: 0.3091 - val_acc: 0.8828 - val_auROC: 0.9074\n",
      "Epoch 317/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2652 - acc: 0.9267 - auROC: 0.9653 - val_loss: 0.3089 - val_acc: 0.8828 - val_auROC: 0.9084\n",
      "Epoch 318/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2651 - acc: 0.9271 - auROC: 0.9653 - val_loss: 0.3087 - val_acc: 0.8828 - val_auROC: 0.9086\n",
      "Epoch 319/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2651 - acc: 0.9267 - auROC: 0.9653 - val_loss: 0.3084 - val_acc: 0.8759 - val_auROC: 0.9087\n",
      "Epoch 320/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2650 - acc: 0.9259 - auROC: 0.9653 - val_loss: 0.3084 - val_acc: 0.8828 - val_auROC: 0.9089\n",
      "Epoch 321/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2650 - acc: 0.9263 - auROC: 0.9654 - val_loss: 0.3083 - val_acc: 0.8828 - val_auROC: 0.9090\n",
      "Epoch 322/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2649 - acc: 0.9275 - auROC: 0.9654 - val_loss: 0.3082 - val_acc: 0.8828 - val_auROC: 0.9086\n",
      "Epoch 323/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2649 - acc: 0.9275 - auROC: 0.9653 - val_loss: 0.3082 - val_acc: 0.8828 - val_auROC: 0.9088\n",
      "Epoch 324/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2648 - acc: 0.9275 - auROC: 0.9653 - val_loss: 0.3081 - val_acc: 0.8828 - val_auROC: 0.9091\n",
      "Epoch 325/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2647 - acc: 0.9275 - auROC: 0.9654 - val_loss: 0.3079 - val_acc: 0.8828 - val_auROC: 0.9090\n",
      "Epoch 326/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2647 - acc: 0.9279 - auROC: 0.9654 - val_loss: 0.3079 - val_acc: 0.8828 - val_auROC: 0.9091\n",
      "Epoch 327/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2646 - acc: 0.9279 - auROC: 0.9654 - val_loss: 0.3078 - val_acc: 0.8828 - val_auROC: 0.9093\n",
      "Epoch 328/361\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.2646 - acc: 0.9275 - auROC: 0.9653 - val_loss: 0.3078 - val_acc: 0.8828 - val_auROC: 0.9091\n",
      "Epoch 329/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2646 - acc: 0.9275 - auROC: 0.9654 - val_loss: 0.3081 - val_acc: 0.8828 - val_auROC: 0.9089\n",
      "Epoch 330/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2645 - acc: 0.9275 - auROC: 0.9654 - val_loss: 0.3077 - val_acc: 0.8828 - val_auROC: 0.9096\n",
      "Epoch 331/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2644 - acc: 0.9279 - auROC: 0.9653 - val_loss: 0.3073 - val_acc: 0.8828 - val_auROC: 0.9095\n",
      "Epoch 332/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2644 - acc: 0.9279 - auROC: 0.9653 - val_loss: 0.3071 - val_acc: 0.8828 - val_auROC: 0.9102\n",
      "Epoch 333/361\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2644 - acc: 0.9279 - auROC: 0.9653 - val_loss: 0.3069 - val_acc: 0.8862 - val_auROC: 0.9122\n",
      "Epoch 334/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2644 - acc: 0.9287 - auROC: 0.9652 - val_loss: 0.3055 - val_acc: 0.8862 - val_auROC: 0.9133\n",
      "Epoch 335/361\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2642 - acc: 0.9283 - auROC: 0.9651 - val_loss: 0.3047 - val_acc: 0.8828 - val_auROC: 0.9138\n",
      "Epoch 336/361\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.2641 - acc: 0.9275 - auROC: 0.9651 - val_loss: 0.3045 - val_acc: 0.8828 - val_auROC: 0.9145\n",
      "Epoch 337/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2640 - acc: 0.9275 - auROC: 0.9651 - val_loss: 0.3046 - val_acc: 0.8828 - val_auROC: 0.9147\n",
      "Epoch 338/361\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.2639 - acc: 0.9283 - auROC: 0.9650 - val_loss: 0.3042 - val_acc: 0.8828 - val_auROC: 0.9142\n",
      "Epoch 339/361\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2638 - acc: 0.9283 - auROC: 0.9650 - val_loss: 0.3041 - val_acc: 0.8828 - val_auROC: 0.9145\n",
      "Epoch 340/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2637 - acc: 0.9283 - auROC: 0.9650 - val_loss: 0.3040 - val_acc: 0.8828 - val_auROC: 0.9146\n",
      "Epoch 341/361\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.2636 - acc: 0.9290 - auROC: 0.9650 - val_loss: 0.3039 - val_acc: 0.8828 - val_auROC: 0.9146\n",
      "Epoch 342/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2636 - acc: 0.9283 - auROC: 0.9650 - val_loss: 0.3049 - val_acc: 0.8793 - val_auROC: 0.9134\n",
      "Epoch 343/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2637 - acc: 0.9279 - auROC: 0.9650 - val_loss: 0.3044 - val_acc: 0.8828 - val_auROC: 0.9138\n",
      "Epoch 344/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2635 - acc: 0.9294 - auROC: 0.9650 - val_loss: 0.3042 - val_acc: 0.8828 - val_auROC: 0.9139\n",
      "Epoch 345/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2634 - acc: 0.9294 - auROC: 0.9650 - val_loss: 0.3048 - val_acc: 0.8828 - val_auROC: 0.9122\n",
      "Epoch 346/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2632 - acc: 0.9287 - auROC: 0.9651 - val_loss: 0.3054 - val_acc: 0.8793 - val_auROC: 0.9116\n",
      "Epoch 347/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2630 - acc: 0.9271 - auROC: 0.9652 - val_loss: 0.3054 - val_acc: 0.8793 - val_auROC: 0.9116\n",
      "Epoch 348/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2629 - acc: 0.9283 - auROC: 0.9652 - val_loss: 0.3051 - val_acc: 0.8828 - val_auROC: 0.9117\n",
      "Epoch 349/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2628 - acc: 0.9287 - auROC: 0.9652 - val_loss: 0.3055 - val_acc: 0.8828 - val_auROC: 0.9109\n",
      "Epoch 350/361\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.2627 - acc: 0.9275 - auROC: 0.9652 - val_loss: 0.3054 - val_acc: 0.8793 - val_auROC: 0.9104\n",
      "Epoch 351/361\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2626 - acc: 0.9271 - auROC: 0.9652 - val_loss: 0.3051 - val_acc: 0.8793 - val_auROC: 0.9107\n",
      "Epoch 352/361\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2626 - acc: 0.9271 - auROC: 0.9652 - val_loss: 0.3049 - val_acc: 0.8828 - val_auROC: 0.9113\n",
      "Epoch 353/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2625 - acc: 0.9279 - auROC: 0.9652 - val_loss: 0.3049 - val_acc: 0.8828 - val_auROC: 0.9117\n",
      "Epoch 354/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2624 - acc: 0.9283 - auROC: 0.9652 - val_loss: 0.3050 - val_acc: 0.8793 - val_auROC: 0.9116\n",
      "Epoch 355/361\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2624 - acc: 0.9275 - auROC: 0.9653 - val_loss: 0.3052 - val_acc: 0.8793 - val_auROC: 0.9110\n",
      "Epoch 356/361\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.2623 - acc: 0.9275 - auROC: 0.9653Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2623 - acc: 0.9275 - auROC: 0.9653 - val_loss: 0.3054 - val_acc: 0.8793 - val_auROC: 0.9107\n",
      "Epoch 00356: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "1     0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "2     0.0    0.0    0.0  0.042440  ...    0.0 -0.198785 -0.197201    0.0\n",
      "3     0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "4     0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "60    0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "61    0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "62    0.0    0.0    0.0  1.044472  ...    0.0 -0.198785 -0.197201    0.0\n",
      "63    0.0    0.0    0.0 -0.353448  ...    0.0 -0.198785 -0.197201    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                  ...                                      \n",
      "0.00   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.01   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.02   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.03   3  58   0   0  0.0492  0.0  ...  0.9508  0.0  0.0 NaN      0.0    NaN\n",
      "0.04  18  43   0   0  0.2951  0.0  ...  0.7049  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...  ...  ..      ...    ...\n",
      "0.97  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr     F1  ROC-AUC   F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.9394  0.8718\n",
      "0.01   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.9394  0.8718\n",
      "0.02   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.9394  0.8718\n",
      "0.03   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.9394  0.8718\n",
      "0.04   0  41   0  21  0.3387  1.0  ...  1.0  1.0  0.3387  0.506   0.9394  0.8718\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...    ...      ...     ...\n",
      "0.97  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.9394  0.8718\n",
      "0.98  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.9394  0.8718\n",
      "0.99  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.9394  0.8718\n",
      "1.00  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.9394  0.8718\n",
      "1.01  41   0  21   0  0.6613  0.0  ...  0.0  0.0  0.0000    NaN   0.9394  0.8718\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.8863  0.7368\n",
      "0.01   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.8863  0.7368\n",
      "0.02   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.8863  0.7368\n",
      "0.03   0  49   0  13  0.2097  1.0  ...  1.0  1.0  0.2097  0.3467   0.8863  0.7368\n",
      "0.04   0  48   0  13  0.2131  1.0  ...  1.0  1.0  0.2131  0.3514   0.8863  0.7368\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.8863  0.7368\n",
      "0.98  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.8863  0.7368\n",
      "0.99  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.8863  0.7368\n",
      "1.00  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.8863  0.7368\n",
      "1.01  49   0  13   0  0.7903  0.0  ...  0.0  0.0  0.0000     NaN   0.8863  0.7368\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                          \n",
      "0.00   0  56   0   6  0.0968  1.0  ...  1.0  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.01   0  56   0   6  0.0968  1.0  ...  1.0  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.02   0  56   0   6  0.0968  1.0  ...  1.0  1.0  0.0968  0.1765      1.0    1.0\n",
      "0.03   0  55   0   6  0.0984  1.0  ...  1.0  1.0  0.0984  0.1791      1.0    1.0\n",
      "0.04   0  55   0   6  0.0984  1.0  ...  1.0  1.0  0.0984  0.1791      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...    ...\n",
      "0.97  56   0   6   0  0.9032  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  56   0   6   0  0.9032  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  56   0   6   0  0.9032  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  56   0   6   0  0.9032  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  56   0   6   0  0.9032  0.0  ...  0.0  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...  FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                           \n",
      "0.00   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9848  0.9231\n",
      "0.01   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9848  0.9231\n",
      "0.02   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9848  0.9231\n",
      "0.03   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9848  0.9231\n",
      "0.04   0  43   0  19  0.3065  1.0  ...  1.0  1.0  0.3065  0.4691   0.9848  0.9231\n",
      "...   ..  ..  ..  ..     ...  ...  ...  ...  ...     ...     ...      ...     ...\n",
      "0.97  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9848  0.9231\n",
      "0.98  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9848  0.9231\n",
      "0.99  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9848  0.9231\n",
      "1.00  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9848  0.9231\n",
      "1.01  43   0  19   0  0.6935  0.0  ...  0.0  0.0  0.0000     NaN   0.9848  0.9231\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n",
      "Reordering labels and samples...\n",
      "Total matched samples: 571\n",
      "Total correct samples: 571?571\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.015718  0.044470\n",
      "4      0.015697  0.044456\n",
      "...         ...       ...\n",
      "18013  0.000059  0.000240\n",
      "18014  0.000000  0.000000\n",
      "18015  0.002371  0.011928\n",
      "18016  0.002307  0.011699\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Training using optimizer with lr=0.001...\n",
      "Epoch 1/300\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.6473 - acc: 0.6339 - auROC: 0.5087 - val_loss: 0.5916 - val_acc: 0.6724 - val_auROC: 0.5473\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.5659 - acc: 0.7185 - auROC: 0.5642 - val_loss: 0.5459 - val_acc: 0.7103 - val_auROC: 0.6066\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.5235 - acc: 0.7665 - auROC: 0.6353 - val_loss: 0.5003 - val_acc: 0.7862 - val_auROC: 0.6861\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4935 - acc: 0.7852 - auROC: 0.6948 - val_loss: 0.4730 - val_acc: 0.7793 - val_auROC: 0.7402\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4735 - acc: 0.7895 - auROC: 0.7296 - val_loss: 0.4636 - val_acc: 0.7828 - val_auROC: 0.7618\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4617 - acc: 0.7992 - auROC: 0.7694 - val_loss: 0.4470 - val_acc: 0.7931 - val_auROC: 0.7847\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.4501 - acc: 0.7988 - auROC: 0.7726 - val_loss: 0.4383 - val_acc: 0.7966 - val_auROC: 0.7953\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4439 - acc: 0.8043 - auROC: 0.7725 - val_loss: 0.4345 - val_acc: 0.8034 - val_auROC: 0.7935\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4399 - acc: 0.8117 - auROC: 0.7658 - val_loss: 0.4321 - val_acc: 0.7931 - val_auROC: 0.8077\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4331 - acc: 0.8109 - auROC: 0.7934 - val_loss: 0.4193 - val_acc: 0.8069 - val_auROC: 0.8372\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4263 - acc: 0.8121 - auROC: 0.8043 - val_loss: 0.4130 - val_acc: 0.8138 - val_auROC: 0.8393\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.4184 - acc: 0.8164 - auROC: 0.8196 - val_loss: 0.4047 - val_acc: 0.8241 - val_auROC: 0.8472\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4127 - acc: 0.8183 - auROC: 0.8237 - val_loss: 0.4009 - val_acc: 0.8103 - val_auROC: 0.8539\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.4088 - acc: 0.8214 - auROC: 0.8304 - val_loss: 0.3986 - val_acc: 0.8069 - val_auROC: 0.8494\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4026 - acc: 0.8211 - auROC: 0.8347 - val_loss: 0.3945 - val_acc: 0.8345 - val_auROC: 0.8534\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4040 - acc: 0.8324 - auROC: 0.8137 - val_loss: 0.3925 - val_acc: 0.8172 - val_auROC: 0.8600\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3997 - acc: 0.8257 - auROC: 0.8333 - val_loss: 0.3938 - val_acc: 0.8138 - val_auROC: 0.8412\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4017 - acc: 0.8347 - auROC: 0.8119 - val_loss: 0.3858 - val_acc: 0.8241 - val_auROC: 0.8598\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3901 - acc: 0.8343 - auROC: 0.8374 - val_loss: 0.3744 - val_acc: 0.8379 - val_auROC: 0.8775\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3908 - acc: 0.8398 - auROC: 0.8355 - val_loss: 0.3718 - val_acc: 0.8276 - val_auROC: 0.8783\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3839 - acc: 0.8382 - auROC: 0.8445 - val_loss: 0.3736 - val_acc: 0.8310 - val_auROC: 0.8714\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3875 - acc: 0.8366 - auROC: 0.8346 - val_loss: 0.3788 - val_acc: 0.8276 - val_auROC: 0.8722\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3811 - acc: 0.8398 - auROC: 0.8479 - val_loss: 0.3605 - val_acc: 0.8483 - val_auROC: 0.8947\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3748 - acc: 0.8444 - auROC: 0.8634 - val_loss: 0.3618 - val_acc: 0.8379 - val_auROC: 0.8896\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3677 - acc: 0.8464 - auROC: 0.8683 - val_loss: 0.3542 - val_acc: 0.8586 - val_auROC: 0.8922\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3723 - acc: 0.8417 - auROC: 0.8588 - val_loss: 0.3540 - val_acc: 0.8483 - val_auROC: 0.8976\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3676 - acc: 0.8480 - auROC: 0.8681 - val_loss: 0.3556 - val_acc: 0.8586 - val_auROC: 0.8803\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3684 - acc: 0.8503 - auROC: 0.8543 - val_loss: 0.3508 - val_acc: 0.8517 - val_auROC: 0.8814\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.3610 - acc: 0.8515 - auROC: 0.8696 - val_loss: 0.3471 - val_acc: 0.8517 - val_auROC: 0.9013\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3529 - acc: 0.8616 - auROC: 0.8776 - val_loss: 0.3511 - val_acc: 0.8621 - val_auROC: 0.8811\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3597 - acc: 0.8671 - auROC: 0.8602 - val_loss: 0.3480 - val_acc: 0.8655 - val_auROC: 0.8782\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3477 - acc: 0.8671 - auROC: 0.8781 - val_loss: 0.3335 - val_acc: 0.8828 - val_auROC: 0.9016\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3387 - acc: 0.8678 - auROC: 0.8895 - val_loss: 0.3271 - val_acc: 0.8759 - val_auROC: 0.9111\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3312 - acc: 0.8756 - auROC: 0.8989 - val_loss: 0.3306 - val_acc: 0.8655 - val_auROC: 0.9019\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3400 - acc: 0.8706 - auROC: 0.8856 - val_loss: 0.3340 - val_acc: 0.8621 - val_auROC: 0.9032\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3344 - acc: 0.8710 - auROC: 0.8908 - val_loss: 0.3142 - val_acc: 0.8586 - val_auROC: 0.9193\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3311 - acc: 0.8694 - auROC: 0.8912 - val_loss: 0.3443 - val_acc: 0.8552 - val_auROC: 0.8773\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3465 - acc: 0.8554 - auROC: 0.8736 - val_loss: 0.3149 - val_acc: 0.8724 - val_auROC: 0.9245\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3574 - acc: 0.8558 - auROC: 0.8576 - val_loss: 0.3690 - val_acc: 0.8517 - val_auROC: 0.8413\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3574 - acc: 0.8561 - auROC: 0.8571 - val_loss: 0.3428 - val_acc: 0.8310 - val_auROC: 0.8809\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3298 - acc: 0.8721 - auROC: 0.8882 - val_loss: 0.3139 - val_acc: 0.8828 - val_auROC: 0.9076\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3139 - acc: 0.8869 - auROC: 0.9017 - val_loss: 0.3074 - val_acc: 0.8759 - val_auROC: 0.9199\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.3146 - acc: 0.8834 - auROC: 0.9056 - val_loss: 0.3015 - val_acc: 0.8759 - val_auROC: 0.9278\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3083 - acc: 0.8924 - auROC: 0.9090 - val_loss: 0.2929 - val_acc: 0.9000 - val_auROC: 0.9253\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3018 - acc: 0.8947 - auROC: 0.9113 - val_loss: 0.2824 - val_acc: 0.9000 - val_auROC: 0.9394\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2921 - acc: 0.9033 - auROC: 0.9198 - val_loss: 0.2874 - val_acc: 0.9000 - val_auROC: 0.9318\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2855 - acc: 0.9099 - auROC: 0.9242 - val_loss: 0.2782 - val_acc: 0.9000 - val_auROC: 0.9389\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2800 - acc: 0.9146 - auROC: 0.9288 - val_loss: 0.2942 - val_acc: 0.8862 - val_auROC: 0.9210\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2870 - acc: 0.9018 - auROC: 0.9189 - val_loss: 0.2854 - val_acc: 0.9172 - val_auROC: 0.9321\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2894 - acc: 0.9146 - auROC: 0.9167 - val_loss: 0.2691 - val_acc: 0.9241 - val_auROC: 0.9460\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2893 - acc: 0.9029 - auROC: 0.9178 - val_loss: 0.2972 - val_acc: 0.8931 - val_auROC: 0.9156\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3034 - acc: 0.8897 - auROC: 0.9032 - val_loss: 0.2529 - val_acc: 0.9345 - val_auROC: 0.9641\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2774 - acc: 0.9076 - auROC: 0.9252 - val_loss: 0.2525 - val_acc: 0.9310 - val_auROC: 0.9589\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2777 - acc: 0.9135 - auROC: 0.9251 - val_loss: 0.2562 - val_acc: 0.9103 - val_auROC: 0.9605\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2608 - acc: 0.9212 - auROC: 0.9417 - val_loss: 0.2381 - val_acc: 0.9414 - val_auROC: 0.9744\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2504 - acc: 0.9267 - auROC: 0.9449 - val_loss: 0.2301 - val_acc: 0.9414 - val_auROC: 0.9783\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2406 - acc: 0.9372 - auROC: 0.9526 - val_loss: 0.2464 - val_acc: 0.9310 - val_auROC: 0.9668\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.2797 - acc: 0.9150 - auROC: 0.9185 - val_loss: 0.2715 - val_acc: 0.9138 - val_auROC: 0.9332\n",
      "Epoch 59/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2569 - acc: 0.9220 - auROC: 0.9369 - val_loss: 0.2600 - val_acc: 0.8966 - val_auROC: 0.9511\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2401 - acc: 0.9326 - auROC: 0.9526 - val_loss: 0.2312 - val_acc: 0.9276 - val_auROC: 0.9760\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2312 - acc: 0.9361 - auROC: 0.9560 - val_loss: 0.2255 - val_acc: 0.9414 - val_auROC: 0.9741\n",
      "Epoch 62/300\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.2220 - acc: 0.9493 - auROC: 0.9578 - val_loss: 0.2217 - val_acc: 0.9379 - val_auROC: 0.9789\n",
      "Epoch 63/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2176 - acc: 0.9509 - auROC: 0.9608 - val_loss: 0.2106 - val_acc: 0.9483 - val_auROC: 0.9802\n",
      "Epoch 64/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2117 - acc: 0.9524 - auROC: 0.9626 - val_loss: 0.2082 - val_acc: 0.9483 - val_auROC: 0.9827\n",
      "Epoch 65/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2062 - acc: 0.9575 - auROC: 0.9636 - val_loss: 0.2022 - val_acc: 0.9517 - val_auROC: 0.9810\n",
      "Epoch 66/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.2034 - acc: 0.9583 - auROC: 0.9644 - val_loss: 0.1961 - val_acc: 0.9586 - val_auROC: 0.9819\n",
      "Epoch 67/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.1991 - acc: 0.9595 - auROC: 0.9656 - val_loss: 0.1941 - val_acc: 0.9586 - val_auROC: 0.9825\n",
      "Epoch 68/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.1925 - acc: 0.9626 - auROC: 0.9675 - val_loss: 0.1895 - val_acc: 0.9655 - val_auROC: 0.9851\n",
      "Epoch 69/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1903 - acc: 0.9637 - auROC: 0.9675 - val_loss: 0.1861 - val_acc: 0.9586 - val_auROC: 0.9852\n",
      "Epoch 70/300\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.1865 - acc: 0.9649 - auROC: 0.9687 - val_loss: 0.1779 - val_acc: 0.9655 - val_auROC: 0.9877\n",
      "Epoch 71/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1879 - acc: 0.9602 - auROC: 0.9656 - val_loss: 0.1753 - val_acc: 0.9690 - val_auROC: 0.9864\n",
      "Epoch 72/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1850 - acc: 0.9622 - auROC: 0.9661 - val_loss: 0.1744 - val_acc: 0.9655 - val_auROC: 0.9878\n",
      "Epoch 73/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1906 - acc: 0.9556 - auROC: 0.9664 - val_loss: 0.1882 - val_acc: 0.9448 - val_auROC: 0.9806\n",
      "Epoch 74/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1845 - acc: 0.9653 - auROC: 0.9683 - val_loss: 0.1841 - val_acc: 0.9517 - val_auROC: 0.9823\n",
      "Epoch 75/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1854 - acc: 0.9622 - auROC: 0.9659 - val_loss: 0.1857 - val_acc: 0.9483 - val_auROC: 0.9820\n",
      "Epoch 76/300\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1826 - acc: 0.9579 - auROC: 0.9690 - val_loss: 0.1691 - val_acc: 0.9655 - val_auROC: 0.9873\n",
      "Epoch 77/300\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1728 - acc: 0.9661 - auROC: 0.9714 - val_loss: 0.1782 - val_acc: 0.9586 - val_auROC: 0.9824\n",
      "Epoch 78/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1705 - acc: 0.9665 - auROC: 0.9721 - val_loss: 0.1712 - val_acc: 0.9586 - val_auROC: 0.9829\n",
      "Epoch 79/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1761 - acc: 0.9630 - auROC: 0.9650 - val_loss: 0.2481 - val_acc: 0.9034 - val_auROC: 0.9354\n",
      "Epoch 80/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2780 - acc: 0.8982 - auROC: 0.9160 - val_loss: 0.2956 - val_acc: 0.8897 - val_auROC: 0.9022\n",
      "Epoch 81/300\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 0.2495 - acc: 0.9044 - auROC: 0.9335\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2811 - acc: 0.8877 - auROC: 0.9107 - val_loss: 0.2855 - val_acc: 0.8897 - val_auROC: 0.9120\n",
      "Epoch 82/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2611 - acc: 0.9002 - auROC: 0.9261 - val_loss: 0.2736 - val_acc: 0.9034 - val_auROC: 0.9187\n",
      "Epoch 83/300\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2472 - acc: 0.9154 - auROC: 0.9344 - val_loss: 0.2587 - val_acc: 0.9172 - val_auROC: 0.9259\n",
      "Epoch 84/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2348 - acc: 0.9228 - auROC: 0.9418 - val_loss: 0.2413 - val_acc: 0.9276 - val_auROC: 0.9339\n",
      "Epoch 85/300\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2245 - acc: 0.9310 - auROC: 0.9471 - val_loss: 0.2339 - val_acc: 0.9310 - val_auROC: 0.9368\n",
      "Epoch 86/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2192 - acc: 0.9366 - auROC: 0.9478\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2157 - acc: 0.9384 - auROC: 0.9510 - val_loss: 0.2220 - val_acc: 0.9414 - val_auROC: 0.9433\n",
      "Epoch 87/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2095 - acc: 0.9431 - auROC: 0.9548 - val_loss: 0.2211 - val_acc: 0.9414 - val_auROC: 0.9442\n",
      "Epoch 88/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2087 - acc: 0.9435 - auROC: 0.9554 - val_loss: 0.2207 - val_acc: 0.9448 - val_auROC: 0.9438\n",
      "Epoch 89/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2079 - acc: 0.9439 - auROC: 0.9555 - val_loss: 0.2204 - val_acc: 0.9448 - val_auROC: 0.9434\n",
      "Epoch 90/300\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2072 - acc: 0.9435 - auROC: 0.9556 - val_loss: 0.2200 - val_acc: 0.9517 - val_auROC: 0.9431\n",
      "Epoch 91/300\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 0.2058 - acc: 0.9424 - auROC: 0.9583\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.2064 - acc: 0.9431 - auROC: 0.9562 - val_loss: 0.2194 - val_acc: 0.9517 - val_auROC: 0.9432\n",
      "Epoch 00091: early stopping\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 10)                21550     \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 18,998,051\n",
      "Trainable params: 21,795\n",
      "Non-trainable params: 18,976,256\n",
      "_________________________________________________________________\n",
      "Fine-tuning using optimizer with lr=1e-05...\n",
      "Epoch 91/390\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 0.1700 - acc: 0.9676 - auROC: 0.9722 - val_loss: 0.1690 - val_acc: 0.9586 - val_auROC: 0.9867\n",
      "Epoch 92/390\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1664 - acc: 0.9696 - auROC: 0.9728 - val_loss: 0.1673 - val_acc: 0.9621 - val_auROC: 0.9865\n",
      "Epoch 93/390\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1640 - acc: 0.9696 - auROC: 0.9738 - val_loss: 0.1662 - val_acc: 0.9621 - val_auROC: 0.9867\n",
      "Epoch 94/390\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1624 - acc: 0.9708 - auROC: 0.9741 - val_loss: 0.1648 - val_acc: 0.9621 - val_auROC: 0.9871\n",
      "Epoch 95/390\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1613 - acc: 0.9712 - auROC: 0.9743 - val_loss: 0.1651 - val_acc: 0.9621 - val_auROC: 0.9871\n",
      "Epoch 96/390\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1600 - acc: 0.9715 - auROC: 0.9749 - val_loss: 0.1639 - val_acc: 0.9621 - val_auROC: 0.9874\n",
      "Epoch 97/390\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1591 - acc: 0.9719 - auROC: 0.9750 - val_loss: 0.1642 - val_acc: 0.9621 - val_auROC: 0.9868\n",
      "Epoch 98/390\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1584 - acc: 0.9727 - auROC: 0.9749 - val_loss: 0.1642 - val_acc: 0.9621 - val_auROC: 0.9870\n",
      "Epoch 99/390\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1577 - acc: 0.9731 - auROC: 0.9753 - val_loss: 0.1634 - val_acc: 0.9621 - val_auROC: 0.9871\n",
      "Epoch 100/390\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1572 - acc: 0.9735 - auROC: 0.9757 - val_loss: 0.1623 - val_acc: 0.9621 - val_auROC: 0.9874\n",
      "Epoch 101/390\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1566 - acc: 0.9735 - auROC: 0.9757 - val_loss: 0.1619 - val_acc: 0.9621 - val_auROC: 0.9872\n",
      "Epoch 102/390\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1561 - acc: 0.9739 - auROC: 0.9757 - val_loss: 0.1617 - val_acc: 0.9621 - val_auROC: 0.9872\n",
      "Epoch 103/390\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1562 - acc: 0.9743 - auROC: 0.9752 - val_loss: 0.1641 - val_acc: 0.9621 - val_auROC: 0.9860\n",
      "Epoch 104/390\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1561 - acc: 0.9743 - auROC: 0.9754 - val_loss: 0.1622 - val_acc: 0.9621 - val_auROC: 0.9866\n",
      "Epoch 105/390\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.1555 - acc: 0.9743 - auROC: 0.9757 - val_loss: 0.1596 - val_acc: 0.9621 - val_auROC: 0.9879\n",
      "Epoch 106/390\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.1550 - acc: 0.9743 - auROC: 0.9756 - val_loss: 0.1586 - val_acc: 0.9621 - val_auROC: 0.9880\n",
      "Epoch 107/390\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1545 - acc: 0.9750 - auROC: 0.9757 - val_loss: 0.1587 - val_acc: 0.9621 - val_auROC: 0.9881\n",
      "Epoch 108/390\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1540 - acc: 0.9750 - auROC: 0.9757 - val_loss: 0.1585 - val_acc: 0.9621 - val_auROC: 0.9880\n",
      "Epoch 109/390\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.1535 - acc: 0.9747 - auROC: 0.9762 - val_loss: 0.1583 - val_acc: 0.9586 - val_auROC: 0.9881\n",
      "Epoch 110/390\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1527 - acc: 0.9754 - auROC: 0.9764 - val_loss: 0.1635 - val_acc: 0.9586 - val_auROC: 0.9810\n",
      "Epoch 111/390\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1524 - acc: 0.9762 - auROC: 0.9764 - val_loss: 0.1647 - val_acc: 0.9586 - val_auROC: 0.9796\n",
      "Epoch 112/390\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1521 - acc: 0.9766 - auROC: 0.9767 - val_loss: 0.1652 - val_acc: 0.9586 - val_auROC: 0.9785\n",
      "Epoch 113/390\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.1523 - acc: 0.9758 - auROC: 0.9772 - val_loss: 0.1650 - val_acc: 0.9586 - val_auROC: 0.9777\n",
      "Epoch 114/390\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1532 - acc: 0.9750 - auROC: 0.9778 - val_loss: 0.1659 - val_acc: 0.9586 - val_auROC: 0.9770\n",
      "Epoch 115/390\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1517 - acc: 0.9762 - auROC: 0.9781 - val_loss: 0.1662 - val_acc: 0.9586 - val_auROC: 0.9771\n",
      "Epoch 116/390\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.1509 - acc: 0.9770 - auROC: 0.9778 - val_loss: 0.1669 - val_acc: 0.9586 - val_auROC: 0.9770\n",
      "Epoch 117/390\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1505 - acc: 0.9774 - auROC: 0.9778 - val_loss: 0.1668 - val_acc: 0.9586 - val_auROC: 0.9766\n",
      "Epoch 118/390\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1502 - acc: 0.9774 - auROC: 0.9779 - val_loss: 0.1668 - val_acc: 0.9586 - val_auROC: 0.9767\n",
      "Epoch 119/390\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.1499 - acc: 0.9774 - auROC: 0.9781 - val_loss: 0.1669 - val_acc: 0.9586 - val_auROC: 0.9767\n",
      "Epoch 120/390\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1496 - acc: 0.9774 - auROC: 0.9782 - val_loss: 0.1668 - val_acc: 0.9621 - val_auROC: 0.9767\n",
      "Epoch 121/390\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1494 - acc: 0.9774 - auROC: 0.9783 - val_loss: 0.1665 - val_acc: 0.9621 - val_auROC: 0.9767\n",
      "Epoch 122/390\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.1492 - acc: 0.9778 - auROC: 0.9782 - val_loss: 0.1660 - val_acc: 0.9655 - val_auROC: 0.9765\n",
      "Epoch 123/390\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.1489 - acc: 0.9782 - auROC: 0.9783 - val_loss: 0.1656 - val_acc: 0.9655 - val_auROC: 0.9768\n",
      "Epoch 124/390\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.1487 - acc: 0.9778 - auROC: 0.9787Restoring model weights from the end of the best epoch.\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.1487 - acc: 0.9778 - auROC: 0.9787 - val_loss: 0.1653 - val_acc: 0.9655 - val_auROC: 0.9767\n",
      "Epoch 00124: early stopping\n",
      "    0      1      2         3      ...  18014     18015     18016  18017\n",
      "0     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "1     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "2     0.0    0.0    0.0  0.042439  ...    0.0 -0.198785 -0.197201    0.0\n",
      "3     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "4     0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "..    ...    ...    ...       ...  ...    ...       ...       ...    ...\n",
      "59    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "60    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "61    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "62    0.0    0.0    0.0  1.044471  ...    0.0 -0.198785 -0.197201    0.0\n",
      "63    0.0    0.0    0.0 -0.353447  ...    0.0 -0.198785 -0.197201    0.0\n",
      "\n",
      "[64 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "Reordering labels and prediction result\n",
      "Reordering labels and prediction result for samples\n",
      "Running evaluation...\n",
      "Evaluating biome source: root:CRC (stage 0)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc   Pr  F1  ROC-AUC  F-max\n",
      "t                                  ...                                      \n",
      "0.00   0  62   0   0  0.0000  0.0  ...  1.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.01  15  46   0   0  0.2459  0.0  ...  0.7541  0.0  0.0 NaN      0.0    NaN\n",
      "0.02  26  35   0   0  0.4262  0.0  ...  0.5738  0.0  0.0 NaN      0.0    NaN\n",
      "0.03  37  24   0   0  0.6066  0.0  ...  0.3934  0.0  0.0 NaN      0.0    NaN\n",
      "0.04  43  18   0   0  0.7049  0.0  ...  0.2951  0.0  0.0 NaN      0.0    NaN\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...  ...  ..      ...    ...\n",
      "0.97  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.98  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "0.99  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.00  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "1.01  62   0   0   0  1.0000  0.0  ...  0.0000  0.0  0.0 NaN      0.0    NaN\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage I)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...    FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  41   0  21  0.3387  1.0  ...  1.000  1.0  0.3387  0.5060    0.975  0.9189\n",
      "0.01   0  41   0  21  0.3387  1.0  ...  1.000  1.0  0.3387  0.5060    0.975  0.9189\n",
      "0.02  11  29   0  21  0.5246  1.0  ...  0.725  1.0  0.4200  0.5915    0.975  0.9189\n",
      "0.03  11  29   0  21  0.5246  1.0  ...  0.725  1.0  0.4200  0.5915    0.975  0.9189\n",
      "0.04  13  27   0  21  0.5574  1.0  ...  0.675  1.0  0.4375  0.6087    0.975  0.9189\n",
      "...   ..  ..  ..  ..     ...  ...  ...    ...  ...     ...     ...      ...     ...\n",
      "0.97  41   0  21   0  0.6613  0.0  ...  0.000  0.0  0.0000     NaN    0.975  0.9189\n",
      "0.98  41   0  21   0  0.6613  0.0  ...  0.000  0.0  0.0000     NaN    0.975  0.9189\n",
      "0.99  41   0  21   0  0.6613  0.0  ...  0.000  0.0  0.0000     NaN    0.975  0.9189\n",
      "1.00  41   0  21   0  0.6613  0.0  ...  0.000  0.0  0.0000     NaN    0.975  0.9189\n",
      "1.01  41   0  21   0  0.6613  0.0  ...  0.000  0.0  0.0000     NaN    0.975  0.9189\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage II)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC  F-max\n",
      "t                                  ...                                             \n",
      "0.00   0  49   0  13  0.2097  1.0  ...  1.0000  1.0  0.2097  0.3467      1.0    1.0\n",
      "0.01   0  49   0  13  0.2097  1.0  ...  1.0000  1.0  0.2097  0.3467      1.0    1.0\n",
      "0.02  16  32   0  13  0.4754  1.0  ...  0.6667  1.0  0.2889  0.4483      1.0    1.0\n",
      "0.03  20  28   0  13  0.5410  1.0  ...  0.5833  1.0  0.3171  0.4815      1.0    1.0\n",
      "0.04  20  28   0  13  0.5410  1.0  ...  0.5833  1.0  0.3171  0.4815      1.0    1.0\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...    ...\n",
      "0.97  49   0  13   0  0.7903  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.98  49   0  13   0  0.7903  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "0.99  49   0  13   0  0.7903  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.00  49   0  13   0  0.7903  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "1.01  49   0  13   0  0.7903  0.0  ...  0.0000  0.0  0.0000     NaN      1.0    1.0\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage III)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765   0.9964  0.9231\n",
      "0.01   0  56   0   6  0.0968  1.0  ...  1.0000  1.0  0.0968  0.1765   0.9964  0.9231\n",
      "0.02   2  53   0   6  0.1311  1.0  ...  0.9636  1.0  0.1017  0.1846   0.9964  0.9231\n",
      "0.03   4  51   0   6  0.1639  1.0  ...  0.9273  1.0  0.1053  0.1905   0.9964  0.9231\n",
      "0.04  20  35   0   6  0.4262  1.0  ...  0.6364  1.0  0.1463  0.2553   0.9964  0.9231\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9964  0.9231\n",
      "0.98  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9964  0.9231\n",
      "0.99  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9964  0.9231\n",
      "1.00  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9964  0.9231\n",
      "1.01  56   0   6   0  0.9032  0.0  ...  0.0000  0.0  0.0000     NaN   0.9964  0.9231\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Evaluating biome source: root:CRC (stage IV)\n",
      "      TN  FP  FN  TP     Acc   Sn  ...     FPR   Rc      Pr      F1  ROC-AUC   F-max\n",
      "t                                  ...                                              \n",
      "0.00   0  43   0  19  0.3065  1.0  ...  1.0000  1.0  0.3065  0.4691   0.9921  0.9412\n",
      "0.01   0  43   0  19  0.3065  1.0  ...  1.0000  1.0  0.3065  0.4691   0.9921  0.9412\n",
      "0.02   0  43   0  19  0.3065  1.0  ...  1.0000  1.0  0.3065  0.4691   0.9921  0.9412\n",
      "0.03   1  41   0  19  0.3279  1.0  ...  0.9762  1.0  0.3167  0.4810   0.9921  0.9412\n",
      "0.04   5  37   0  19  0.3934  1.0  ...  0.8810  1.0  0.3393  0.5067   0.9921  0.9412\n",
      "...   ..  ..  ..  ..     ...  ...  ...     ...  ...     ...     ...      ...     ...\n",
      "0.97  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9921  0.9412\n",
      "0.98  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9921  0.9412\n",
      "0.99  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9921  0.9412\n",
      "1.00  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9921  0.9412\n",
      "1.01  43   0  19   0  0.6935  0.0  ...  0.0000  0.0  0.0000     NaN   0.9921  0.9412\n",
      "\n",
      "[102 rows x 14 columns]\n",
      "Saving evaluation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘running_2’: File exists\n",
      "2020-11-26 13:23:18.622388: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:23:18.629832: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:23:18.631653: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563da94111d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:23:18.631681: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-26 13:23:19.640297: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-11-26 13:23:20.730600: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-11-26 13:23:20.861344: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-11-26 13:23:20.907472: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-11-26 13:23:20.914999: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:23:44.560736: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1044.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 419.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.25it/s]\n",
      "2020-11-26 13:24:03.228798: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:24:03.237522: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:24:03.239466: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ec58afbfb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:24:03.239490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:26:40.061431: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 832.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 434.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 52.56it/s]\n",
      "2020-11-26 13:27:02.995491: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:27:03.004693: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:27:03.006648: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56168174e520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:27:03.006689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-26 13:27:03.124088: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-11-26 13:27:03.136909: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-11-26 13:27:03.144422: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-11-26 13:27:03.303255: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "2020-11-26 13:27:03.404579: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 73801728 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:28:25.163168: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 477.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.00it/s]\n",
      "2020-11-26 13:28:49.887619: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:28:49.896185: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:28:49.898579: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56229b3fef70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:28:49.898604: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:29:37.021212: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1099.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 541.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.75it/s]\n",
      "2020-11-26 13:29:58.027595: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:29:58.035884: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:29:58.038147: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556d50bbcf90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:29:58.038176: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:31:11.740723: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1082.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 509.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 51.54it/s]\n",
      "2020-11-26 13:31:32.833933: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:31:32.842511: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:31:32.844710: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5651a94a74f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:31:32.844737: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:33:55.870065: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1081.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 544.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 54.37it/s]\n",
      "2020-11-26 13:34:20.632478: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:34:20.641218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:34:20.643531: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56049cb911d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:34:20.643565: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-11-26 13:34:21.518990: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-11-26 13:34:22.437697: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:34:38.838778: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 911.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 474.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.46it/s]\n",
      "2020-11-26 13:34:57.353248: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:34:57.361799: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:34:57.363830: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f8d5ca1520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:34:57.363856: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:37:17.233905: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 1067.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 505.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 54.60it/s]\n",
      "2020-11-26 13:37:40.399928: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-11-26 13:37:40.408635: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499960000 Hz\n",
      "2020-11-26 13:37:40.410897: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5599ca7fa480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-26 13:37:40.410931: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "2020-11-26 13:37:44.333320: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-11-26 13:37:45.332555: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-11-26 13:37:46.556760: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "2020-11-26 13:37:47.572620: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 144288144 exceeds 10% of free system memory.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2020-11-26 13:38:20.143749: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "100%|██████████| 1/1 [00:00<00:00, 834.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 387.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 46.54it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {0,1,2,3,4}; do\n",
    "    mkdir running_$i;\n",
    "    expert train -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/TrainModel;\n",
    "    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/TrainModel -o experiments/exp_$i/SearchResult_Train;\n",
    "    expert evaluate -i experiments/exp_$i/SearchResult_Train -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Train -S 0;\n",
    "\n",
    "    expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_HM \\\n",
    "            -m HumanModel/ --finetune --update-statistics;\n",
    "    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/AdaptModel_ft_HM -o experiments/exp_$i/SearchResult_Adapt_ft_HM;\n",
    "    expert evaluate -i experiments/exp_$i/SearchResult_Adapt_ft_HM -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Adapt_ft_HM -S 0;\n",
    "\n",
    "    expert transfer -i experiments/exp_$i/SourceCM.h5 -t ontology.pkl -l experiments/exp_$i/SourceLabels.h5 -o experiments/exp_$i/AdaptModel_ft_DM \\\n",
    "            -m ../Disease-diagnosis/experiments/exp_3/TrainModel/ --finetune --update-statistics;\n",
    "    expert search -i experiments/exp_$i/QueryCM.h5 -m experiments/exp_$i/AdaptModel_ft_DM -o experiments/exp_$i/SearchResult_Adapt_ft_DM;\n",
    "    expert evaluate -i experiments/exp_$i/SearchResult_Adapt_ft_DM -l experiments/exp_$i/QueryLabels.h5 -o experiments/exp_$i/EvalResult_Adapt_ft_DM -S 0;\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8769686199556)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 4.8 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: Figure_performance_across_models.pdf\n"
     ]
    }
   ],
   "source": [
    "metrics =[pd.read_csv('experiments/exp_{}/EvalResult_{}/overall.csv'.format(i, suffix), index_col=0).rename(columns=lambda x: '{}-exp_{}-{}'.format(x, i, suffix)).dropna()\n",
    "        for i in range(5) for suffix in ['Train', 'Adapt_ft_DM', 'Adapt_ft_HM']]\n",
    "overall = pd.concat(metrics, axis=1)\n",
    "overall\n",
    "overall = overall.reset_index().melt(id_vars=['index'], value_vars=overall.columns.tolist(), var_name='metric').dropna()\n",
    "overall['Experiment'] = overall['metric'].str.split('-').apply(lambda x: '{}'.format( x[3])).map({'Adapt_ft_DM': 'Transfer (DM)', 'Adapt_ft_HM': 'Transfer (HM)', 'Train': 'Independent'})\n",
    "overall['Metric'] = overall['metric'].str.split('-').apply(lambda x: '{}-{}'.format(x[0], x[1]))\n",
    "\n",
    "from plotnine import *\n",
    "plot = (ggplot(overall, aes(x='Experiment', y='value', fill='Experiment'))\n",
    "         + geom_violin()\n",
    "         + geom_boxplot(width=0.1)\n",
    "         + scale_fill_manual(['#E64B35FF','#4DBBD5FF','#00A087FF','#3C5488FF','#F39B7FFF','#8491B4FF','#91D1C2FF'])\n",
    "         + theme(axis_title_x=element_blank(), axis_text_x=element_blank(), legend_position = (0.70, 0.25))\n",
    "         + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(), axis_ticks_major_x=element_blank(),\n",
    "                 axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "         + xlab('Experiment')\n",
    "         + facet_wrap('Metric')\n",
    "         + ggtitle('Performance of EXPERT in predicting stage of CRC')\n",
    "       )\n",
    "print(plot)\n",
    "plot.save('Figure_performance_across_models.pdf', dpi=120, width=4.8, height=4.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8737347690006)>\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('dataFiles/species_abundance.csv', index_col=0).T\n",
    "Y = pd.read_csv('dataFiles/CRC_samples_stages.csv').set_index('SampleID')\n",
    "\n",
    "pcs = pd.DataFrame(pcoa(distances, number_of_dimensions=2).samples.values.tolist(), index=X.index, columns=['PC1', 'PC2'])\n",
    "pcs = pd.concat([pcs, Y], axis=1)\n",
    "plot = (ggplot(pcs, aes(x='PC1', y='PC2', color='Env'))\n",
    "        + geom_point(size=0.2)\n",
    "        + scale_color_manual(['#E64B35FF','#4DBBD5FF','#00A087FF','#3C5488FF','#F39B7FFF','#8491B4FF','#91D1C2FF'])\n",
    "        + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank())\n",
    "        + stat_ellipse()\n",
    "        + xlab('PC1')\n",
    "        + ylab('PC2')\n",
    "       )\n",
    "print(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =[pd.read_csv('experiments/exp_{}/EvalResult_{}/overall.csv'.format(i, 'Adapt_ft_DM'), index_col=0).rename(columns=lambda x: '{}-exp_{}-{}'.format(x, i, 'Adapt_ft_DM')).dropna()\n",
    "        for i in range(5)]\n",
    "overall = pd.concat(metrics, axis=1)\n",
    "overall\n",
    "overall = overall.reset_index().melt(id_vars=['index'], value_vars=overall.columns.tolist(), var_name='metric').dropna()\n",
    "overall['Experiment'] = overall['metric'].str.split('-').apply(lambda x: '{}'.format( x[3])).map({'Adapt_ft_DM': 'Transfer (DM)', 'Adapt_ft_HM': 'Transfer (HM)', 'Train': 'Independent'})\n",
    "overall['Metric'] = overall['metric'].str.split('-').apply(lambda x: '{}-{}'.format(x[0], x[1]))\n",
    "overall = overall[overall.Metric == 'ROC-AUC'].groupby(by='index', as_index=False).mean().round(4)\n",
    "overall['index'] = overall['index'].apply(lambda x: x.split(' ')[2].rstrip(')'))\n",
    "overall['ROC-AUC'] = overall['value']\n",
    "overall['Stage'] = overall['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (8732487318995)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:727: PlotnineWarning: Saving 2.4 x 4.8 in image.\n",
      "/home/chonghui/envs/miniconda3/envs/expert/lib/python3.8/site-packages/plotnine/ggplot.py:730: PlotnineWarning: Filename: CRC_stage_performance.pdf\n"
     ]
    }
   ],
   "source": [
    "from plotnine import *\n",
    "plot = (ggplot(overall, aes(x='Stage', y='ROC-AUC', fill='Stage'))\n",
    "         + geom_bar(stat='identity', width=0.3, show_legend = False)\n",
    "         + scale_fill_manual(['#E64B35FF','#4DBBD5FF','#00A087FF','#3C5488FF','#F39B7FFF','#8491B4FF','#91D1C2FF'])\n",
    "         + theme(panel_grid_major = element_blank(), panel_grid_minor = element_blank(), panel_background = element_blank(),\n",
    "                 axis_line_x = element_line(color=\"gray\", size = 1), axis_line_y = element_line(color=\"gray\", size = 1))\n",
    "       )\n",
    "print(plot)\n",
    "plot.save('CRC_stage_performance.pdf', dpi=120, width=2.4, height=4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "        ERR475482   ERR475493   ERR475500  ...   ERR481063   ERR481064   ERR481065\n",
      "count  854.000000  854.000000  854.000000  ...  854.000000  854.000000  854.000000\n",
      "mean     0.188848    0.155293    0.091492  ...    0.221712    0.225143    0.222403\n",
      "std      2.274977    1.508092    1.018507  ...    4.716516    4.774864    4.758246\n",
      "min      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "50%      0.000000    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
      "max     47.130700   37.035000   24.845200  ...  135.370960  137.099840  136.803670\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/854 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "         ERR475482    ERR475493  ...    ERR481064    ERR481065\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.026303     0.020940  ...     0.031547     0.031352\n",
      "std       0.859542     0.568293  ...     1.800976     1.794912\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max      47.130699    37.035000  ...   137.099838   136.803665\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Normalizing results...\n",
      "         ERR475482    ERR475493  ...    ERR481064    ERR481065\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.005441     0.004519  ...     0.009505     0.009532\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.298344     0.294471  ...     0.723581     0.726522\n",
      "\n",
      "[6 rows x 635 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.170845  ...    0.0  33.050780  69.748160    0.0\n",
      "1      0.0    0.0    0.0 -0.171821  ...    0.0   2.232037   4.557535    0.0\n",
      "2      0.0    0.0    0.0  5.015766  ...    0.0  15.904994  23.933508    0.0\n",
      "3      0.0    0.0    0.0 -0.170144  ...    0.0  11.578986  24.785040    0.0\n",
      "4      0.0    0.0    0.0  1.735051  ...    0.0   3.927780   8.491182    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "631    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "632    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "633    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "634    0.0    0.0    0.0 -0.172331  ...    0.0  -0.131071  -0.042349    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.172956  ...    0.0  33.823331  71.783639    0.0\n",
      "1      0.0    0.0    0.0 -0.173955  ...    0.0   2.282320   4.691487    0.0\n",
      "2      0.0    0.0    0.0  5.141887  ...    0.0  16.275716  24.632632    0.0\n",
      "3      0.0    0.0    0.0 -0.172237  ...    0.0  11.848324  25.509002    0.0\n",
      "4      0.0    0.0    0.0  1.780061  ...    0.0   4.017805   8.739873    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "631    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "632    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "633    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "634    0.0    0.0    0.0 -0.174478  ...    0.0  -0.136170  -0.042570    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.174914  ...    0.0  33.551714  69.381050    0.0\n",
      "1      0.0    0.0    0.0 -0.175921  ...    0.0   2.265456   4.532195    0.0\n",
      "2      0.0    0.0    0.0  5.177374  ...    0.0  16.145829  23.806587    0.0\n",
      "3      0.0    0.0    0.0 -0.174190  ...    0.0  11.754197  24.653654    0.0\n",
      "4      0.0    0.0    0.0  1.791862  ...    0.0   3.986923   8.445219    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "631    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "632    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "633    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "634    0.0    0.0    0.0 -0.176447  ...    0.0  -0.133500  -0.043574    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.173595  ...    0.0  33.276793  69.338235    0.0\n",
      "1      0.0    0.0    0.0 -0.174612  ...    0.0   2.246413   4.529124    0.0\n",
      "2      0.0    0.0    0.0  5.233645  ...    0.0  16.013264  23.791703    0.0\n",
      "3      0.0    0.0    0.0 -0.172864  ...    0.0  11.657549  24.638252    0.0\n",
      "4      0.0    0.0    0.0  1.813374  ...    0.0   3.953801   8.439750    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "631    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "632    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "633    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "634    0.0    0.0    0.0 -0.175143  ...    0.0  -0.132923  -0.043840    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "     0      1      2         3      ...  18014      18015      18016  18017\n",
      "0      0.0    0.0    0.0 -0.172859  ...    0.0  33.199631  69.466157    0.0\n",
      "1      0.0    0.0    0.0 -0.173845  ...    0.0   2.242247   4.538022    0.0\n",
      "2      0.0    0.0    0.0  5.068282  ...    0.0  15.976713  23.835978    0.0\n",
      "3      0.0    0.0    0.0 -0.172151  ...    0.0  11.631244  24.684081    0.0\n",
      "4      0.0    0.0    0.0  1.753075  ...    0.0   3.945619   8.455830    0.0\n",
      "..     ...    ...    ...       ...  ...    ...        ...        ...    ...\n",
      "630    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "631    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "632    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "633    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "634    0.0    0.0    0.0 -0.174360  ...    0.0  -0.131492  -0.043340    0.0\n",
      "\n",
      "[635 rows x 18018 columns]\n",
      "Total NANs in input samples: 0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "18013    0\n",
      "18014    0\n",
      "18015    0\n",
      "18016    0\n",
      "18017    0\n",
      "Length: 18018, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('dataFiles/species_abundance.csv', index_col=0).to_csv('dataFiles/species_abundance.tsv', sep='\\t')\n",
    "!ls dataFiles/species_abundance.tsv > tmp\n",
    "!expert convert -i tmp -o CRC_cm.h5 --in-cm\n",
    "!for i in {0,1,2,3,4}; do expert search -i CRC_cm.h5 -o CRC_contribution_$i -m ../Disease-diagnosis/experiments/exp_$i/TrainModel; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IV     204\n",
       "I      196\n",
       "II     126\n",
       "III     93\n",
       "0       16\n",
       "Name: disease_stage, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('dataFiles/CRC_samples_stages.csv').disease_stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
