{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed to get more reproducible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(2)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from expert.src.utils import read_genus_abu, read_labels, load_otlg, zero_weight_unk, parse_otlg, get_dmax\n",
    "from expert.src.preprocessing import *\n",
    "from expert.src.model import *\n",
    "from expert.CLI.CLI_utils import find_pkg_resource as find_expert_resource\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, AlphaDropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.initializers import HeUniform, GlorotUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ontology using mapper file of source samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading microbiome structure...\n",
      "Generating Ontology...\n",
      "100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 13706.88it/s]\n",
      "root\n",
      "├── root:adenoma\n",
      "├── root:carcinoma\n",
      "└── root:normal\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!awk -F ',' '{print $4}' exp_0/SourceMapper_0.csv | grep -v \"Env\" | sort | uniq  > microbiomes.txt\n",
    "!expert construct -i microbiomes.txt -o ontology.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyper-parameters for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = HeUniform(seed=2)\n",
    "sig_init = GlorotUniform(seed=2)\n",
    "phylogeny_path = find_expert_resource('resources/phylogeny.csv')\n",
    "ontology = load_otlg('ontology.pkl')\n",
    "phylogeny = pd.read_csv(phylogeny_path, index_col=0)\n",
    "lrreducer = ReduceLROnPlateau(monitor='val_loss', patience=5, min_lr=1e-5, verbose=5, factor=0.1)\n",
    "stopper = EarlyStopping(monitor='val_loss', patience=15, verbose=5, restore_best_weights=True)\n",
    "callbacks = [lrreducer, stopper]\n",
    "phylogeny = pd.read_csv(find_expert_resource('resources/phylogeny.csv'), index_col=0)\n",
    "optimizer = Adam(lr=1e-4)\n",
    "metrics = [BinaryAccuracy(name='acc'), AUC(name='AUC')]\n",
    "loss = BinaryCrossentropy()\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "validation_split = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data, using EXPERT's command-line API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761368    SRR2761286  ...   SRR2761073   SRR2761072\n",
      "count   864.000000    864.000000  ...   864.000000   864.000000\n",
      "mean     13.164352     64.079861  ...    13.928241    10.555556\n",
      "std     156.377417   1572.456088  ...   210.361702    79.561511\n",
      "min       0.000000      0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000      0.000000  ...     0.000000     0.000000\n",
      "max    2737.000000  46002.000000  ...  5340.000000  1188.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761368    SRR2761286  ...   SRR2761073   SRR2761072\n",
      "count  6006.000000   6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.893773      9.218282  ...     2.003663     1.518315\n",
      "std      64.806343    597.089600  ...    82.235092    33.515808\n",
      "min       0.000000      0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000      0.000000  ...     0.000000     0.000000\n",
      "max    3292.000000  46002.000000  ...  5340.000000  1207.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Normalizing results...\n",
      "        SRR2761368   SRR2761286  ...   SRR2761073   SRR2761072\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.005698     0.010785  ...     0.006834     0.003675\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.289432     0.830886  ...     0.443743     0.132361\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761058   SRR2761252  ...  SRR2761077   SRR2761308\n",
      "count   864.000000   864.000000  ...  864.000000   864.000000\n",
      "mean      9.901620    16.127315  ...    6.876157    11.552083\n",
      "std     141.191202   155.258675  ...   52.186222   258.025800\n",
      "min       0.000000     0.000000  ...    0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...    0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...  863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.424409     2.320013  ...     0.989177     1.661838\n",
      "std      54.680592    61.707222  ...    20.957577    97.928490\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...   863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Normalizing results...\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006392     0.004429  ...     0.003528     0.009811\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.434249     0.188245  ...     0.145262     0.748923\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       153\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:adenoma      51\n",
      "root:carcinoma    47\n",
      "root:normal       55\n",
      "Unknown            0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       17\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:adenoma      6\n",
      "root:carcinoma    5\n",
      "root:normal       6\n",
      "Unknown           0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761321   SRR2761399  ...   SRR2761046   SRR2761048\n",
      "count   864.000000   864.000000  ...   864.000000   864.000000\n",
      "mean     12.310185    13.442130  ...    13.165509     7.375000\n",
      "std     175.749733   108.397391  ...   114.079811    80.135303\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    4966.000000  1976.000000  ...  1920.000000  1396.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761321   SRR2761399  ...   SRR2761046   SRR2761048\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.770896     1.933733  ...     1.893939     1.060939\n",
      "std      66.867310    45.346073  ...    47.172951    30.596561\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    4966.000000  2089.000000  ...  2177.000000  1405.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Normalizing results...\n",
      "        SRR2761321   SRR2761399  ...   SRR2761046   SRR2761048\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006287     0.003904  ...     0.004147     0.004802\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.466905     0.179869  ...     0.191385     0.220496\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761058   SRR2761252  ...  SRR2761077   SRR2761308\n",
      "count   864.000000   864.000000  ...  864.000000   864.000000\n",
      "mean      9.901620    16.127315  ...    6.876157    11.552083\n",
      "std     141.191202   155.258675  ...   52.186222   258.025800\n",
      "min       0.000000     0.000000  ...    0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...    0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...  863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.424409     2.320013  ...     0.989177     1.661838\n",
      "std      54.680592    61.707222  ...    20.957577    97.928490\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...   863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Normalizing results...\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006392     0.004429  ...     0.003528     0.009811\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.434249     0.188245  ...     0.145262     0.748923\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       153\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:adenoma      51\n",
      "root:carcinoma    47\n",
      "root:normal       55\n",
      "Unknown            0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       17\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:adenoma      6\n",
      "root:carcinoma    5\n",
      "root:normal       6\n",
      "Unknown           0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761338   SRR2761310  ...    SRR2761039    SRR2761034\n",
      "count   864.000000   864.000000  ...    864.000000    864.000000\n",
      "mean      9.552083    12.314815  ...     19.733796     21.420139\n",
      "std     108.843439   106.286329  ...    525.768989    548.838171\n",
      "min       0.000000     0.000000  ...      0.000000      0.000000\n",
      "50%       0.000000     0.000000  ...      0.000000      0.000000\n",
      "max    2557.000000  1689.000000  ...  15446.000000  16086.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761338   SRR2761310  ...    SRR2761039    SRR2761034\n",
      "count  6006.000000  6006.000000  ...   6006.000000   6006.000000\n",
      "mean      1.374126     1.771562  ...      2.835331      3.081419\n",
      "std      44.290070    43.927742  ...    199.442902    211.518188\n",
      "min       0.000000     0.000000  ...      0.000000      0.000000\n",
      "50%       0.000000     0.000000  ...      0.000000      0.000000\n",
      "max    2557.000000  1689.000000  ...  15446.000000  16346.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Normalizing results...\n",
      "        SRR2761338   SRR2761310  ...   SRR2761039   SRR2761034\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.005367     0.004129  ...     0.011712     0.011429\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.309827     0.158741  ...     0.907041     0.883233\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761058   SRR2761252  ...  SRR2761077   SRR2761308\n",
      "count   864.000000   864.000000  ...  864.000000   864.000000\n",
      "mean      9.901620    16.127315  ...    6.876157    11.552083\n",
      "std     141.191202   155.258675  ...   52.186222   258.025800\n",
      "min       0.000000     0.000000  ...    0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...    0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...  863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.424409     2.320013  ...     0.989177     1.661838\n",
      "std      54.680592    61.707222  ...    20.957577    97.928490\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...   863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Normalizing results...\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006392     0.004429  ...     0.003528     0.009811\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.434249     0.188245  ...     0.145262     0.748923\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       153\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:adenoma      51\n",
      "root:carcinoma    47\n",
      "root:normal       55\n",
      "Unknown            0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       17\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:adenoma      6\n",
      "root:carcinoma    5\n",
      "root:normal       6\n",
      "Unknown           0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761400   SRR2761290  ...  SRR2761037   SRR2761292\n",
      "count   864.000000   864.000000  ...  864.000000   864.000000\n",
      "mean     12.870370    28.462963  ...    2.275463    21.928241\n",
      "std     116.410278   244.300152  ...   35.529995   209.150956\n",
      "min       0.000000     0.000000  ...    0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...    0.000000     0.000000\n",
      "max    2024.000000  4953.000000  ...  964.000000  4001.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761400   SRR2761290  ...   SRR2761037   SRR2761292\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.851482     4.094572  ...     0.327339     3.154512\n",
      "std      48.400639    94.271309  ...    13.495492    83.336151\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    2378.000000  4953.000000  ...   964.000000  4404.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Normalizing results...\n",
      "        SRR2761400   SRR2761290  ...   SRR2761037   SRR2761292\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.004353     0.003833  ...     0.006864     0.004399\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.213849     0.201407  ...     0.490336     0.232450\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761058   SRR2761252  ...  SRR2761077   SRR2761308\n",
      "count   864.000000   864.000000  ...  864.000000   864.000000\n",
      "mean      9.901620    16.127315  ...    6.876157    11.552083\n",
      "std     141.191202   155.258675  ...   52.186222   258.025800\n",
      "min       0.000000     0.000000  ...    0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...    0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...  863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.424409     2.320013  ...     0.989177     1.661838\n",
      "std      54.680592    61.707222  ...    20.957577    97.928490\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...   863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Normalizing results...\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006392     0.004429  ...     0.003528     0.009811\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.434249     0.188245  ...     0.145262     0.748923\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       153\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:adenoma      51\n",
      "root:carcinoma    47\n",
      "root:normal       55\n",
      "Unknown            0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       17\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:adenoma      6\n",
      "root:carcinoma    5\n",
      "root:normal       6\n",
      "Unknown           0\n",
      "dtype: int64\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761390   SRR2761341  ...   SRR2761049   SRR2761057\n",
      "count   864.000000   864.000000  ...   864.000000   864.000000\n",
      "mean     15.630787    15.509259  ...    16.634259    10.638889\n",
      "std     322.884241   127.786104  ...   303.003146   111.593895\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    9426.000000  2380.000000  ...  8719.000000  2307.000000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761390   SRR2761341  ...   SRR2761049  SRR2761057\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.00000\n",
      "mean      2.237429     2.231102  ...     2.392940     1.53047\n",
      "std     122.893173    51.019226  ...   116.168449    46.71463\n",
      "min       0.000000     0.000000  ...     0.000000     0.00000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.00000\n",
      "max    9426.000000  2380.000000  ...  8719.000000  2444.00000\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Normalizing results...\n",
      "        SRR2761390   SRR2761341  ...   SRR2761049   SRR2761057\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.009145     0.003807  ...     0.008083     0.005082\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.701444     0.177612  ...     0.606666     0.265883\n",
      "\n",
      "[6 rows x 17 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "running...\n",
      "Reading and concatenating data, this could be slow if you have huge amount of data\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "        SRR2761058   SRR2761252  ...  SRR2761077   SRR2761308\n",
      "count   864.000000   864.000000  ...  864.000000   864.000000\n",
      "mean      9.901620    16.127315  ...    6.876157    11.552083\n",
      "std     141.191202   155.258675  ...   52.186222   258.025800\n",
      "min       0.000000     0.000000  ...    0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...    0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...  863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Initializing in-memory taxonomy database for ultra-fast querying.\n",
      "db file: /home/chonghui/.etetoolkit/taxa.sqlite\n",
      "There will be 0/864 entries droped cause they are not in NCBI taxanomy database\n",
      "Series([], dtype: object)\n",
      "Extracting lineages for taxonomic entries, this may take a few minutes\n",
      "Filling samples in phylogeny matrix\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      1.424409     2.320013  ...     0.989177     1.661838\n",
      "std      54.680592    61.707222  ...    20.957577    97.928490\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max    3715.000000  2623.000000  ...   863.000000  7475.000000\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Normalizing results...\n",
      "        SRR2761058   SRR2761252  ...   SRR2761077   SRR2761308\n",
      "count  6006.000000  6006.000000  ...  6006.000000  6006.000000\n",
      "mean      0.000167     0.000167  ...     0.000167     0.000167\n",
      "std       0.006392     0.004429  ...     0.003528     0.009811\n",
      "min       0.000000     0.000000  ...     0.000000     0.000000\n",
      "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
      "max       0.434249     0.188245  ...     0.145262     0.748923\n",
      "\n",
      "[6 rows x 153 columns]\n",
      "Total NaNs: 0\n",
      "Saving results...\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       153\n",
      "Unknown      0\n",
      "dtype: int64\n",
      "root:adenoma      51\n",
      "root:carcinoma    47\n",
      "root:normal       55\n",
      "Unknown            0\n",
      "dtype: int64\n",
      "Mapping sources to microbiome ontology...\n",
      "The ontology contains 2 layers.\n",
      "Saving labels for each ontology layer...\n",
      "root       17\n",
      "Unknown     0\n",
      "dtype: int64\n",
      "root:adenoma      6\n",
      "root:carcinoma    5\n",
      "root:normal       6\n",
      "Unknown           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 39.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 262.48it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 32.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 275.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 40.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 112.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 258.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 33.82it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 150.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 24.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 64.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 247.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 33.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 157.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 26.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 106.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 275.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 302.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 38.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 102.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.88it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 321.48it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 39.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 332.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 40.85it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$phylogeny_path\" \"$phylogeny\"\n",
    "\n",
    "for((i=0; i<5; i++)); do \\\n",
    "ls exp_$i/QueryCM_$i.tsv > tmp; expert convert -i tmp --in-cm -o exp_$i/QueryCM_$i.h5; \\\n",
    "ls exp_$i/SourceCM_$i.tsv > tmp; expert convert -i tmp --in-cm -o exp_$i/SourceCM_$i.h5; \\\n",
    "expert map --to-otlg -t ontology.pkl -i exp_$i/SourceMapper_$i.csv -o exp_$i/SourceLabels_$i.h5; \\\n",
    "expert map --to-otlg -t ontology.pkl -i exp_$i/QueryMapper_$i.csv -o exp_$i/QueryLabels_$i.h5; \\\n",
    "done\n",
    "rm tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched samples: 153\n",
      "N. NaN in input features: 0\n",
      "           mean       std\n",
      "0      0.000000  0.000000\n",
      "1      0.000000  0.000000\n",
      "2      0.000000  0.000000\n",
      "3      0.000000  0.000000\n",
      "4      0.000000  0.000000\n",
      "...         ...       ...\n",
      "18013  0.000580  0.002977\n",
      "18014  0.000000  0.000000\n",
      "18015  0.001645  0.007644\n",
      "18016  0.000479  0.003096\n",
      "18017  0.000000  0.000000\n",
      "\n",
      "[18018 rows x 2 columns]\n",
      "Epoch 1/2000\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7711 - acc: 0.5033 - AUC: 0.4650 - val_loss: 0.6665 - val_acc: 0.6042 - val_AUC: 0.5693\n",
      "Epoch 2/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7300 - acc: 0.5669 - AUC: 0.5088 - val_loss: 0.6661 - val_acc: 0.5625 - val_AUC: 0.5674\n",
      "Epoch 3/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7449 - acc: 0.5304 - AUC: 0.5001 - val_loss: 0.6734 - val_acc: 0.6250 - val_AUC: 0.5312\n",
      "Epoch 4/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6998 - acc: 0.5645 - AUC: 0.5495 - val_loss: 0.6697 - val_acc: 0.6250 - val_AUC: 0.5430\n",
      "Epoch 5/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7095 - acc: 0.5572 - AUC: 0.5354 - val_loss: 0.6708 - val_acc: 0.6042 - val_AUC: 0.5234\n",
      "Epoch 6/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7020 - acc: 0.5718 - AUC: 0.5663 - val_loss: 0.6700 - val_acc: 0.6250 - val_AUC: 0.5215\n",
      "Epoch 7/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6864 - acc: 0.5718 - AUC: 0.5722 - val_loss: 0.6609 - val_acc: 0.6042 - val_AUC: 0.5557\n",
      "Epoch 8/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6892 - acc: 0.5718 - AUC: 0.5844 - val_loss: 0.6523 - val_acc: 0.6250 - val_AUC: 0.5762\n",
      "Epoch 9/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6858 - acc: 0.5912 - AUC: 0.5854 - val_loss: 0.6455 - val_acc: 0.6458 - val_AUC: 0.6084\n",
      "Epoch 10/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7214 - acc: 0.5572 - AUC: 0.5215 - val_loss: 0.6398 - val_acc: 0.7083 - val_AUC: 0.6387\n",
      "Epoch 11/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7081 - acc: 0.5547 - AUC: 0.5500 - val_loss: 0.6319 - val_acc: 0.6667 - val_AUC: 0.6689\n",
      "Epoch 12/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6291 - acc: 0.6034 - AUC: 0.6389 - val_loss: 0.6254 - val_acc: 0.6458 - val_AUC: 0.6914\n",
      "Epoch 13/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6873 - acc: 0.5766 - AUC: 0.5682 - val_loss: 0.6182 - val_acc: 0.6875 - val_AUC: 0.7217\n",
      "Epoch 14/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6763 - acc: 0.5985 - AUC: 0.5796 - val_loss: 0.6131 - val_acc: 0.6875 - val_AUC: 0.7461\n",
      "Epoch 15/2000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6433 - acc: 0.6058 - AUC: 0.6186 - val_loss: 0.6131 - val_acc: 0.7083 - val_AUC: 0.7285\n",
      "Epoch 16/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6512 - acc: 0.6010 - AUC: 0.6069 - val_loss: 0.6112 - val_acc: 0.7083 - val_AUC: 0.7275\n",
      "Epoch 17/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6348 - acc: 0.6326 - AUC: 0.6529 - val_loss: 0.6088 - val_acc: 0.7083 - val_AUC: 0.7344\n",
      "Epoch 18/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6456 - acc: 0.6131 - AUC: 0.6147 - val_loss: 0.6059 - val_acc: 0.7292 - val_AUC: 0.7461\n",
      "Epoch 19/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6558 - acc: 0.6253 - AUC: 0.6096 - val_loss: 0.6061 - val_acc: 0.7083 - val_AUC: 0.7334\n",
      "Epoch 20/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6465 - acc: 0.6229 - AUC: 0.6093 - val_loss: 0.6069 - val_acc: 0.6875 - val_AUC: 0.7266\n",
      "Epoch 21/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6577 - acc: 0.6253 - AUC: 0.6121 - val_loss: 0.6052 - val_acc: 0.6875 - val_AUC: 0.7256\n",
      "Epoch 22/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6252 - acc: 0.6204 - AUC: 0.6405 - val_loss: 0.6048 - val_acc: 0.6875 - val_AUC: 0.7305\n",
      "Epoch 23/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6276 - acc: 0.6521 - AUC: 0.6479 - val_loss: 0.6035 - val_acc: 0.7083 - val_AUC: 0.7354\n",
      "Epoch 24/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6652 - acc: 0.5937 - AUC: 0.5787 - val_loss: 0.6020 - val_acc: 0.7292 - val_AUC: 0.7529\n",
      "Epoch 25/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6117 - acc: 0.6423 - AUC: 0.6514 - val_loss: 0.6006 - val_acc: 0.7292 - val_AUC: 0.7480\n",
      "Epoch 26/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6438 - acc: 0.6229 - AUC: 0.6291 - val_loss: 0.6004 - val_acc: 0.7292 - val_AUC: 0.7422\n",
      "Epoch 27/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6439 - acc: 0.6204 - AUC: 0.6282 - val_loss: 0.5954 - val_acc: 0.7292 - val_AUC: 0.7568\n",
      "Epoch 28/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6373 - acc: 0.6472 - AUC: 0.6351 - val_loss: 0.5916 - val_acc: 0.7292 - val_AUC: 0.7734\n",
      "Epoch 29/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6226 - acc: 0.6399 - AUC: 0.6524 - val_loss: 0.5879 - val_acc: 0.7500 - val_AUC: 0.7891\n",
      "Epoch 30/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6662 - acc: 0.5912 - AUC: 0.5922 - val_loss: 0.5832 - val_acc: 0.7292 - val_AUC: 0.8105\n",
      "Epoch 31/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6247 - acc: 0.6448 - AUC: 0.6401 - val_loss: 0.5797 - val_acc: 0.7500 - val_AUC: 0.8232\n",
      "Epoch 32/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6423 - acc: 0.6375 - AUC: 0.6328 - val_loss: 0.5784 - val_acc: 0.7500 - val_AUC: 0.8359\n",
      "Epoch 33/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6362 - acc: 0.6423 - AUC: 0.6316 - val_loss: 0.5776 - val_acc: 0.7708 - val_AUC: 0.8555\n",
      "Epoch 34/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5962 - acc: 0.6618 - AUC: 0.6651 - val_loss: 0.5780 - val_acc: 0.7708 - val_AUC: 0.8594\n",
      "Epoch 35/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6305 - acc: 0.6131 - AUC: 0.6140 - val_loss: 0.5765 - val_acc: 0.7708 - val_AUC: 0.8721\n",
      "Epoch 36/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6286 - acc: 0.6472 - AUC: 0.6309 - val_loss: 0.5747 - val_acc: 0.8125 - val_AUC: 0.8730\n",
      "Epoch 37/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6251 - acc: 0.6448 - AUC: 0.6302 - val_loss: 0.5735 - val_acc: 0.7917 - val_AUC: 0.8711\n",
      "Epoch 38/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6190 - acc: 0.6740 - AUC: 0.6574 - val_loss: 0.5735 - val_acc: 0.7917 - val_AUC: 0.8750\n",
      "Epoch 39/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6335 - acc: 0.6448 - AUC: 0.6624 - val_loss: 0.5725 - val_acc: 0.8125 - val_AUC: 0.8770\n",
      "Epoch 40/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6373 - acc: 0.6302 - AUC: 0.6297 - val_loss: 0.5708 - val_acc: 0.8333 - val_AUC: 0.8721\n",
      "Epoch 41/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6239 - acc: 0.6521 - AUC: 0.6417 - val_loss: 0.5700 - val_acc: 0.8333 - val_AUC: 0.8682\n",
      "Epoch 42/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6286 - acc: 0.5937 - AUC: 0.6307 - val_loss: 0.5686 - val_acc: 0.8333 - val_AUC: 0.8701\n",
      "Epoch 43/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6133 - acc: 0.6642 - AUC: 0.6545 - val_loss: 0.5656 - val_acc: 0.8333 - val_AUC: 0.8730\n",
      "Epoch 44/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6147 - acc: 0.6448 - AUC: 0.6530 - val_loss: 0.5655 - val_acc: 0.8333 - val_AUC: 0.8760\n",
      "Epoch 45/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6144 - acc: 0.6715 - AUC: 0.6627 - val_loss: 0.5689 - val_acc: 0.8333 - val_AUC: 0.8633\n",
      "Epoch 46/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6267 - acc: 0.6229 - AUC: 0.6337 - val_loss: 0.5708 - val_acc: 0.8125 - val_AUC: 0.8535\n",
      "Epoch 47/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6036 - acc: 0.6667 - AUC: 0.6628 - val_loss: 0.5722 - val_acc: 0.7917 - val_AUC: 0.8330\n",
      "Epoch 48/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6247 - acc: 0.6204 - AUC: 0.6365 - val_loss: 0.5705 - val_acc: 0.7917 - val_AUC: 0.8359\n",
      "Epoch 49/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5891 - acc: 0.6715 - AUC: 0.6902 - val_loss: 0.5697 - val_acc: 0.7917 - val_AUC: 0.8271\n",
      "Epoch 50/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5946 - acc: 0.6691 - AUC: 0.6999 - val_loss: 0.5713 - val_acc: 0.7917 - val_AUC: 0.8223\n",
      "Epoch 51/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6159 - acc: 0.6521 - AUC: 0.6612 - val_loss: 0.5710 - val_acc: 0.7917 - val_AUC: 0.8232\n",
      "Epoch 52/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5852 - acc: 0.6618 - AUC: 0.6996 - val_loss: 0.5711 - val_acc: 0.7917 - val_AUC: 0.8232\n",
      "Epoch 53/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6699 - acc: 0.6277 - AUC: 0.6030 - val_loss: 0.5730 - val_acc: 0.7917 - val_AUC: 0.8242\n",
      "Epoch 54/2000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5964 - acc: 0.6691 - AUC: 0.6960 - val_loss: 0.5742 - val_acc: 0.7917 - val_AUC: 0.8242\n",
      "Epoch 55/2000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6055 - acc: 0.6302 - AUC: 0.6751 - val_loss: 0.5731 - val_acc: 0.7917 - val_AUC: 0.8281\n",
      "Epoch 56/2000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5980 - acc: 0.6715 - AUC: 0.6934 - val_loss: 0.5713 - val_acc: 0.8333 - val_AUC: 0.8369\n",
      "Epoch 57/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5933 - acc: 0.6253 - AUC: 0.6675 - val_loss: 0.5712 - val_acc: 0.8333 - val_AUC: 0.8398\n",
      "Epoch 58/2000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6062 - acc: 0.6448 - AUC: 0.6563 - val_loss: 0.5719 - val_acc: 0.8333 - val_AUC: 0.8389\n",
      "Epoch 59/2000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.5841 - acc: 0.6771 - AUC: 0.7397Restoring model weights from the end of the best epoch.\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5856 - acc: 0.6813 - AUC: 0.7128 - val_loss: 0.5722 - val_acc: 0.8125 - val_AUC: 0.8379\n",
      "Epoch 00059: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f651447b850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyModel(Model):\n",
    "    \n",
    "\tdef init_base_block(self, num_features):\n",
    "\t\tblock = tf.keras.Sequential(name='base')\n",
    "\t\tblock.add(Flatten()) # (1000, )\n",
    "\t\tblock.add(Dropout(0.5, seed=2))\n",
    "\t\tblock.add(Dense(2**6, kernel_initializer=init))\n",
    "\t\tblock.add(Activation('relu')) # (1024, )\n",
    "\t\tblock.add(Dense(2**5, kernel_initializer=init))\n",
    "\t\tblock.add(Activation('relu')) # (1024, )\n",
    "\t\treturn block\n",
    "    \n",
    "\tdef init_inter_block(self, index, name, n_units):\n",
    "\t\tk = index\n",
    "\t\tblock = tf.keras.Sequential(name=name)\n",
    "\t\tblock.add(Dropout(0.7, seed=2))\n",
    "\t\treturn block\n",
    "    \n",
    "\tdef _init_integ_block(self, index, name, n_units):\n",
    "\t\tblock = tf.keras.Sequential(name=name)\n",
    "\t\tk = index\n",
    "\t\tblock.add(Dense(self._get_n_units(2**6), name='l' + str(k) + '_integ_fc0', kernel_initializer=sig_init))\n",
    "\t\tblock.add(Activation('relu'))\n",
    "\t\treturn block\n",
    "    \n",
    "    \n",
    "exp=0\n",
    "X, idx = read_genus_abu('exp_{0}/SourceCM_{0}.h5'.format(exp))\n",
    "Y = read_labels('exp_{0}/SourceLabels_{0}.h5'.format(exp), shuffle_idx=idx, dmax=get_dmax('exp_{0}/SourceLabels_{0}.h5'.format(exp)))\n",
    "IDs = sorted(list(set(X.index.to_list()).intersection(Y[0].index.to_list())))\n",
    "X = X.loc[IDs, :]\n",
    "Y = [y.loc[IDs, :] for y in Y]\n",
    "print('Total matched samples:', sum(X.index == Y[0].index))\n",
    "\n",
    "model = TinyModel(phylogeny=phylogeny, num_features=X.shape[1],  ontology=ontology)\n",
    "\n",
    "# Feature encoding and standardization\n",
    "X = model.encoder.predict(X, batch_size=batch_size)\n",
    "X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "print('N. NaN in input features:', np.isnan(X).sum())\n",
    "model.update_statistics(mean=X.mean(axis=0), std=X.std(axis=0))\n",
    "X = model.standardize(X)\n",
    "\n",
    "# Sample weight \"zero\" to mask unknown samples' contribution to loss\n",
    "sample_weight = [zero_weight_unk(y=y, sample_weight=np.ones(y.shape[0])) for i, y in enumerate(Y)]\n",
    "Y = [y.drop(columns=['Unknown']) for y in Y]\n",
    "\n",
    "model.nn.compile(optimizer=optimizer, loss=loss, weighted_metrics=metrics)\n",
    "model.nn.fit(X, Y, validation_split=validation_split, batch_size=batch_size, epochs=epochs, sample_weight=sample_weight, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        [(None, 18018)]           0         \n",
      "_________________________________________________________________\n",
      "base (Sequential)            (None, 512)               18976256  \n",
      "_________________________________________________________________\n",
      "l2_inter (Sequential)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "l2_integration (Sequential)  (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "l2o (Sequential)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,992,771\n",
      "Trainable params: 18,992,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
